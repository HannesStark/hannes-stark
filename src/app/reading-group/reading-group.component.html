<div fxLayout="column" fxLayoutAlign="center center" xmlns="http://www.w3.org/1999/html">

  <div class="width100" fxLayout="row" fxLayout.lt-md="column" fxLayoutAlign="center center">
    <div fxFlex="35" fxLayoutAlign="end center" fxLayoutAlign.lt-md="center center">
      <img class="profile-image" src="assets/randomEuclidean.png" alt="Euclid Elements">
    </div>
    <div fxFlex="55" fxFlex.xs="65" ngClass.sm="small-margin" ngClass.xs="small-margin--xs">
      <h1 class="Title" ngClass.lt-md="text-align-center">LoGaG: Learning on Graphs and Geometry Reading Group!</h1>
      <!--
      <div class="underline-title--top" fxLayoutAlign.lt-md="center center">
        <h1 class="left-titles">M. Sc. Computer Science Student</h1>
      </div>
      -->
      <div class="item" fxLayout="column" fxLayoutAlign.lt-md="center center">
        <p class="text-align-left">
          Welcome to the LoGaG reading group where GraphML researchers present and discuss their papers every week! <br>
          Below
          are the <a (click)="scroll('Schedule')" class="link-style-green">Schedule</a> and <a
          (click)="scroll('previous')" class="link-style-green">previously</a> covered papers. I organize this reading
          group with the awesome help of <a
          href="https://www.valencediscovery.com/"
          target="_blank"
          class="link-style-green">Valence Discovery</a>!
        </p>

        <div fxLayout="row" fxLayoutAlign="left center">
          <p>
            <a mat-button class="group-button item-title"
               href="https://zoom.us/j/5775722530?pwd=ZzlGTXlDNThhUDZOdU4vN2JRMm5pQT09"
               target="_blank">Zoom Link</a>
          </p>
          <div class="next-to-button">
            Just hop on! We meet every Tuesday at 5pm CEST / 3pm UTC / 11am EST / 8am PDT.
          </div>
        </div>

        <div fxLayout="row" fxLayoutAlign="left center">
          <p>
            <a mat-button class="group-button item-title"
               href="https://join.slack.com/t/logag/shared_invite/zt-u0mbo1ec-zElmvd1oSCXGjXvxLSokvg"
               target="_blank">Join Slack!</a>
          </p>
          <div class="next-to-button">
            For discussions outside of our meetings and to vote for papers.
          </div>
        </div>

        <div fxLayout="row" fxLayoutAlign="left center">
          <p>
            <a mat-button class="group-button item-title"
               href="https://groups.google.com/g/logag"
               target="_blank">Join Mailing List!</a>
          </p>
          <div class="next-to-button">
            For weekly updates about the next paper (via Google groups).
          </div>
        </div>

        <p>
          You can also subscribe to the meetings via <a
          href="https://calendar.google.com/calendar/u/0?cid=dmR1am4ycGJwa2hncjVmNTVjbTM5cWJtdThAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ"
          target="_blank"
          class="link-style-green">Google Calendar</a>, or <a
          href="https://calendar.google.com/calendar/ical/vdujn2pbpkhgr5f55cm39qbmu8%40group.calendar.google.com/public/basic.ics"
          target="_blank"
          class="link-style-green">via iCal</a>. Alternatively, <a
          href="https://calendar.google.com/event?action=TEMPLATE&tmeid=MWhmZDJzMnI0aG10YTlvZzZpZGVsYnJmaWNfMjAyMTA4MDNUMTUwMDAwWiB2ZHVqbjJwYnBraGdyNWY1NWNtMzlxYm11OEBn&tmsrc=vdujn2pbpkhgr5f55cm39qbmu8%40group.calendar.google.com&scp=ALL"
          target="_blank"
          class="link-style-green">add the events</a>.
        </p>
        <!--div class="item" fxLayout="column" fxLayoutAlign.lt-md="center center">
          <div fxLayout="row">
            <p>
              <a mat-button class="group-button item-title" href="https://groups.google.com/g/logag"
                 target="_blank">Sign up for email updates!</a>
            </p>
          </div>
          <div class="item-content">
            <div>

              <form class="contact" [formGroup]="form" method="post" (ngSubmit)="onSubmit()">
                <div fxLayout="row" fxLayout.lt-md="column">
                  <mat-form-field>
                    <mat-label>Name (optional)</mat-label>
                    <input matInput [formControl]="name">
                    <mat-error *ngIf="name.invalid">Please enter your name</mat-error>
                  </mat-form-field>
                  <mat-form-field [ngClass.gt-sm]="'margin-left'">
                    <mat-label>Email</mat-label>
                    <input matInput [formControl]="email" placeholder="email@example.com" required type="email">
                    <mat-error *ngIf="email.invalid">Please enter a valid email</mat-error>
                  </mat-form-field>

                </div>
                <div style="text-align:center">
                  <button mat-button [class.spinner]="isLoading" [disabled]="isLoading" class="submit submit-button"
                          type="submit">
                    Submit
                  </button>
                </div>
                <input [formControl]="honeypot" class="hidden" type="text"/>
                <div [ngClass]="!(success && submit)? 'hidden' : 'visible'" class="success-message">
                  <span>{{responseMessage}}</span>
                </div>
                <div [ngClass]="!(!success && submit)? 'hidden' : 'visible'" class="failed-message">
                  <span>{{responseMessage}}</span>
                </div>
              </form>
            </div>
          </div>
        </div-->
      </div>
    </div>
    <div fxFlex="10" fxFlex.xs="">
    </div>
  </div>

  <div fxLayout="column" class="width100">
    <!-- section 0 -->
    <div>
      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" class="underline-title">
          <h1 class="left-titles">LATEST VIDEO (scroll down for more)</h1>
        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>


      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" fxLayout="column">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2202.13013"
                   target="_blank">
                  Sign and Basis Invariant Networks for Spectral Graph Representation Learning (2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                12th of April 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Derek Lim, Joshua Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, Stefanie Jegelka
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLSignNet' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their <a class="item-subtitle link-style-black"
                         href="https://hannes-stark.com/assets/final_reading_group_slides.pdf"
                         target="_blank">slides</a> (96 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First authors <a
                class="item-subtitle link-style-black"
                href="https://cptq.github.io/"
                target="_blank">Derek Lim</a> and <a
                class="item-subtitle link-style-black"
                href="https://joshrobinson.mit.edu/"
                target="_blank">Joshua Robinson</a>, who both are PhD students in the group of Prof. Stefanie Jegelka at
                MIT. Josh is also co-advised by Prof. Suvrit Sra and Derek previously presented ESAN in the reading
                group!
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Many machine learning tasks involve processing eigenvectors derived from data. Especially valuable are
                Laplacian eigenvectors, which capture useful structural information about graphs and other geometric
                objects. However, ambiguities arise when computing eigenvectors: for each eigenvector v, the sign
                flipped
                −v is also an eigenvector. More generally, higher dimensional eigenspaces contain infinitely many
                choices
                of basis eigenvectors. These ambiguities make it a challenge to process eigenvectors and eigenspaces in
                a
                consistent way. In this work we introduce SignNet and BasisNet -- new neural architectures that are
                invariant to all requisite symmetries and hence process collections of eigenspaces in a principled
                manner.
                Our networks are universal, i.e., they can approximate any continuous function of eigenvectors with the
                proper invariances. They are also theoretically strong for graph representation learning -- they can
                approximate any spectral graph convolution, can compute spectral invariants that go beyond message
                passing
                neural networks, and can provably simulate previously proposed graph positional encodings. Experiments
                show the strength of our networks for learning spectral graph filters and learning graph positional
                encodings.
              </p>
            </div>
          </div>

        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>
    </div>

    <div id="Schedule">
      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" class="underline-title">
          <h1 class="left-titles">SCHEDULE</h1>
        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>


      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" fxLayout="column">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.02096"
                   target="_blank">
                  Top-N: Equivariant set and graph generation without exchangeability (ICLR 2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                26th of April 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Clément Vignac, Pascal Frossard
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author <a
              class="item-subtitle link-style-black"
              href="https://cvignac.github.io/"
              target="_blank">Clément Vignac</a> who is a PhD student at EPFL in the lab of Prof. Pascal Frossard. He
              works on ML for sets and graphs, incorporating symmetry priors of these data modalities, and graph
              generation.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              This work addresses one-shot set and graph generation, and, more specifically, the parametrization of
              probabilistic decoders that map a vector-shaped prior to a distribution over sets or graphs. Sets and
              graphs are most commonly generated by first sampling points i.i.d. from a normal distribution, and then
              processing these points along with the prior vector using Transformer layers or Graph Neural Networks.
              This architecture is designed to generate exchangeable distributions, i.e., all permutations of the
              generated outputs are equally likely. We however show that it only optimizes a proxy to the evidence lower
              bound, which makes it hard to train. We then study equivariance in generative settings and show that
              non-exchangeable methods can still achieve permutation equivariance. Using this result, we introduce Top-n
              creation, a differentiable generation mechanism that uses the latent vector to select the most relevant
              points from a trainable reference set. Top-n can replace i.i.d. generation in any Variational Autoencoder
              or Generative Adversarial Network. Experimentally, our method outperforms i.i.d. generation by 15% at
              SetMNIST reconstruction, by 33% at object detection on CLEVR, generates sets that are 74% closer to the
              true distribution on a synthetic molecule-like dataset, and generates more valid molecules on QM9.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2204.05249"
                   target="_blank">
                  Learning Local Equivariant Representations for Large-Scale Atomistic Dynamics
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                3rd of May 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J. Owen, Mordechai Kornbluth, Boris Kozinsky
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              The authors <a
              class="item-subtitle link-style-black"
              href="https://simonbatzner.github.io/"
              target="_blank">Simon Batzner</a> and <a
              class="item-subtitle link-style-black"
              href="https://bkoz.seas.harvard.edu/news/doe-csgf-albert-musaelian"
              target="_blank">Albert Musaelian</a>. Simon is 3rd year PhD student at Harvard interested in how
              to leverage Deep Learning to advance Molecular Simulation. Prior to joining Harvard, he obtained a
              Master’s Degree from MIT and spent a year conducting research at the NASA Armstrong Research Center.
              Albert Musaelian is a second-year PhD student in the MIR group at Harvard and a Department of Energy
              Computational Science Graduate Fellow. He works on the design of machine learning techniques for
              atomic-scale modeling.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              A simultaneously accurate and computationally efficient parametrization of the energy and atomic forces of
              molecules and materials is a long-standing goal in the natural sciences. In pursuit of this goal, neural
              message passing has lead to a paradigm shift by describing many-body correlations of atoms through
              iteratively passing messages along an atomistic graph. This propagation of information, however, makes
              parallel computation difficult and limits the length scales that can be studied. Strictly local
              descriptor-based methods, on the other hand, can scale to large systems but do not currently match the
              high accuracy observed with message passing approaches. This work introduces Allegro, a strictly local
              equivariant deep learning interatomic potential that simultaneously exhibits excellent accuracy and
              scalability of parallel computation. Allegro learns many-body functions of atomic coordinates using a
              series of tensor products of learned equivariant representations, but without relying on message passing.
              Allegro obtains improvements over state-of-the-art methods on the QM9 and revised MD-17 data sets. A
              single tensor product layer is shown to outperform existing deep message passing neural networks and
              transformers on the QM9 benchmark. Furthermore, Allegro displays remarkable generalization to
              out-of-distribution data. Molecular dynamics simulations based on Allegro recover structural and kinetic
              properties of an amorphous phosphate electrolyte in excellent agreement with first principles
              calculations. Finally, we demonstrate the parallel scaling of Allegro with a dynamics simulation of 100
              million atoms.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2202.03376"
                   target="_blank">
                  Message Passing Neural PDE Solvers (ICLR 2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                10th of May 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Johannes Brandstetter, Daniel Worrall, Max Welling
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              The two first authors: Professor <a
              class="item-subtitle link-style-black"
              href="https://www.jku.at/institut-fuer-machine-learning/ueber-uns/team/ass-prof-dr-johannes-brandstetter/"
              target="_blank">Johannes Brandstetter</a> who holds his professorship at JKU
              Linz, where he previously worked with Prof. Sepp Hochreiter. He is a guest researcher at the University of
              Amsterdam in Prof. Max Wellings's group and started working with Microsoft's molecular ML group. Also
              joining us is Dr <a
              class="item-subtitle link-style-black"
              href="https://danielewworrall.github.io/"
              target="_blank">Daniel Worrall</a> who is an ML researcher at UvA and a Sidney Sussex scholar at the
              University of Cambridge. Previously he was a PostDoc in Prof. Max Wellings group.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of
              research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which
              piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only
              generalize over a subset of properties to which a generic solver would be faced, including: resolution,
              topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this
              work, we build a solver, satisfying these properties, where all the components are based on neural message
              passing, replacing all heuristically designed components in the computation graph with backprop-optimized
              neural function approximators. We show that neural message passing solvers representationally contain some
              classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage
              stability in training autoregressive models, we put forward a method that is based on the principle of
              zero-stability, posing stability as a domain adaptation problem. We validate our method on various
              fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain
              topologies, discretization, etc. in 1D and 2D. Our model outperforms state-of-the-art numerical solvers in
              the low resolution regime in terms of speed and accuracy.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                17th of May 2022
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              Maybe you? If you want to present a paper (doesn't have to be yours) or you have someone who might be
              interested in presenting his work then just write me a message on
              <a href="https://join.slack.com/t/logag/shared_invite/zt-u0mbo1ec-zElmvd1oSCXGjXvxLSokvg" target="_blank"
                 class="link-style-green">Slack</a>
              or via email!
            </p>
          </div>

          <!--
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2201.12843"
                   target="_blank">
                  On Recoverability of Graph Neural Network Representations (2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                3rd of May 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Maxim Fishman, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Avi Mendelson
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper Maxim Fishman who is an MSc student in the Electrical and Computer Engineering
              Department of Technion, advised by Dr. Chaim Baskin and Prof. Avi Mendelson. Previously, he did his BSc in
              Computer Science and BSc in Physics at Technion. As an applied mathematician and engineer he works as a
              researcher and developer at Intel.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Despite their growing popularity, graph neural networks (GNNs) still have multiple unsolved problems,
              including finding more expressive aggregation methods, propagation of information to distant nodes, and
              training on large-scale graphs. Understanding and solving such problems require developing analytic tools
              and techniques. In this work, we propose the notion of recoverability, which is tightly related to
              information aggregation in GNNs, and based on this concept, develop the method for GNN embedding analysis.
              We define recoverability theoretically and propose a method for its efficient empirical estimation. We
              demonstrate, through extensive experimental results on various datasets and different GNN architectures,
              that estimated recoverability correlates with aggregation method expressivity and graph sparsification
              quality. Therefore, we believe that the proposed method could provide an essential tool for understanding
              the roots of the aforementioned problems, and potentially lead to a GNN design that overcomes them. The
              code to reproduce our experiments is available at this https URL
            </p>
          </div>
          -->

        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>
    </div>


    <div class="top-margin" id="previous">
      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" class="underline-title">
          <h1 class="left-titles">PREVIOUS PAPERS AND RECORDINGS</h1>
        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>


      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" fxLayout="column">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2202.13060"
                   target="_blank">
                  Graph Attention Retrospective (2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                19th of April 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Kimon Fountoulakis, Amit Levi, Shenghao Yang, Aseem Baranwal, Aukosh Jagannath
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGATRetro' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their <a class="item-subtitle link-style-black"
                         href="https://hannes-stark.com/assets/.pdf"
                         target="_blank">slides</a>. (86 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author Professor <a
                class="item-subtitle link-style-black"
                href="https://opallab.ca/team/"
                target="_blank">Kimon Fountoulakis</a> who works at the University of Waterloo and previously did a
                PostDoc at Berkeley after finishing his PhD at University of Edinburgh with Prof. Gondzio.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Graph-based learning is a rapidly growing sub-field of machine learning with applications in social
                networks, citation networks, and bioinformatics. One of the most popular type of models is graph
                attention
                networks. These models were introduced to allow a node to aggregate information from the features of
                neighbor nodes in a non-uniform way in contrast to simple graph convolution which does not distinguish
                the
                neighbors of a node. In this paper, we study theoretically this expected behaviour of graph attention
                networks. We prove multiple results on the performance of the graph attention mechanism for the problem
                of
                node classification for a contextual stochastic block model. Here the features of the nodes are obtained
                from a mixture of Gaussians and the edges from a stochastic block model where the features and the edges
                are coupled in a natural way. First, we show that in an "easy" regime, where the distance between the
                means of the Gaussians is large enough, graph attention maintains the weights of intra-class edges and
                significantly reduces the weights of the inter-class edges. As a corollary, we show that this implies
                perfect node classification independent of the weights of inter-class edges. However, a classical
                argument
                shows that in the "easy" regime, the graph is not needed at all to classify the data with high
                probability. In the "hard" regime, we show that every attention mechanism fails to distinguish
                intra-class
                from inter-class edges. We evaluate our theoretical results on synthetic and real-world data.
              </p>
            </div>

            <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

            <div class="item" fxLayout="column">
              <div fxLayout="row">
                <h3>
                  <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2202.13013"
                     target="_blank">
                    Sign and Basis Invariant Networks for Spectral Graph Representation Learning (2022)
                  </a></h3>
              </div>
              <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
                <div>
                  <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                  12th of April 2022
                </div>
                <div>
                  <a>
                    <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                    Derek Lim, Joshua Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, Stefanie Jegelka
                  </a>
                </div>
              </div>
              <div>
                <iframe ngClass.gt-sm="float-right" [src]='safeURLSignNet' allowfullscreen></iframe>
                <p class="paragraph">
                  <a class="slight-bold">Resources:</a>
                  Their <a class="item-subtitle link-style-black"
                           href="https://hannes-stark.com/assets/final_reading_group_slides.pdf"
                           target="_blank">slides</a>. (96 participants)
                </p>
                <p class="paragraph">
                  <a class="slight-bold">Speaker:</a>
                  First authors <a
                  class="item-subtitle link-style-black"
                  href="https://cptq.github.io/"
                  target="_blank">Derek Lim</a> and <a
                  class="item-subtitle link-style-black"
                  href="https://joshrobinson.mit.edu/"
                  target="_blank">Joshua Robinson</a>, who both are PhD students in the group of Prof. Stefanie Jegelka
                  at
                  MIT. Josh is also co-advised by Prof. Suvrit Sra and Derek previously presented ESAN in the reading
                  group!
                </p>
                <p class="no-margin">
                  <a class="slight-bold">Abstract:</a>
                  Many machine learning tasks involve processing eigenvectors derived from data. Especially valuable are
                  Laplacian eigenvectors, which capture useful structural information about graphs and other geometric
                  objects. However, ambiguities arise when computing eigenvectors: for each eigenvector v, the sign
                  flipped
                  −v is also an eigenvector. More generally, higher dimensional eigenspaces contain infinitely many
                  choices
                  of basis eigenvectors. These ambiguities make it a challenge to process eigenvectors and eigenspaces
                  in
                  a
                  consistent way. In this work we introduce SignNet and BasisNet -- new neural architectures that are
                  invariant to all requisite symmetries and hence process collections of eigenspaces in a principled
                  manner.
                  Our networks are universal, i.e., they can approximate any continuous function of eigenvectors with
                  the
                  proper invariances. They are also theoretically strong for graph representation learning -- they can
                  approximate any spectral graph convolution, can compute spectral invariants that go beyond message
                  passing
                  neural networks, and can provably simulate previously proposed graph positional encodings. Experiments
                  show the strength of our networks for learning spectral graph filters and learning graph positional
                  encodings.
                </p>
              </div>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2109.07103"
                   target="_blank">
                  Automatic Symmetry Discovery with Lie Algebra Convolutional Network (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                5th of April 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Nima Dehmamy, Robin Walters, Yanchen Liu, Dashun Wang, Rose Yu
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLLConv' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Prof. Dehmamy's <a class="item-subtitle link-style-black"
                                   href="https://hannes-stark.com/assets/L-conv-Long2-Automatic Symmetry Discovery with Lie Algebra Convolutional Network.pdf"
                                   target="_blank">slides</a> are coming soon. (81 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author Professor <a
                class="item-subtitle link-style-black"
                href="http://nimadehmamy.com/"
                target="_blank">Nima Dehmamy</a> who works at Northwestern University and earned his PhD in Physics at
                Boston University. His research involves AI in graph learning, using physics to understand optimization
                landscapes, and neuroscience.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Existing equivariant neural networks require prior knowledge of the symmetry group and discretization
                for
                continuous groups. We propose to work with Lie algebras (infinitesimal generators) instead of Lie
                groups.
                Our model, the Lie algebra convolutional network (L-conv) can automatically discover symmetries and does
                not require discretization of the group. We show that L-conv can serve as a building block to construct
                any group equivariant feedforward architecture. Both CNNs and Graph Convolutional Networks can be
                expressed as L-conv with appropriate groups. We discover direct connections between L-conv and physics:
                (1) group invariant loss generalizes field theory (2) Euler-Lagrange equation measures the robustness,
                and
                (3) equivariance leads to conservation laws and Noether current.These connections open up new avenues
                for
                designing more general equivariant networks and applying them to important problems in physical
                sciences.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2010.16103"
                   target="_blank">
                  Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                29th of March 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, Long Jin
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLlabelTrick' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Prof. Li's <a class="item-subtitle link-style-black"
                              href="https://hannes-stark.com/assets/DF-LT.pdf"
                              target="_blank">slides</a>. (45 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Professor <a
                class="item-subtitle link-style-black"
                href="https://sites.google.com/view/panli-purdue/home?authuser=0"
                target="_blank">Pan Li</a> from Purdue University. He obtained his Ph.D. in Electrical and
                Computer
                Engineering at the University of Illinois Urbana - Champaign before completing a one-year postdoc in the
                SNAP group at Stanford with Prof. Jure Leskovec. He works on principled GraphML and its mathematical
                foundations.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                In this paper, we provide a theory of using graph neural networks (GNNs) for multi-node representation
                learning (where we are interested in learning a representation for a set of more than one node). We know
                that GNN is designed to learn single-node representations. When we want to learn a node set
                representation
                involving multiple nodes, a common practice in previous works is to directly aggregate the multiple node
                representations learned by a GNN into a joint representation of the node set. In this paper, we show a
                fundamental constraint of such an approach, namely the inability to capture the dependence between nodes
                in the node set, and argue that directly aggregating individual node representations does not lead to an
                effective joint representation for multiple nodes. Then, we notice that a few previous successful works
                for multi-node representation learning, including SEAL, Distance Encoding, and ID-GNN, all used node
                labeling. These methods first label nodes in the graph according to their relationships with the target
                node set before applying a GNN. Then, the node representations obtained in the labeled graph are
                aggregated into a node set representation. By investigating their inner mechanisms, we unify these node
                labeling techniques into a single and most basic form, namely labeling trick. We prove that with
                labeling
                trick a sufficiently expressive GNN learns the most expressive node set representations, thus in
                principle
                can solve any joint learning tasks over node sets. Experiments on one important two-node representation
                learning task, link prediction, verified our theory. Our work establishes a theoretical foundation of
                using GNNs for joint prediction tasks over node sets.
              </p>
            </div>
          </div>


          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="http://vladlen.info/papers/neural-fields.pdf"
                   target="_blank">
                  Geometry Processing with Neural Fields (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                22nd of March 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Guandao Yang, Serge Belongie, Bharath Hariharan, Vladlen Koltun
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGeomProc' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Guandao's <a class="item-subtitle link-style-black"
                             href="https://hannes-stark.com/assets/NeurIPS2021-GPwithNIF-Mar22.pdf"
                             target="_blank">slides</a>. (67 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a
                class="item-subtitle link-style-black"
                href="https://www.guandaoyang.com/"
                target="_blank">Guandao Yang</a>, who is a a Computer Science PhD student at Cornell University, advised
                by Serge Belongie and Bharath Hariharan. His research interests include computer vision for augmented
                reality and 3D generation.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Most existing geometry processing algorithms use meshes as the default shape representation.
                Manipulating
                meshes, however, requires one to maintain high quality in the surface discretization. For example,
                changing the topology of a mesh usually requires additional procedures such as remeshing. This paper
                instead proposes the use of neural fields for geometry processing. Neural fields can compactly store
                complicated shapes without spatial discretization. Moreover, neural fields are infinitely
                differentiable,
                which allows them to be optimized for objectives that involve higher-order derivatives. This raises the
                question: can geometry processing be done entirely using neural fields? We introduce loss functions and
                architectures to show that some of the most challenging geometry processing tasks, such as deformation
                and
                filtering, can be done with neural fields. Experimental results show that our methods are on par with
                the
                well-established mesh-based methods without committing to a particular surface discretization. Code is
                available at <a
                class="item-subtitle link-style-black"
                href="https://github.com/stevenygd/NFGP"
                target="_blank">https://github.com/stevenygd/NFGP</a>.
              </p>
            </div>
          </div>


          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.02905"
                   target="_blank">
                  Geometric and Physical Quantities Improve E(3) Equivariant Message Passing (2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                15th of March 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, Max Welling
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLSEGNN' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their <a class="item-subtitle link-style-black"
                         href="https://hannes-stark.com/assets/SEGNN.pdf"
                         target="_blank">slides</a>. (104 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper professor <a
                class="item-subtitle link-style-black"
                href="https://www.jku.at/institut-fuer-machine-learning/ueber-uns/team/ass-prof-dr-johannes-brandstetter/"
                target="_blank">Johannes Brandstetter</a> who holds his
                professorship at JKU Linz, where he previously worked with Prof. Sepp Hochreiter. He is a guest
                researcher
                at the University of Amsterdam in Prof. Max Wellings's group and started working with Microsoft's
                molecular ML group. He has a physics and data science background and was one of the main researchers at
                CERN who made the first direct observation of the Higgs boson decayinginto pairs of fermions! Next to
                his
                impressive work on physics-informed ML he also works on RL, NLP, Few-Shot Learning, and Generative
                Modeling. Also joining us was Professor <a
                class="item-subtitle link-style-black"
                href="https://erikbekkers.bitbucket.io/"
                target="_blank">Erik Bekkers</a>!
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Including covariant information, such as position, force, velocity or spin is important in many tasks in
                computational physics and chemistry. We introduce Steerable E(3) Equivariant Graph Neural Networks
                (SEGNNs) that generalise equivariant graph networks, such that node and edge attributes are not
                restricted
                to invariant scalars, but can contain covariant information, such as vectors or tensors. This model,
                composed of steerable MLPs, is able to incorporate geometric and physical information in both the
                message
                and update functions. Through the definition of steerable node attributes, the MLPs provide a new class
                of
                activation functions for general use with steerable feature fields. We discuss ours and related work
                through the lens of equivariant non-linear convolutions, which further allows us to pin-point the
                successful components of SEGNNs: non-linear message aggregation improves upon classic linear (steerable)
                point convolutions; steerable messages improve upon recent equivariant graph networks that send
                invariant
                messages. We demonstrate the effectiveness of our method on several tasks in computational physics and
                chemistry and provide extensive ablation studies.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black"
                   href="https://www.chaitjo.com/post/deep-learning-for-routing-problems"
                   target="_blank">
                  Recent Advances in Deep Learning for Routing Problems (2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                8th of March 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Chaitanya K. Joshi, Rishabh Anand
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLRouting' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Chaitanya's <a class="item-subtitle link-style-black"
                               href="https://www.chaitjo.com/post/deep-learning-for-routing-problems/recent-advances-in-deep-learning-for-routing-problems.pdf"
                               target="_blank">slides</a>. (70 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Author of the blog post and associated paper <a
                class="item-subtitle link-style-black"
                href="https://www.chaitjo.com/"
                target="_blank">Chaitanya K. Joshi</a>, who is an incoming PhD student at the University
                of Cambridge, supervised by Prof. Pietro Liò. His research explores the intersection of Graph and
                Geometric Deep Learning with applications in biomedicine and drug discovery. He previously worked on
                Graph
                Neural Network architectures and applications in Combinatorial Optimization at the NTU Graph Deep
                Learning
                Lab and A*STAR, Singapore.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Developing Graph Neural Network-driven solvers for combinatorial optimization problems such as the
                Travelling Salesperson Problem have seen a surge of academic interest recently. This talk aims to serve
                as
                a whirlwind tour of this research area. We first presents a Neural Combinatorial Optimization pipeline
                that unifies several recently proposed models into one single framework. Through the lens of the
                pipeline,
                we analyze recent advances in deep learning for routing problems and provide new directions to stimulate
                future research.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2111.12128"
                   target="_blank">
                  On the Unreasonable Effectiveness of Feature propagation in Learning on Graphs with Missing Node
                  Features (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                1st of March 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Emanuele Rossi, Henry Kenlay, Maria I. Gorinova, Benjamin Paul Chamberlain, Xiaowen Dong, Michael
                  Bronstein
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLUnreasonableEffect' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Ema's <a class="item-subtitle link-style-black"
                         href="https://docs.google.com/presentation/d/11dAeJRalTI7K1YAxMNz_yElZ0lVO5Bw7n0LBqSd-OUY/edit?usp=sharing"
                         target="_blank">slides</a>. (69 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a
                class="item-subtitle link-style-black"
                href="https://www.emanuelerossi.co.uk/"
                target="_blank">Emanuele Rossi</a> who is a Machine Learning Researcher at Twitter and a Ph.D.
                student at Imperial College London, working on Graph Neural Networks and supervised by Prof. Michael
                Bronstein. His research interests span a wide array of topics around graph neural networks, including
                scalability, dynamic graphs, and learning with missing node features. Before his current position,
                Emanuele was working at Fabula AI, which was then acquired by Twitter in June 2019. Previously, he
                completed an MPhil at the University of Cambridge and a BEng at Imperial College London, both in
                Computer
                Science.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                While Graph Neural Networks (GNNs) have recently become the de facto standard for modeling relational
                data, they impose a strong assumption on the availability of the node or edge features of the graph. In
                many real-world applications, however, features are only partially available; for example, in social
                networks, age and gender are available only for a small subset of users. We present a general approach
                for
                handling missing features in graph machine learning applications that is based on minimization of the
                Dirichlet energy and leads to a diffusion-type differential equation on the graph. The discretization of
                this equation produces a simple, fast and scalable algorithm which we call Feature Propagation. We
                experimentally show that the proposed approach outperforms previous methods on seven common
                node-classification benchmarks and can withstand surprisingly high rates of missing features: on average
                we observe only around 4% relative accuracy drop when 99% of the features are missing. Moreover, it
                takes
                only 10 seconds to run on a graph with ∼2.5M nodes and ∼123M edges on a single GPU.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.04941"
                   target="_blank">
                  Symmetric Spaces for Graph Embeddings: A Finsler-Riemannian Approach (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                22nd of February 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Federico López, Beatrice Pozzetti, Steve Trettel, Michael Strube, Anna Wienhard
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLSymSpaces' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper <a
                class="item-subtitle link-style-black"
                href="https://fedelopez77.github.io/"
                target="_blank">Federico López</a>, who is a Ph.D candidate in HITS at Heidelberg University. Prior to
                this, he studied Software Engineering at the University of Buenos Aires.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Learning faithful graph representations as sets of vertex embeddings has become a fundamental
                intermediary
                step in a wide range of machine learning applications. We propose the systematic use of symmetric spaces
                in representation learning, a class encompassing many of the previously used embedding targets. This
                enables us to introduce a new method, the use of Finsler metrics integrated in a Riemannian optimization
                scheme, that better adapts to dissimilar structures in the graph. We develop a tool to analyze the
                embeddings and infer structural properties of the data sets. For implementation, we choose Siegel
                spaces,
                a versatile family of symmetric spaces. Our approach outperforms competitive baselines for graph
                reconstruction tasks on various synthetic and real-world datasets. We further demonstrate its
                applicability on two downstream tasks, recommender systems and node classification.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.14012"
                   target="_blank">
                  Graph Posterior Network: Bayesian Predictive Uncertainty for Node Classification (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                15th of February 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Maximilian Stadler, Bertrand Charpentier, Simon Geisler, Daniel Zügner, Stephan Günnemann
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGraphPostNet' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their <a class="item-subtitle link-style-black"
                         href="https://hannes-stark.com/assets/.pdf"
                         target="_blank">slides</a> are coming soon. (83 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First authors <a
                class="item-subtitle link-style-black"
                href="https://maximilian-stadler.de/"
                target="_blank">Maximilian Stadler</a> and <a
                class="item-subtitle link-style-black"
                href="https://www.in.tum.de/en/daml/team/bertrand-charpentier/"
                target="_blank">Bertrand Charpentier</a>. Maximilian received his M.Sc.
                with high distinction from the Technical University of Munich. He wrote his thesis in the group of Prof.
                Günnemann. In this group, Bertrand is a PhD candidate working on uncertainty estimation and ML for
                graphs
                in this group.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                The interdependence between nodes in graphs is key to improve class predictions on nodes and utilized in
                approaches like Label Propagation (LP) or in Graph Neural Networks (GNN). Nonetheless, uncertainty
                estimation for non-independent node-level predictions is under-explored. In this work, we explore
                uncertainty quantification for node classification in three ways: (1) We derive three axioms explicitly
                characterizing the expected predictive uncertainty behavior in homophilic attributed graphs. (2) We
                propose a new model Graph Posterior Network (GPN) which explicitly performs Bayesian posterior updates
                for
                predictions on interdependent nodes. GPN provably obeys the proposed axioms. (3) We extensively evaluate
                GPN and a strong set of baselines on semi-supervised node classification including detection of
                anomalous
                features, and detection of left-out classes. GPN outperforms existing approaches for uncertainty
                estimation in the experiments.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2111.14522"
                   target="_blank">
                  Understanding over-squashing and bottlenecks on graphs via curvature (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                8th of February 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, Michael M. Bronstein
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLUnderstandingOversquashing' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their <a class="item-subtitle link-style-black"
                         href="https://hannes-stark.com/assets/understanding_oversquashing_slides_feb22.pdf"
                         target="_blank">slides</a>. (95 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Jake Topping and Dr. <a class="item-subtitle link-style-black"
                                        href="https://scholar.google.com/citations?user=yzjjeqsAAAAJ&hl=en"
                                        target="_blank">Francesco Di Giovanni</a>. Jake is a PhD student at Oxford
                working with Prof. Xiaowen Dong and Prof. Michael
                Bronstein. Francesco obtained his PhD from University College London and is working together with Ben at
                Twitter Research.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Most graph neural networks (GNNs) use the message passing paradigm, in which node features are
                propagated
                on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as
                a
                factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This
                phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks
                where
                the number of k-hop neighbors grows rapidly with k. We provide a precise description of the
                over-squashing
                phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we
                introduce
                a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the
                over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to
                alleviate the over-squashing.
              </p>
            </div>
          </div>
          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.14961"
                   target="_blank">
                  Roto-translated Local Coordinate Frames For Interacting Dynamical Systems (NeurIPS 2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                1st of February 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Miltiadis Kofinas, Naveen Shankar Nagaraja, Efstratios Gavves
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLLoCS' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Miltiadis's <a class="item-subtitle link-style-black"
                               href="https://hannes-stark.com/assets/Kofinas_LoCS_LoGaG_presentation_1_February_2022.pdf"
                               target="_blank">slides</a>. (50 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper <a class="item-subtitle link-style-black"
                                             href="https://www.linkedin.com/in/miltiadiskofinas/"
                                             target="_blank">Miltiadis Kofinas</a> who is a PhD student at the
                University of Amsterdam.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Modelling interactions is critical in learning complex dynamical systems, namely systems of interacting
                objects with highly non-linear and time-dependent behaviour. A large class of such systems can be
                formalized as geometric graphs, i.e., graphs with nodes positioned in the Euclidean space given an
                arbitrarily chosen global coordinate system, for instance vehicles in a traffic scene. Notwithstanding
                the
                arbitrary global coordinate system, the governing dynamics of the respective dynamical systems are
                invariant to rotations and translations, also known as Galilean invariance. As ignoring these
                invariances
                leads to worse generalization, in this work we propose local coordinate frames per node-object to induce
                roto-translation invariance to the geometric graph of the interacting dynamical system. Further, the
                local
                coordinate frames allow for a natural definition of anisotropic filtering in graph neural networks.
                Experiments in traffic scenes, 3D motion capture, and colliding particles demonstrate that the proposed
                approach comfortably outperforms the recent state-of-the-art.
              </p>
            </div>
          </div>
          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2101.10050"
                   target="_blank">
                  Learning Parametrised Graph Shift Operators (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                25t of January 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  George Dasoulas, Johannes Lutzeyer, Michalis Vazirgiannis
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLPGSO' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their <a class="item-subtitle link-style-black"
                         href="https://hannes-stark.com/assets/LoGaG_PGSO_slides.pdf"
                         target="_blank">slides</a>. (51 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Paper author <a class="item-subtitle link-style-black"
                                href="https://gdasoulas.github.io/"
                                target="_blank">George Dasoulas</a>, who is a final year PhD
                student in both the Data Science and Mining group at the Ecole Polytechnique and Noah’s Ark Lab of
                Huawei
                Technologies France. Under the supervision of Prof. Michalis Vazirgiannis, he works in the field of
                Graph
                Representation Learning, studying the expressive power of Graph Neural Networks (GNNs). Using the gained
                theoretical understanding of GNN architectures, his goal is to make impactful contributions to
                real-world
                problems that are characterized by an inherent graph structure ranging from communication graphs to
                molecular networks. Dr. <a class="item-subtitle link-style-black"
                                           href="https://scholar.google.com/citations?user=OfT4ns8AAAAJ&hl=en"
                                           target="_blank">Johannes Lutzeyer</a> is currently a postdoctoral researcher
                in
                the Data Science and
                Mining group at the Ecole Polytechnique. He works in the field of Graph Representation Learning with a
                focus on Graph Neural Networks and their theoretical analysis. He completed his PhD thesis at Imperial
                College London, where he studied the spectral properties of different graph representation matrices such
                as the adjacency and Laplacian matrices.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                In many domains data is currently represented as graphs and therefore, the graph representation of this
                data becomes increasingly important in machine learning. Network data is, implicitly or explicitly,
                always
                represented using a graph shift operator (GSO) with the most common choices being the adjacency,
                Laplacian
                matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where
                specific parameter values result in the most commonly used GSOs and message-passing operators in graph
                neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are
                used
                in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included
                in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors
                independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are
                shown
                to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they
                are found to automatically replicate the GSO regularisation found in the literature. On several
                real-world
                datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in
                both node- and graph-classification tasks.
              </p>
            </div>
          </div>


          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.14056" target="_blank">
                  How to transfer algorithmic reasoning knowledge to learn new algorithms?</a> + <a
                class="item-subtitle link-style-black"
                href="https://arxiv.org/abs/2110.05442" target="_blank">
                Neural Algorithmic Reasoners are Implicit Planners (2021)</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                18th of January 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  First paper: Louis-Pascal A. C. Xhonneux, Andreea Deac, Petar Velickovic, Jian Tang
                </a>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Second paper: Andreea Deac, Petar Veličković, Ognjen Milinković, Pierre-Luc Bacon, Jian Tang, Mladen
                  Nikolić
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLNAR' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their <a class="item-subtitle link-style-black"
                         href="https://hannes-stark.com/assets/2021-NeurIPS-graph-neural-cellular-automata-long.pdf"
                         target="_blank">slides </a>. (85 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speakers:</a>

                First authors of the papers <a href="https://www.linkedin.com/in/louis-pascal-xhonneux-3a85a2141/"
                                               target="_blank"
                                               class="link-style-green">Luis-Pascal Xhonneux</a> and <a
                href="https://andreeadeac22.github.io/" target="_blank"
                class="link-style-green">Andreea-Ioana Deac</a>.
                Luis-Pascal did his master's at Cambridge and is now a PhD student at Mila. Andreea is a PhD student
                in Machine Learning at Mila, with Prof Jian Tang. She is broadly interested in how learning can be
                improved through the use of graph representations, having previously worked on algorithmic alignment
                for implicit planning and applications to biotechnology (drug discovery and drug combinations).
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract Paper 1:</a>
                Learning to execute algorithms is a fundamental problem that has been widely studied. Prior work has
                shown that to enable systematic generalisation on graph algorithms it is critical to have access to the
                intermediate steps of the program/algorithm. In many reasoning tasks, where algorithmic-style reasoning
                is important, we only have access to the input and output examples. Thus, inspired by the success of
                pre-training on similar tasks or data in Natural Language Processing (NLP) and Computer Vision, we set
                out to study how we can transfer algorithmic reasoning knowledge. Specifically, we investigate how we
                can use algorithms for which we have access to the execution trace to learn to solve similar tasks for
                which we do not. We investigate two major classes of graph algorithms, parallel algorithms such as
                breadth-first search and Bellman-Ford and sequential greedy algorithms such as Prim and Dijkstra. Due to
                the fundamental differences between algorithmic reasoning knowledge and feature extractors such as used
                in Computer Vision or NLP, we hypothesise that standard transfer techniques will not be sufficient to
                achieve systematic generalisation. To investigate this empirically we create a dataset including 9
                algorithms and 3 different graph types. We validate this empirically and show how instead multi-task
                learning can be used to achieve the transfer of algorithmic reasoning knowledge.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract Paper 2:</a>
                Implicit planning has emerged as an elegant technique for combining learned models of the world with
                end-to-end model-free reinforcement learning. We study the class of implicit planners inspired by value
                iteration, an algorithm that is guaranteed to yield perfect policies in fully-specified tabular
                environments. We find that prior approaches either assume that the environment is provided in such a
                tabular form -- which is highly restrictive -- or infer "local neighbourhoods" of states to run value
                iteration over -- for which we discover an algorithmic bottleneck effect. This effect is caused by
                explicitly running the planning algorithm based on scalar predictions in every state, which can be
                harmful to data efficiency if such scalars are improperly predicted. We propose eXecuted Latent Value
                Iteration Networks (XLVINs), which alleviate the above limitations. Our method performs all planning
                computations in a high-dimensional latent space, breaking the algorithmic bottleneck. It maintains
                alignment with value iteration by carefully leveraging neural graph-algorithmic reasoning and
                contrastive self-supervised learning. Across eight low-data settings -- including classical control,
                navigation and Atari -- XLVINs provide significant improvements to data efficiency against value
                iteration-based implicit planners, as well as relevant model-free baselines. Lastly, we empirically
                verify that XLVINs can closely align with value iteration.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.14237"
                   target="_blank">
                  Learning Graph Cellular Automata (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                11th of January 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Daniele Grattarola, Lorenzo Livi, Cesare Alippi
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGCA' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Daniele's <a class="item-subtitle link-style-black"
                             href="https://hannes-stark.com/assets/2021-NeurIPS-graph-neural-cellular-automata-long.pdf"
                             target="_blank">slides </a>. (90 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper Dr. <a class="item-subtitle link-style-black"
                                                 href="https://danielegrattarola.github.io/"
                                                 target="_blank">Daniele Grattarola</a> who is a post-doc researcher at
                EPFL, working in collaboration with A. Loukas, B. Correia, P. Vandergheynst and M. Bronstein. He
                obtained
                his PhD working in
                Graph Machine Learning Group at the Università della Svizzera Italiana in 2021. His research is on graph
                neural networks and their applications to dynamical systems and computational biology, specifically for
                protein design.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Cellular automata (CA) are a class of computational models that exhibit rich dynamics emerging from the
                local interaction of cells arranged in a regular lattice. In this work we focus on a generalised version
                of typical CA, called graph cellular automata (GCA), in which the lattice structure is replaced by an
                arbitrary graph. In particular, we extend previous work that used convolutional neural networks to learn
                the transition rule of conventional CA and we use graph neural networks to learn a variety of transition
                rules for GCA. First, we present a general-purpose architecture for learning GCA, and we show that it
                can
                represent any arbitrary GCA with finite and discrete state space. Then, we test our approach on three
                different tasks: 1) learning the transition rule of a GCA on a Voronoi tessellation; 2) imitating the
                behaviour of a group of flocking agents; 3) learning a rule that converges to a desired target state.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.06935"
                   target="_blank">
                  Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                4th of January 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, Jian Tang
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLNBFNets' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Zhaocheng's <a class="item-subtitle link-style-black"
                               href="https://hannes-stark.com/assets/NBFNetLoGaG.pdf"
                               target="_blank">slides </a>. (77 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a class="item-subtitle link-style-black"
                                href="https://kiddozhu.github.io/"
                                target="_blank">Zhaocheng Zhu</a>, who is a third-year Ph.D.
                candidate at Mila - Quebec AI Institute, University of Montreal, advised by Prof. Jian Tang. He works on
                graph representation learning, machine learning systems, and drug discovery.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Link prediction is a very fundamental task on graphs. Inspired by traditional path-based methods, in
                this
                paper we propose a general and flexible representation learning framework based on paths for link
                prediction. Specifically, we define the representation of a pair of nodes as the generalized sum of all
                path representations, with each path representation as the generalized product of the edge
                representations
                in the path. Motivated by the Bellman-Ford algorithm for solving the shortest path problem, we show that
                the proposed path formulation can be efficiently solved by the generalized Bellman-Ford algorithm. To
                further improve the capacity of the path formulation, we propose the Neural Bellman-Ford Network
                (NBFNet),
                a general graph neural network framework that solves the path formulation with learned operators in the
                generalized Bellman-Ford algorithm. The NBFNet parameterizes the generalized Bellman-Ford algorithm with
                3
                neural components, namely INDICATOR, MESSAGE and AGGREGATE functions, which corresponds to the boundary
                condition, multiplication operator, and summation operator respectively. The NBFNet is very general,
                covers many traditional path-based methods, and can be applied to both homogeneous graphs and
                multi-relational graphs (e.g., knowledge graphs) in both transductive and inductive settings.
                Experiments
                on both homogeneous graphs and knowledge graphs show that the proposed NBFNet outperforms existing
                methods
                by a large margin in both transductive and inductive settings, achieving new state-of-the-art results.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black"
                   href="https://physical-reasoning.github.io/assets/pdf/papers/06.pdf"
                   target="_blank">
                  Learning Graph Search Heuristics (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                28th of December 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Michal Pándy, Rex Ying, Gabriele Corso, Petar Veličković, Jure Leskovec, Pietro Liò
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGraphSearch' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Michal's <a class="item-subtitle link-style-black" href="https://youtu.be/FpXIGTFD8Qs"
                            target="_blank">slides </a> should be coming soon! (46 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper <a class="item-subtitle link-style-black"
                                             href="https://mpmisko.github.io/"
                                             target="_blank">Michal Pándy</a> who is an incoming ML Engineer at Waymo.
                He
                will join Prof. Shimon Whiteson 's team to work on self-driving cars. He did his master's at the
                University of Cambridge and is currently a research intern at Google Research.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Searching for a path between two nodes in a graph is one of the most well-studied and fundamental
                problems
                in computer science. In numerous domains such as robotics, AI, or biology, practitioners develop search
                heuristics to accelerate their pathfinding algorithms. However, it is a laborious and complex process to
                hand-design heuristics based on the problem and the structure of a given use case. Here we present PHIL
                (Path Heuristic with Imitation Learning), a novel neural architecture and a training algorithm for
                discovering graph search and navigation heuristics from data by leveraging recent advances in imitation
                learning and graph representation learning. At training time, we aggregate datasets of search
                trajectories
                and ground-truth shortest path distances, which we use to train a specialized graph neural network-based
                heuristic function using backpropagation through steps of the pathfinding process. Our heuristic
                function
                learns graph embeddings useful for inferring node distances, runs in constant time independent of graph
                sizes, and can be easily incorporated in an algorithm such as A* at test time. Experiments show that
                PHIL
                reduces the number of explored nodes compared to state-of-the-art methods on benchmark datasets by 40.8%
                on average and allows for fast planning in time-critical robotics domains.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.04624"
                   target="_blank">
                  Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                21st of December 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Wengong Jin, Jeremy Wohlwend, Regina Barzilay, Tommi Jaakkola
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLIterativeGNN' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                I recommend <a class="item-subtitle link-style-black" href="https://youtu.be/FpXIGTFD8Qs"
                               target="_blank">this lecture</a> on immunology to learn a bit about antibodies. (61
                participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author Dr. <a class="item-subtitle link-style-black" href="http://people.csail.mit.edu/wengong/"
                                    target="_blank">Wengong Jin</a> who is a Postdoctoral Associate at Eric and Wendy
                Schmidt Center of the Broad
                Institute. He finished his Ph.D. in MIT CSAIL, advised by <a class="item-subtitle link-style-black"
                                                                             href="https://www.regina.csail.mit.edu/"
                                                                             target="_blank">Regina Barzilay</a> and <a
                class="item-subtitle link-style-black" href="https://people.csail.mit.edu/tommi/"
                target="_blank">Tommi Jaakkola</a>. He develops
                novel machine learning algorithms for biology, including drug discovery, immunology, genetic
                engineering,
                and synthetic biology. He is particularly interested in deep generative models and graph neural
                networks.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Antibodies are versatile proteins that bind to pathogens like viruses and stimulate the adaptive immune
                system. The specificity of antibody binding is determined by complementarity-determining regions (CDRs)
                at
                the tips of these Y-shaped proteins. In this paper, we propose a generative model to automatically
                design
                the CDRs of antibodies with enhanced binding specificity or neutralization capabilities. Previous
                generative approaches formulate protein design as a structure-conditioned sequence generation task,
                assuming the desired 3D structure is given a priori. In contrast, we propose to co-design the sequence
                and
                3D structure of CDRs as graphs. Our model unravels a sequence autoregressively while iteratively
                refining
                its predicted global structure. The inferred structure in turn guides subsequent residue choices. For
                efficiency, we model the conditional dependence between residues inside and outside of a CDR in a
                coarse-grained manner. Our method achieves superior log-likelihood on the test set and outperforms
                previous baselines in designing antibodies capable of neutralizing the SARS-CoV-2 virus.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.02910"
                   target="_blank">
                  Equivariant Subgraph Aggregation Networks (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                14th of December 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath
                  Balamurugan, Michael M. Bronstein, Haggai Maron
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLESAN' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their
                <a href="https://hannes-stark.com/assets/ESAN_Reading_Group_Slides.pdf"
                   target="_blank"
                   class="link-style-green">slides</a>.
                (35 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speakers:</a>
                Joint first authors <a class="item-subtitle link-style-black"
                                       href="https://www.linkedin.com/in/beabevi/"
                                       target="_blank">Beatrice Bevilacqua</a>, <a
                class="item-subtitle link-style-black"
                href="https://scholar.google.com/citations?user=PT2CDA4AAAAJ&hl=en"
                target="_blank">Fabrizio Frasca</a>,
                and <a class="item-subtitle link-style-black" href="https://cptq.github.io/"
                       target="_blank">Derek Lim</a>. Beatrice is a PhD Student at
                Purdue University with Prof. Bruno Ribeiro. Fabrizio is a PhD candidate at Imperial College London
                supervised by Prof. Michael Bronstein and he works as an ML researcher at Twitter. Derek is a PhD
                student
                at MIT under the supervision of Prof. Stefanie Jegelka.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Message-passing neural networks (MPNNs) are the leading architecture for deep learning on
                graph-structured
                data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these
                architectures are limited in their expressive power. This paper proposes a novel framework called
                Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that
                while
                two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we
                propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process
                it
                using a suitable equivariant architecture. We develop novel variants of the 1-dimensional
                Weisfeiler-Leman
                (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of
                these
                new WL variants. We further prove that our approach increases the expressive power of both MPNNs and
                more
                expressive architectures. Moreover, we provide theoretical results that describe how design choices such
                as the subgraph selection policy and equivariant neural architecture affect our architecture's
                expressive
                power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can
                be
                viewed as a stochastic version of our framework. A comprehensive set of experiments on real and
                synthetic
                datasets demonstrates that our framework improves the expressive power and overall performance of
                popular
                GNN architectures.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.07875"
                   target="_blank">
                  Graph Neural Networks with Learnable Structural and Positional Representations</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                30th of November 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, Xavier Bresson
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLLSPE' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Vijay's
                <a href="https://hannes-stark.com/assets/VPDwivedi_GNN_LSPE_LoGaG_2021.pdf"
                   target="_blank"
                   class="link-style-green">slides</a>.
                (71 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a class="item-subtitle link-style-black" href="https://vijaydwivedi.com.np/"
                                target="_blank">Vijay Prakash Dwivedi</a> who is a PhD student in Machine Learning at
                Nanyang Technological
                University, Singapore working with Prof. <a class="item-subtitle link-style-black"
                                                            href="https://tuanluu.github.io/"
                                                            target="_blank">Luu Anh Tuan</a> (NTU) and Prof. <a
                class="item-subtitle link-style-black"
                href="https://scholar.google.com/citations?user=9pSK04MAAAAJ&hl=en"
                target="_blank">Xavier Bresson</a> (NUS). His primary
                interest is ML for graph-structured data and he has contributed in "<a
                class="item-subtitle link-style-black" href="https://arxiv.org/abs/2003.00982"
                target="_blank">Benchmarking Graph Neural Networks</a>"
                and “<a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2012.09699"
                        target="_blank">A Generalization of Transformer Networks to Graphs</a>” as part of his research.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been
                applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and
                natural language processing. A major issue with arbitrary graphs is the absence of canonical positional
                information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic
                nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding
                (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian
                eigenvectors. In this work, we propose to decouple structural and positional representations to make
                easy
                for the network to learn these two essential properties. We introduce a novel generic architecture which
                we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and
                fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from
                2.87% up to 64.14% when considering learnable PE for both GNN classes.
              </p>
            </div>
          </div>
          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://openreview.net/forum?id=Wi5KUNlqWty"
                   target="_blank">
                  How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                23rd of November 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Dongkwan Kim, Alice Oh
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Dongkwan's
              <a href="https://hannes-stark.com/assets/211124_SuperGAT@LoGaG.pdf"
                 target="_blank"
                 class="link-style-green">slides</a>.
              (48 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author <a class="item-subtitle link-style-black" href="https://dongkwan-kim.github.io/"
                              target="_blank">Dongkwan
              Kim</a> who is a Ph.D. student at KAIST School of Computing, advised by <a
              class="item-subtitle link-style-black" href="https://aliceoh9.github.io/" target="_blank">Alice Oh</a>.
              His
              research interest is graph representation learning models and applications to
              social networks, code, or molecules.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor
              nodes for better representation. However, what graph attention learns is not understood well,
              particularly
              when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT),
              an
              improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible
              with a self-supervised task to predict edges, whose presence and absence contain the inherent
              information
              about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more
              expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence
              the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our
              recipe
              provides guidance on which attention design to use when those two graph characteristics are known. Our
              experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of
              them,
              and our models designed by recipe show improved performance over baselines.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/1905.12560"
                   target="_blank">
                  On the equivalence between graph isomorphism testing and function approximation with GNNs
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                16th of November 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Zhengdao Chen, Soledad Villar, Lei Chen, Joan Bruna
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Zhengdao's
              <a href="https://hannes-stark.com/assets/Slides_Zhengdao_Chen_LoGaG_11162021_2.pdf"
                 target="_blank"
                 class="link-style-green">slides</a>.
              (58 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>

              Joint authors of the paper <a class="item-subtitle link-style-black"
                                            href="https://cims.nyu.edu/~chenzh/">Zhengdao Chen</a> and Prof. <a
              class="item-subtitle link-style-black" href="https://www.ams.jhu.edu/villar/">Soledad Villar</a>. Zhengdao
              is a PhD
              student in Mathematics at New York University. Prof. Villar holds her position at the
              <a
                class="item-subtitle link-style-black" href="https://engineering.jhu.edu/ams/">Department of Applied
                Mathematics & Statistics</a>, and <a
              class="item-subtitle link-style-black" href="https://www.minds.jhu.edu/">Mathematical Institute for Data
              Science</a> at Johns Hopkins University. She is interested in ML, optimization, graph
              representation learning, and GNNs.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Graph neural networks (GNNs) have achieved lots of success on graph-structured data. In the light of this,
              there has been increasing interest in studying their representation power. One line of work focuses on the
              universal approximation of permutation-invariant functions by certain classes of GNNs, and another
              demonstrates the limitation of GNNs via graph isomorphism tests.
              Our work connects these two perspectives and proves their equivalence. We further develop a framework of
              the representation power of GNNs with the language of sigma-algebra, which incorporates both viewpoints.
              Using this framework, we compare the expressive power of different classes of GNNs as well as other
              methods on graphs. In particular, we prove that order-2 Graph G-invariant networks fail to distinguish
              non-isomorphic regular graphs with the same degree. We then extend them to a new architecture, Ring-GNNs,
              which succeeds on distinguishing these graphs and provides improvements on real-world social network
              datasets.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2109.04173" target="_blank">
                  Relating Graph Neural Networks to Structural Causal Model</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                9th of November 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Matej Zečević, Devendra Singh Dhami, Petar Veličković, Kristian Kersting
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGNNSCM' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Matej's
                <a href="https://hannes-stark.com/assets/MZ-NeuroCausality-Talk-LoGaG.pdf"
                   target="_blank"
                   class="link-style-green">slides</a>.
                (100 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a class="item-subtitle link-style-black" href="https://matej-zecevic.de/" target="_blank">
                Matej Zečević</a> who is a PhD candidate at TU Darmstadt working with <a
                class="item-subtitle link-style-black" href="https://ml-research.github.io/people/kkersting/index.html"
                target="_blank">
                Kristian Kersting</a> on
                Causality for ML. Before that, he completed his M.Sc. in Computer Science under <a
                class="item-subtitle link-style-black"
                href="https://www.ias.informatik.tu-darmstadt.de/Member/JanPeters"
                target="_blank">
                Jan Peters</a>, <a
                class="item-subtitle link-style-black" href="https://cifar.ca/bios/stefan-bauer/" target="_blank">
                Stefan Bauer</a>,
                and <a
                class="item-subtitle link-style-black" href="https://www.is.mpg.de/~bs" target="_blank">
                Bernhard Schölkopf</a> at MPI for Intelligent Systems (Tübingen).
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Causality can be described in terms of a structural causal model (SCM) that carries information on the
                variables of interest and their mechanistic relations. For most processes of interest the underlying SCM
                will only be partially observable, thus causal inference tries to leverage any exposed information.
                Graph
                neural networks (GNN) as universal approximators on structured input pose a viable candidate for causal
                learning, suggesting a tighter integration with SCM. To this effect we present a theoretical analysis
                from
                first principles that establishes a novel connection between GNN and SCM while providing an extended
                view
                on general neural-causal models. We then establish a new model class for GNN-based causal inference that
                is necessary and sufficient for causal effect identification. Our empirical illustration on simulations
                and standard benchmarks validate our theoretical proofs.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2102.06790" target="_blank">
                  A Unified Lottery Ticket Hypothesis for Graph Neural Networks</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                2nd of November 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, Zhangyang Wang
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLLTH' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Tianlong's
                <a href="https://hannes-stark.com/assets/Paper Reading Presentation in LoGaG.pdf"
                   target="_blank"
                   class="link-style-green">slides</a>.
                (54 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a class="item-subtitle link-style-black" href="https://tianlong-chen.github.io/about/"
                                target="_blank">Tianlong Chen (陈天龙)</a> who is a third-year Ph.D. student of Electrical
                and Computer Engineering
                (DICE) at VITA, The University of Texas at Austin, advised by Dr. <a
                class="item-subtitle link-style-black" href="https://vita-group.github.io/" target="_blank">Zhangyang
                (Atlas) Wang</a>. His research
                interests include AutoML, Adversarial Robustness, Self-Supervision, and Graph Neural Networks.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                With graphs rapidly growing in size and deeper graph neural networks (GNNs) emerging, the training and
                inference of GNNs become increasingly expensive. Existing network weight pruning algorithms cannot
                address
                the main space and computational bottleneck in GNNs, caused by the size and connectivity of the graph.
                To
                this end, this paper first presents a unified GNN sparsification (UGS) framework that simultaneously
                prunes the graph adjacency matrix and the model weights, for effectively accelerating GNN inference on
                large-scale graphs. Leveraging this new tool, we further generalize the recently popular lottery ticket
                hypothesis to GNNs for the first time, by defining a graph lottery ticket (GLT) as a pair of core
                sub-dataset and sparse sub-network, which can be jointly identified from the original GNN and the full
                dense graph by iteratively applying UGS. Like its counterpart in convolutional neural networks, GLT can
                be
                trained in isolation to match the performance of training with the full model and graph, and can be
                drawn
                from both randomly initialized and self-supervised pre-trained GNNs. Our proposal has been
                experimentally
                verified across various GNN architectures and diverse tasks, on both small-scale graph datasets (Cora,
                Citeseer and PubMed), and large-scale datasets from the challenging Open Graph Benchmark (OGB).
                Specifically, for node classification, our found GLTs achieve the same accuracies with 20%~98% MACs
                saving
                on small graphs and 25%~85% MACs saving on large ones. For link prediction, GLTs lead to 48%~97% and 70%
                MACs saving on small and large graph datasets, respectively, without compromising predictive
                performance.
                Codes available at <a class="item-subtitle link-style-black"
                                      href="https://github.com/VITA-Group/Unified-LTH-GNN" target="_blank">this https
                URL</a>.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.10934" target="_blank">
                  GRAND: Graph Neural Diffusion</a> + <a class="item-subtitle link-style-black"
                                                         href="https://arxiv.org/abs/2110.09443" target="_blank">
                Beltrami Flow and Neural Diffusion on Graphs</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                26th of October 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  First paper: Benjamin Paul Chamberlain, James Rowbottom, Maria Gorinova, Stefan Webb, Emanuele Rossi,
                  Michael M.
                  Bronstein
                </a>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Second paper: Benjamin Paul Chamberlain, James Rowbottom, Davide Eynard, Francesco Di Giovanni,
                  Xiaowen Dong, Michael M Bronstein
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGRAND' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Ben's and James's
                <a href="https://hannes-stark.com/assets/Graph_Neural_Networks_and_Diffusion_PDEs.pdf"
                   target="_blank"
                   class="link-style-green">slides</a>.
                (75 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Joint first authors of the paper
                <a href="https://www.linkedin.com/in/jamesrowbottom/" target="_blank"
                   class="link-style-green">James Rowbottom</a> and Dr. <a
                href="https://www.linkedin.com/in/benjaminchamberlain/" target="_blank"
                class="link-style-green">Benjamin Paul Chamberlain</a>.
                James received his master in AI with distinction from Imperial College London and worked on GRAND as an
                ML
                Reserch intern at Prof.
                <a href="https://www.imperial.ac.uk/people/m.bronstein" target="_blank"
                   class="link-style-green">Michael Bronstein's</a>
                graph ML research group at Twitter. Ben is a Machine Learning Researcher at Twitter who received his PhD
                from Imperial College London.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract GRAND:</a>
                We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous
                diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In
                our
                model, the layer structure and topology correspond to the discretisation choices of temporal and spatial
                operators. Our approach
                allows a principled development of a broad new class of GNNs that are able to address the common plights
                of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models
                are stability with respect to perturbations in the data and this is addressed for both implicit and
                explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve
                competitive results on many standard graph benchmarks.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract BLEND:</a>
                We propose a novel class of graph neural networks based on the discretised Beltrami flow, a
                non-Euclidean
                diffusion PDE. In our model, node features are supplemented with positional encodings derived from the
                graph topology and jointly evolved by the Beltrami flow, producing simultaneously continuous feature
                learning and topology evolution. The resulting model generalises many popular graph neural networks and
                achieves state-of-the-art results on several benchmarks.
              </p>
            </div>
          </div>
          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2006.05205" target="_blank">
                  On the Bottleneck of Graph Neural Networks and its Practical Implications</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                19th of October 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Uri Alon, Eran Yahav
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLoversquashing' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Uri's
                <a href="https://hannes-stark.com/assets/bottleneck-tum.pdf"
                   target="_blank"
                   class="link-style-green">slides</a>.
                (50 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper Dr. <a href="https://urialon.ml/" target="_blank"
                                                 class="link-style-green">Uri Alon</a> who obtained my PhD at Technion
                (Israel), advised by Prof. <a href="https://www.cs.technion.ac.il/~yahave/" target="_blank"
                                              class="link-style-green">Eran Yahav</a>. He is now a Postdoctoral
                Researcher
                at the Language Technologies Institute of Carnegie Mellon
                University, working with Prof. <a href="http://www.phontron.com/" target="_blank"
                                                  class="link-style-green">Graham Neubig</a> on NLP and learning from
                source code, where GNNs find many
                applications. My most recent GNN work includes "<a href="https://openreview.net/pdf?id=i80OPhOCVH2"
                                                                   target="_blank"
                                                                   class="link-style-green">On the Bottleneck of Graph
                Neural Networks and its
                Practical Implications</a>" and "<a href="https://arxiv.org/pdf/2105.14491.pdf" target="_blank"
                                                    class="link-style-green">How Attentive are Graph Attention
                Networks?</a>".
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008),
                one of the major problems in training GNNs was their struggle to propagate information between distant
                nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck
                when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially
                growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating
                from
                distant nodes and perform poorly when the prediction task depends on long-range interaction. In this
                paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck
                hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs
                that
                absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and
                GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems,
                suffers
                from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without
                any
                tuning or additional weights. Our code is available at this <a
                href="https://github.com/tech-srl/bottleneck/" target="_blank"
                class="link-style-green">https URL</a>.
              </p>
            </div>
          </div>


          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2102.05152" target="_blank">
                  On Explainability of Graph Neural Networks via Subgraph Explorations</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                12th of October 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, Shuiwang Ji
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Haiyang's
              <a href="https://hannes-stark.com/assets/On Explainability of Graph Neural Networks via Subgraph.pdf"
                 target="_blank"
                 class="link-style-green">slides</a>.
              (59 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              Author of the paper
              <a href="https://scholar.google.com/citations?user=LZKU1hUAAAAJ&hl=en" target="_blank"
                 class="link-style-green">Haiyang Yu</a> who is a PhD student
              at Texas A&M University supervised by Prof. <a href="http://people.tamu.edu/~sji/" target="_blank"
                                                             class="link-style-green">Shuiwang Ji</a>.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are
              considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes
              or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this
              work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs.
              Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently
              exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we
              propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions
              among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute
              Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying
              subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly
              improved explanations, while keeping computations at a reasonable level.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2010.09891" target="_blank">
                  FLAG: Adversarial Data Augmentation for Graph Neural Networks</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                28th of September 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, Tom Goldstein
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Kezhi's
              <a href="https://hannes-stark.com/assets/FLAG_slides.pdf" target="_blank"
                 class="link-style-green">slides</a>.
              (46 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author <a
              href="https://scholar.google.com/citations?user=MG46jrMAAAAJ&hl=en" target="_blank"
              class="link-style-green">Kezhi Kong</a> who is a PhD student at the University of Maryland. Advised by
              Prof. Tom Goldstein, he does research in Machine Learning, with a focus on Graph Learning and
              Adversarial
              Attacks/Defenses.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Data augmentation helps neural networks generalize better, but it remains an open question how to
              effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most
              existing graph regularizers focus on augmenting graph topological structures by adding/removing edges,
              we
              offer a novel direction to augment in the input node feature space for better performance. We propose a
              simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which
              iteratively augments node features with gradient-based adversarial perturbations during training, and
              boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code
              and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and
              in both transductive and inductive settings. Without modifying a model's architecture or training setup,
              FLAG yields a consistent and salient performance boost across both node and graph classification tasks.
              Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and
              ogbg-code
              datasets.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.03893" target="_blank">
                  Rethinking Graph Transformers with Spectral Attention</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                21st of September 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Devin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent Létourneau, Prudencio Tossou
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLSAN' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Devin's
                <a href="https://hannes-stark.com/assets/spectral_attention_networks.pdf" target="_blank"
                   class="link-style-green">slides</a>.
                (69 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Joint first authors of the paper: Dr.
                <a href="https://www.linkedin.com/in/dbeaini/" target="_blank"
                   class="link-style-green">Dominique Beaini</a> who obtained his PhD at École Polytechnique de
                Montréal
                and is now working as ML Researcher at Valence Discovery on using GNNs for molecules.
                And <a href="https://www.linkedin.com/in/devin-kreuzer-847b52b5/" target="_blank"
                       class="link-style-green">Devin Kreuzer</a> who is doing his masters at McGill University
                supervised
                by Prof. <a href="https://williamleif.github.io/" target="_blank"
                            class="link-style-green">William L. Hamilton</a> while working at MILA on GNNs and
                AI-enabled
                drug discovery.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                In recent years, the Transformer architecture has proven to be very successful in sequence processing,
                but
                its application to other data structures, such as graphs, has remained limited due to the difficulty
                of
                properly defining positions. Here, we present the Spectral Attention Network (SAN), which uses a
                learned
                positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position
                of
                each node in a given graph. This LPE is then added to the node features of the graph and passed to a
                fully-connected Transformer. By leveraging the full spectrum of the Laplacian, our model is
                theoretically
                powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance.
                Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an
                information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat
                transfer and electric interaction. When tested empirically on a set of 4 standard datasets, our model
                performs on par or better than state-of-the-art GNNs, and outperforms any attention-based model by a
                wide
                margin, becoming the first fully-connected architecture to perform well on graph benchmarks.
              </p>
            </div>
          </div>


          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2107.01952" target="_blank">
                  Partition and Code: learning how to compress graphs</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                14th of September 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Giorgos Bouritsas, Andreas Loukas, Nikolaos Karalias, Michael M. Bronstein
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Giorgos's slides as
              <a href="https://hannes-stark.com/assets/PnC_slides_logag.pdf" target="_blank"
                 class="link-style-green">pdf</a> or <a href="https://hannes-stark.com/assets/PnC_slides_logag.key"
                                                        target="_blank"
                                                        class="link-style-green">keynote</a>.
              (67 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              The main presenter will be first author <a
              href="https://www.linkedin.com/in/giorgos-bouritsas/?originalSubdomain=uk" target="_blank"
              class="link-style-green">Giorgos Bouritsas</a> who is a PhD student at Imperial College London
              under the supervision of Prof. <a
              href="https://www.imperial.ac.uk/people/m.bronstein" target="_blank"
              class="link-style-green">Michael Bronstein</a> and he is currently a visiting PhD at EPFL, Switzerland.
              Also joining us will be paper author <a
              href="https://andreasloukas.blog/" target="_blank"
              class="link-style-green">Dr. Andreas Loukas</a> who is a research scientist (Ambizione fellow) at
              the LTS2 lab in EPFL.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Can we use machine learning to compress graph data? The absence of ordering in graphs poses a
              significant
              challenge to conventional compression algorithms, limiting their attainable gains as well as their
              ability
              to discover relevant patterns. On the other hand, most graph compression approaches rely on
              domain-dependent handcrafted representations and cannot adapt to different underlying graph
              distributions.
              This work aims to establish the necessary principles a lossless graph compression method should follow
              to
              approach the entropy storage lower bound. Instead of making rigid assumptions about the graph
              distribution, we formulate the compressor as a probabilistic model that can be learned from data and
              generalise to unseen instances. Our "Partition and Code" framework entails three steps: first, a
              partitioning algorithm decomposes the graph into elementary structures, then these are mapped to the
              elements of a small dictionary on which we learn a probability distribution, and finally, an entropy
              encoder translates the representation into bits. All three steps are parametric and can be trained with
              gradient descent. We theoretically compare the compression quality of several graph encodings and prove,
              under mild conditions, a total ordering of their expected description lengths. Moreover, we show that,
              under the same conditions, PnC achieves compression gains w.r.t. the baselines that grow either linearly
              or quadratically with the number of vertices. Our algorithms are quantitatively evaluated on diverse
              real-world networks obtaining significant performance improvements with respect to different families of
              non-parametric and parametric graph compressors.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.12575" target="_blank">
                  Weisfeiler and Lehman Go Cellular: CW Networks</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                7th of September 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yu Guang Wang, Pietro Liò, Guido Montúfar, Michael
                  Bronstein
                </a>
              </div>
            </div>


            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLCWNetworks' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                The <a href="https://youtu.be/MTQGNVTn9lQ" target="_blank"
                       class="link-style-green">Recording</a> of the whole presentation.
                Cristian's and Fabrizio's slides
                <a href="https://crisbodnar.github.io/files/cwn_logag_talk.pdf" target="_blank"
                   class="link-style-green">Slides</a>. My
                <a
                  href="https://hannes-stark.com/assets/Weisfeiler and Lehman Go Cellular CW Networks.pdf"
                  target="_blank"
                  class="link-style-green">Paper Annotations</a>.
                (49 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Joint first authors of the paper
                <a href="https://crisbodnar.github.io/" target="_blank"
                   class="link-style-green">Cristian Bodnar</a> and <a
                href="https://scholar.google.com/citations?user=PT2CDA4AAAAJ&hl=en" target="_blank"
                class="link-style-green">Fabrizio Frasca</a>. Cristian is a second-year PhD student
                at Cambridge supervised by Prof. <a href="https://www.cl.cam.ac.uk/~pl219/" target="_blank"
                                                    class="link-style-green">Pietro Liò</a>. He works on topological
                and
                geometric deep learning. Fabrizio
                is a PhD candidate at Imperial College London supervised by Prof. <a
                href="https://www.imperial.ac.uk/people/m.bronstein" target="_blank"
                class="link-style-green">Michael Bronstein</a> and he works as an ML
                researcher at Twitter.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Graph Neural Networks (GNNs) are limited in their expressive power, struggle with long-range
                interactions
                and lack a principled way to model higher-order structures. These problems can be attributed to the
                strong
                coupling between the computational graph and the input graph structure. The recently proposed Message
                Passing Simplicial Networks naturally decouple these elements by performing message passing on the
                clique
                complex of the graph. Nevertheless, these models are severely constrained by the rigid combinatorial
                structure of Simplicial Complexes (SCs). In this work, we extend recent theoretical results on SCs to
                regular Cell Complexes, topological objects that flexibly subsume SCs and graphs. We show that this
                generalisation provides a powerful set of graph ``lifting'' transformations, each leading to a unique
                hierarchical message passing procedure. The resulting methods, which we collectively call CW Networks
                (CWNs), are strictly more powerful than the WL test and, in certain cases, not less powerful than the
                3-WL
                test. In particular, we demonstrate the effectiveness of one such scheme, based on rings, when applied
                to
                molecular graph problems. The proposed architecture benefits from provably larger expressivity than
                commonly used GNNs, principled modelling of higher-order signals and from compressing the distances
                between nodes. We demonstrate that our model achieves state-of-the-art results on a variety of
                molecular
                datasets.
              </p>

            </div>

          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://openreview.net/forum?id=-qh0M9XWxnv"
                   target="_blank">
                  Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                31st of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Muhammet Balcilar, Guillaume Renton, Pierre Héroux, Benoit Gaüzère, Sébastien Adam, Paul Honeine
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Muhammet's
              <a href="https://hannes-stark.com/assets/structural_spectral_awareness_GNN.pdf" target="_blank"
                 class="link-style-green">Slides</a>.
              (54 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper Dr.
              <a href="https://scholar.google.com.tr/citations?user=LRyde44AAAAJ&hl=en" target="_blank"
                 class="link-style-green">Muhammet Balcilar</a>
              who works as R&I Researcher at Interdigital, Rennes. He obtained his Ph.D. at Yildiz Technical
              University
              and has held several PostDoc positions since then.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              In the recent literature of Graph Neural Networks (GNN), the expressive power of models has been studied
              through their capability to distinguish if two given graphs are isomorphic or not. Since the graph
              isomorphism problem is NP-intermediate, and Weisfeiler-Lehman (WL) test can give sufficient but not
              enough
              evidence in polynomial time, the theoretical power of GNNs is usually evaluated by the equivalence of
              WL-test order, followed by an empirical analysis of the models on some reference inductive and
              transductive datasets. However, such analysis does not account the signal processing pipeline, whose
              capability is generally evaluated in the spectral domain. In this paper, we argue that a spectral
              analysis
              of GNNs behavior can provide a complementary point of view to go one step further in the understanding
              of
              GNNs. By bridging the gap between the spectral and spatial design of graph convolutions, we
              theoretically
              demonstrate some equivalence of the graph convolution process regardless it is designed in the spatial
              or
              the spectral domain. Using this connection, we managed to re-formulate most of the state-of-the-art
              graph
              neural networks into one common framework. This general framework allows to lead a spectral analysis of
              the most popular GNNs, explaining their performance and showing their limits according to spectral point
              of view. Our theoretical spectral analysis is confirmed by experiments on various graph databases.
              Furthermore, we demonstrate the necessity of high and/or band-pass filters on a graph dataset, while the
              majority of GNN is limited to only low-pass and inevitably it fails.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2103.06857" target="_blank">
                  Should Graph Neural Networks Use Features, Edges, Or Both?</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                24th of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Lukas Faber, Yifan Lu, Roger Wattenhofer
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a> My
              <a
                href="https://hannes-stark.com/assets/Should Graph Neural Networks Use Edges Features of Both.pdf"
                target="_blank"
                class="link-style-green">Paper Annotations</a>.
              (48 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper
              <a href="https://disco.ethz.ch/members/lfaber" target="_blank"
                 class="link-style-green">Lukas Faber</a>
              who is a PhD student in the Distributed Computing Group at ETH Zürich supervised by Prof. Roger
              Wattenhofer. He is also working at Google Zürich.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Graph Neural Networks (GNNs) are the first choice for learning algorithms on graph data. GNNs promise to
              integrate (i) node features as well as (ii) edge information in an end-to-end learning algorithm. How
              does
              this promise work out practically? In this paper, we study to what extend GNNs are necessary to solve
              prominent graph classification problems. We find that for graph classification, a GNN is not more than
              the
              sum of its parts. We also find that, unlike features, predictions with an edge-only model do not always
              transfer to GNNs.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.07594" target="_blank">
                  Graph Contrastive Learning Automated</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                17th of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Yuning You, Tianlong Chen, Yang Shen, Zhangyang Wang
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a> My
              <a
                href="https://hannes-stark.com/assets/Graph Contrastive Learning Automated.pdf"
                target="_blank"
                class="link-style-green">Paper Annotations</a>.
              Yuning's
              <a href="https://hannes-stark.com/assets/yuning_LoGaG_talk.pdf" target="_blank"
                 class="link-style-green">Slides</a>.
              (32 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper
              <a href="https://yyou1996.github.io/" target="_blank" class="link-style-green">Yuning You</a>
              who is a third-year Ph.D. student in ECE at Texas A&M University supervised by Prof.
              <a href="https://shen-lab.github.io/" target="_blank" class="link-style-green">Yang Shen</a>,
              and unofficially co-supervised by Prof. Zhangyang Wang. He has done a lot of popular work on
              self-supervised learning on graphs.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Self-supervised learning on graph-structured data has drawn recent interest for learning generalizable,
              transferable and robust representations from unlabeled graphs. Among many, graph contrastive learning
              (GraphCL) has emerged with promising representation learning performance. Unfortunately, unlike its
              counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data augmentations, which have
              to be manually picked per dataset, by either rules of thumb or trial-and-errors, owing to the diverse
              nature of graph data. That significantly limits the more general applicability of GraphCL. Aiming to
              fill in this crucial gap, this paper proposes a unified bi-level optimization framework to
              automatically, adaptively and dynamically select data augmentations when performing GraphCL on specific
              graph data. The general framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as
              min-max optimization. The selections of augmentations made by JOAO are shown to be in general aligned
              with previous "best practices" observed from handcrafted tuning: yet now being automated, more flexible
              and versatile. Moreover, we propose a new augmentation-aware projection head mechanism, which will route
              output features through different projection heads corresponding to different augmentations chosen at
              each training step. Extensive experiments demonstrate that JOAO performs on par with or sometimes better
              than the state-of-the-art competitors including GraphCL, on multiple graph datasets of various scales
              and types, yet without resorting to any laborious dataset-specific tuning on augmentation selection. We
              release the code at this https URL.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2105.04550" target="_blank">
                  Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth</a>
              </h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                10th of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, Kenji Kawaguchi
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a> My
              <a
                href="https://hannes-stark.com/assets/Optimization%20of%20Graph%20Neural%20Networks%20Implicit%20Acceleration%20by%20Skip%20Connections%20and%20More%20Depth.pdf"
                target="_blank"
                class="link-style-green">Paper Annotations</a>.
              Keyulu's
              <a href="https://people.csail.mit.edu/keyulux/pdf/optimization.pdf" target="_blank"
                 class="link-style-green">Slides</a>.
              (50 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper Dr. <a href="https://people.csail.mit.edu/keyulux/" target="_blank"
                                               class="link-style-green">Keyulu Xu</a>: He received his Ph.D. in EECS
              from MIT, where he was affiliated
              with CSAIL and advised by <a href="https://people.csail.mit.edu/stefje/" target="_blank"
                                           class="link-style-green">Stefanie Jegelka</a>. His papers got multiple
              spotlights + orals and one was, for
              instance, the highest reviewed paper at ICLR 2021. Also joining us is paper author <a
              href="http://www.mozhi.umiacs.io/" target="_blank" class="link-style-green">Mozhi Zhang</a> who is a
              last year PhD student at the University of Maryland working with Jordan Boyd-Graber as advisor on
              generalization properties of neural networks among other topics.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Graph Neural Networks (GNNs) have been studied through the lens of expressive power and generalization.
              However, their optimization properties are less well understood. We take the first step towards
              analyzing
              GNN training by studying the gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that
              despite the non-convexity of training, convergence to a global minimum at a linear rate is guaranteed
              under mild assumptions that we validate on real world graphs. Second, we study what may affect the GNNs’
              training speed. Our results show that the training of GNNs is implicitly accelerated by skip
              connections,
              more depth, and/or a good label distribution. Empirical results confirm that our theoretical results for
              linearized GNNs align with the training behavior of nonlinear GNNs. Our results provide the first
              theoretical support for the success of GNNs with skip connections in terms of optimization, and suggest
              that deep GNNs with skip connections would be promising in practice.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.05234" target="_blank">
                  Do Transformers Really Perform Bad for Graph Representation?</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                3rd of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a> My
              <a
                href="https://hannes-stark.com/assets/Do%20Transformers%20Really%20Perform%20Bad%20for%20Graph%20Representation.pdf"
                target="_blank"
                class="link-style-green">Paper Annotations</a>.
              (35 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              Hey thats me!
              <a href="https://hannes-stark.com/" target="_blank"
                 class="link-style-green">Hannes Stärk</a>.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              The Transformer architecture has become a dominant choice in many domains, such as natural language
              processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards
              of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how
              Transformers could perform well for graph representation learning. In this paper, we solve this mystery
              by
              presenting Graphormer, which is built upon the standard Transformer architecture, and could attain
              excellent results on a broad range of graph representation learning tasks, especially on the recent OGB
              Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of
              effectively encoding the structural information of a graph into the model. To this end, we propose
              several
              simple yet effective structural encoding methods to help Graphormer better model graph-structured data.
              Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our
              ways
              of encoding the structural information of graphs, many popular GNN variants could be covered as the
              special cases of Graphormer.
            </p>
          </div>
        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>
    </div>
  </div>
</div>

