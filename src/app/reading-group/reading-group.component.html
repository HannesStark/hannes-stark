<div fxLayout="column" fxLayoutAlign="center center" xmlns="http://www.w3.org/1999/html">

  <div class="width100" fxLayout="row" fxLayout.lt-md="column" fxLayoutAlign="center center">
    <div fxFlex="35" fxLayoutAlign="end center" fxLayoutAlign.lt-md="center center">
      <img class="profile-image" src="assets/randomEuclidean.png" alt="Euclid Elements">
    </div>
    <div fxFlex="55" fxFlex.xs="65" ngClass.sm="small-margin" ngClass.xs="small-margin--xs">
      <h1 class="Title" ngClass.lt-md="text-align-center">LoGaG: Learning on Graphs and Geometry Reading Group!</h1>
      <!--
      <div class="underline-title--top" fxLayoutAlign.lt-md="center center">
        <h1 class="left-titles">M. Sc. Computer Science Student</h1>
      </div>
      -->
      <div class="item" fxLayout="column" fxLayoutAlign.lt-md="center center">
        <p class="text-align-left">
          Welcome to the LoGaG reading group where GraphML researchers present and discuss their papers every week! <br>
          Below
          are the <a (click)="scroll('Schedule')" class="link-style-green">Schedule</a> and <a
          (click)="scroll('previous')" class="link-style-green">previously</a> covered papers. I organize this reading
          group with the awesome help of <a
          href="https://www.valencediscovery.com/"
          target="_blank"
          class="link-style-green">Valence Discovery</a>!
        </p>

        <div fxLayout="row" fxLayoutAlign="left center">
          <p>
            <a mat-button class="group-button item-title"
               href="https://zoom.us/j/5775722530?pwd=ZzlGTXlDNThhUDZOdU4vN2JRMm5pQT09"
               target="_blank">Zoom Link</a>
          </p>
          <div class="next-to-button">
            Just hop on! We meet every Monday at 11am EDT / 3pm UTC / 5pm CEST / 8am PDT.
          </div>
        </div>

        <div fxLayout="row" fxLayoutAlign="left center">
          <p>
            <a mat-button class="group-button item-title"
               href="https://join.slack.com/t/logag/shared_invite/zt-u0mbo1ec-zElmvd1oSCXGjXvxLSokvg"
               target="_blank">Join Slack!</a>
          </p>
          <div class="next-to-button">
            For discussions outside of our meetings and to vote for papers.
          </div>
        </div>

        <div fxLayout="row" fxLayoutAlign="left center">
          <p>
            <a mat-button class="group-button item-title"
               href="https://groups.google.com/g/logag"
               target="_blank">Join Mailing List!</a>
          </p>
          <div class="next-to-button">
            For weekly updates about the next paper (via Google groups).
          </div>
        </div>

        <p>
          You can also subscribe to the meetings via <a
          href="https://calendar.google.com/calendar/u/0?cid=dmR1am4ycGJwa2hncjVmNTVjbTM5cWJtdThAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ"
          target="_blank"
          class="link-style-green">Google Calendar</a>, or <a
          href="https://calendar.google.com/calendar/ical/vdujn2pbpkhgr5f55cm39qbmu8%40group.calendar.google.com/public/basic.ics"
          target="_blank"
          class="link-style-green">via iCal</a>. Alternatively, <a
          href="https://calendar.google.com/event?action=TEMPLATE&tmeid=MWhmZDJzMnI0aG10YTlvZzZpZGVsYnJmaWNfMjAyMTA4MDNUMTUwMDAwWiB2ZHVqbjJwYnBraGdyNWY1NWNtMzlxYm11OEBn&tmsrc=vdujn2pbpkhgr5f55cm39qbmu8%40group.calendar.google.com&scp=ALL"
          target="_blank"
          class="link-style-green">add the events</a>.
        </p>
        <!--div class="item" fxLayout="column" fxLayoutAlign.lt-md="center center">
          <div fxLayout="row">
            <p>
              <a mat-button class="group-button item-title" href="https://groups.google.com/g/logag"
                 target="_blank">Sign up for email updates!</a>
            </p>
          </div>
          <div class="item-content">
            <div>

              <form class="contact" [formGroup]="form" method="post" (ngSubmit)="onSubmit()">
                <div fxLayout="row" fxLayout.lt-md="column">
                  <mat-form-field>
                    <mat-label>Name (optional)</mat-label>
                    <input matInput [formControl]="name">
                    <mat-error *ngIf="name.invalid">Please enter your name</mat-error>
                  </mat-form-field>
                  <mat-form-field [ngClass.gt-sm]="'margin-left'">
                    <mat-label>Email</mat-label>
                    <input matInput [formControl]="email" placeholder="email@example.com" required type="email">
                    <mat-error *ngIf="email.invalid">Please enter a valid email</mat-error>
                  </mat-form-field>

                </div>
                <div style="text-align:center">
                  <button mat-button [class.spinner]="isLoading" [disabled]="isLoading" class="submit submit-button"
                          type="submit">
                    Submit
                  </button>
                </div>
                <input [formControl]="honeypot" class="hidden" type="text"/>
                <div [ngClass]="!(success && submit)? 'hidden' : 'visible'" class="success-message">
                  <span>{{responseMessage}}</span>
                </div>
                <div [ngClass]="!(!success && submit)? 'hidden' : 'visible'" class="failed-message">
                  <span>{{responseMessage}}</span>
                </div>
              </form>
            </div>
          </div>
        </div-->
      </div>
    </div>
    <div fxFlex="10" fxFlex.xs="">
    </div>
  </div>

  <div fxLayout="column" class="width100">
    <!-- section 0 -->
    <div>
      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" class="underline-title">
          <h1 class="left-titles">RECENT VIDEO (scroll down for more)</h1>
        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>


      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" fxLayout="column">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2204.10348"
                   target="_blank">
                  Simulate Time-integrated Coarse-grained Molecular Dynamics with Geometric Machine Learning
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                5th of July 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Xiang Fu, Tian Xie, Nathan J. Rebello, Bradley D. Olsen, Tommi Jaakkola
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLCoarseMD' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                (86 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper <a
                class="item-subtitle link-style-black"
                href="https://xiangfu.co/"
                target="_blank">Xiang Fu</a> who is a PhD student at MIT in the groups of Tommi Jaakkola and Pulkit
                Agrawal. Currently, he works on molecular dynamics and coarse-graining or other abstraction-related
                approaches.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Molecular dynamics (MD) simulation is the workhorse of various scientific domains but is limited by high
                computational cost. Learning-based force fields have made major progress in accelerating ab-initio MD
                simulation but are still not fast enough for many real-world applications that require long-time MD
                simulation. In this paper, we adopt a different machine learning approach where we coarse-grain a
                physical
                system using graph clustering, and model the system evolution with a very large time-integration step
                using graph neural networks. A novel score-based GNN refinement module resolves the long-standing
                challenge of long-time simulation instability. Despite only trained with short MD trajectory data, our
                learned simulator can generalize to unseen novel systems and simulate for much longer than the training
                trajectories. Properties requiring 10-100 ns level long-time dynamics can be accurately recovered at
                several-orders-of-magnitude higher speed than classical force fields. We demonstrate the effectiveness
                of
                our method on two realistic complex systems: (1) single-chain coarse-grained polymers in implicit
                solvent;
                (2) multi-component Li-ion polymer electrolyte systems.
              </p>
            </div>
          </div>

        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>
    </div>

    <div id="Schedule">
      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" class="underline-title">
          <h1 class="left-titles">SCHEDULE</h1>
        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>


      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" fxLayout="column">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2104.11225"
                   target="_blank">
                  Pri3D: Can 3D Priors Help 2D Representation Learning?
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                6th of September 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Ji Hou, Saining Xie, Benjamin Graham, Angela Dai, Matthias Nie√üner
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              Dr <a
              class="item-subtitle link-style-black"
              href="https://sekunde.github.io/"
              target="_blank">Ji Hou</a> who is a Research Scientist at Meta Reality Labs. Previously, he did his Ph.D.
              at the TUM Visual Computing Group headed by Prof. Matthias Niessner, where he worked on Computer Vision
              and 3D Scene Understanding.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Recent advances in 3D perception have shown impressive progress in understanding geometric structures of
              3Dshapes and even scenes. Inspired by these advances in geometric understanding, we aim to imbue
              image-based perception with representations learned under geometric constraints. We introduce an approach
              to learn view-invariant,geometry-aware representations for network pre-training, based on multi-view RGB-D
              data, that can then be effectively transferred to downstream 2D tasks. We propose to employ contrastive
              learning under both multi-view im-age constraints and image-geometry constraints to encode3D priors into
              learned 2D representations. This results not only in improvement over 2D-only representation learning on
              the image-based tasks of semantic segmentation, instance segmentation, and object detection on real-world
              in-door datasets, but moreover, provides significant improvement in the low data regime. We show a
              significant improvement of 6.0% on semantic segmentation on full data as well as 11.9% on 20% data against
              baselines on ScanNet.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2201.11969"
                   target="_blank">
                  Approximately Equivariant Networks for Imperfectly Symmetric Dynamics
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                13th of September 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Rui Wang, Robin Walters, Rose Yu
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              The authors <a
              class="item-subtitle link-style-black"
              href="https://rui1521.github.io/online-cv/"
              target="_blank">Rui Wang</a>, Prof. <a
              class="item-subtitle link-style-black"
              href="https://scholar.google.com/citations?user=fnprJmUAAAAJ&hl=en"
              target="_blank">Robin Walters</a>, and maybe Prof. <a
              class="item-subtitle link-style-black"
              href="https://roseyu.com/"
              target="_blank">Rose Yu</a>: Rui is a PhD student in the group of Prof. Yu at UC San Diego, and Prof.
              Walters is at North Eastern University. All have lots of interesting work on symmetries in ML.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Incorporating symmetry as an inductive bias into neural network architecture has led to improvements in
              generalization, data efficiency, and physical consistency in dynamics modeling. Methods such as CNNs or
              equivariant neural networks use weight tying to enforce symmetries such as shift invariance or rotational
              equivariance. However, despite the fact that physical laws obey many symmetries, real-world dynamical data
              rarely conforms to strict mathematical symmetry either due to noisy or incomplete data or to symmetry
              breaking features in the underlying dynamical system. We explore approximately equivariant networks which
              are biased towards preserving symmetry but are not strictly constrained to do so. By relaxing equivariance
              constraints, we find that our models can outperform both baselines with no symmetry bias and baselines
              with overly strict symmetry in both simulated turbulence domains and real-world multi-stream jet flow.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://www.pnas.org/doi/10.1073/pnas.0500334102"
                   target="_blank">
                  Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps
                  (2005)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                Monday 19th of September 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Ronald Coifman, S. LAFON, A. B. LEE, M. MAGGIONI, B.NADLER, F. WARNER, AND S. ZUCKER
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              Professor <a
              class="item-subtitle link-style-black"
              href="https://cpsc.yale.edu/people/ronald-coifman"
              target="_blank">Ronald Coifman</a> who is Sterling Professor of Mathematics at Yale University. He
              is also a member of the American Academy of Arts and Sciences, the Connecticut Academy of Science and
              Engineering, and the National Academy of Sciences. He is a recipient of the 1996 DARPA Sustained
              Excellence Award, the 1996 Connecticut Science Medal, the 1999 Pioneer Award of the International Society
              for Industrial and Applied Science, and the 1999 National Medal of Science.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              We provide a framework for structural multiscale geometric organization of graphs and subsets of ‚Ñùùëõ. We
              use diffusion semigroups to generate multiscale geometries in order to organize and represent complex
              structures. We show that appropriately selected eigenfunctions or scaling functions of Markov matrices,
              which describe local transitions, lead to macroscopic descriptions at different scales. The process of
              iterating or diffusing the Markov matrix is seen as a generalization of some aspects of the Newtonian
              paradigm, in which local infinitesimal transitions of a system lead to global macroscopic descriptions by
              integration. We provide a unified view of ideas from data analysis, machine learning, and numerical
              analysis.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2206.10991"
                   target="_blank">
                  No paper due to ICLR submission deadline!
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                26th of September 2022
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              No one.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2206.07697"
                   target="_blank">
                  MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                3rd of October 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Ilyes Batatia, D√°vid P√©ter Kov√°cs, Gregor N. C. Simm, Christoph Ortner, G√°bor Cs√°nyi
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              <a
                class="item-subtitle link-style-black"
                href="https://scholar.google.com/citations?user=fTVuWFMAAAAJ&hl=en"
                target="_blank">Ilyes Batatia</a> who is a research intern at the University of Cambridge and <a
              class="item-subtitle link-style-black"
              href="https://www.linkedin.com/in/d%C3%A1vid-p%C3%A9ter-kov%C3%A1cs-9b8465104/"
              target="_blank">D√°vid P√©ter Kov√°cs</a> who is a PhD student there.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Creating fast and accurate force fields is a long-standing challenge in computational chemistry and
              materials science. Recently, several equivariant message passing neural networks (MPNNs) have been shown
              to outperform models built using other approaches in terms of accuracy. However, most MPNNs suffer from
              high computational cost and poor scalability. We propose that these limitations arise because MPNNs only
              pass two-body messages leading to a direct relationship between the number of layers and the expressivity
              of the network. In this work, we introduce MACE, a new equivariant MPNN model that uses higher body order
              messages. In particular, we show that using four-body messages reduces the required number of message
              passing iterations to just two, resulting in a fast and highly parallelizable model, reaching or
              exceeding state-of-the-art accuracy on the rMD17, 3BPA, and AcAc benchmark tasks. We also demonstrate that
              using higher order messages leads to an improved steepness of the learning curves.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.07832"
                   target="_blank">
                  Learning Equivariant Energy Based Models with Equivariant Stein Variational Gradient Descent
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                10th of October 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Priyank Jaini, Lars Holdijk, Max Welling
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              Authors <a
              class="item-subtitle link-style-black"
              href="https://www.larsholdijk.com/"
              target="_blank">Lars Holdijk</a> and Dr. <a
              class="item-subtitle link-style-black"
              href="https://priyankjaini.github.io/"
              target="_blank">Priyank Jaini</a>. Lars is an ELLIS Program MSc student of Prof. Max Welling at
              University of Amsterdam and Prof. Yarin Gal at University of Oxford. Dr Jaini is a Research Scientist in
              the Brain team at Google Research in Toronto and before that he was a PostDoc of Prof. Welling.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              We focus on the problem of efficient sampling and learning of probability densities by incorporating
              symmetries in probabilistic models. We first introduce Equivariant Stein Variational Gradient Descent
              algorithm -- an equivariant sampling method based on Stein's identity for sampling from densities with
              symmetries. Equivariant SVGD explicitly incorporates symmetry information in a density through equivariant
              kernels which makes the resultant sampler efficient both in terms of sample complexity and the quality of
              generated samples. Subsequently, we define equivariant energy based models to model invariant densities
              that are learned using contrastive divergence. By utilizing our equivariant SVGD for training equivariant
              EBMs, we propose new ways of improving and scaling up training of energy based models. We apply these
              equivariant energy models for modelling joint densities in regression and classification tasks for image
              datasets, many-body particle systems and molecular structure generation.
            </p>
          </div>


        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>
    </div>


    <div class="top-margin" id="previous">
      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" class="underline-title">
          <h1 class="left-titles">PREVIOUS PAPERS AND RECORDINGS</h1>
        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>

      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" fxLayout="column">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2206.10991"
                   target="_blank">
                  Graph Neural Networks as Gradient Flows
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                30th of August 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Francesco Di Giovanni, James Rowbottom, Benjamin P. Chamberlain, Thomas Markovich, Michael M.
                  Bronstein
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGRAFF' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Dr. <a
                class="item-subtitle link-style-black"
                href="https://scholar.google.com/citations?user=64X0hCAAAAAJ&hl=en"
                target="_blank">Francesco Di Giovanni</a> obtained his PhD from University College London and is working
                together with at Twitter Research. <a
                class="item-subtitle link-style-black"
                href="https://www.linkedin.com/in/jamesrowbottom/"
                target="_blank">James Rowbottom</a> received his master in AI with distinction from Imperial College
                London and is also at Twitter.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Dynamical systems minimizing an energy are ubiquitous in geometry and physics. We propose a gradient
                flow
                framework for GNNs where the equations follow the direction of steepest descent of a learnable energy.
                This approach allows to explain the GNN evolution from a multi-particle perspective as learning
                attractive
                and repulsive forces in feature space via the positive and negative eigenvalues of a symmetric
                "channel-mixing" matrix. We perform spectral analysis of the solutions and conclude that gradient flow
                graph convolutional models can induce a dynamics dominated by the graph high frequencies which is
                desirable for heterophilic datasets. We also describe structural constraints on common GNN architectures
                allowing to interpret them as gradient flows. We perform thorough ablation studies corroborating our
                theoretical analysis and show competitive performance of simple and lightweight models on real-world
                homophilic and heterophilic datasets.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.03336"
                   target="_blank">
                  Frame Averaging for Invariant and Equivariant Network Design
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                23rd of August 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Omri Puny, Matan Atzmon, Heli Ben-Hamu, Ishan Misra, Aditya Grover, Edward J. Smith, Yaron Lipman
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLFrameAveraging' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a
                class="item-subtitle link-style-black"
                href="https://omri1348.github.io/"
                target="_blank">Omri Puny</a> who is a PhD student in the Department of Computer Science and Applied
                Mathematics at the Weizmann Institute of Science under the supervision of Prof. Yaron Lipman. My
                research
                focuses on developing deep learning models for iregular data, mainly graphs and point clouds.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Many machine learning tasks involve learning functions that are known to be invariant or equivariant to
                certain symmetries of the input data. However, it is often challenging to design neural network
                architectures that respect these symmetries while being expressive and computationally efficient. For
                example, Euclidean motion invariant/equivariant graph or point cloud neural networks. We introduce Frame
                Averaging (FA), a general purpose and systematic framework for adapting known (backbone) architectures
                to
                become invariant or equivariant to new symmetry types. Our framework builds on the well known group
                averaging operator that guarantees invariance or equivariance but is intractable. In contrast, we
                observe
                that for many important classes of symmetries, this operator can be replaced with an averaging operator
                over a small subset of the group elements, called a frame. We show that averaging over a frame
                guarantees
                exact invariance or equivariance while often being much simpler to compute than averaging over the
                entire
                group. Furthermore, we prove that FA-based models have maximal expressive power in a broad setting and
                in
                general preserve the expressive power of their backbone architectures. Using frame averaging, we propose
                a
                new class of universal Graph Neural Networks (GNNs), universal Euclidean motion invariant point cloud
                networks, and Euclidean motion invariant Message Passing (MP) GNNs. We demonstrate the practical
                effectiveness of FA on several applications including point cloud normal estimation, beyond 2-WL graph
                separation, and n-body dynamics prediction, achieving state-of-the-art results in all of these
                benchmarks.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2207.09453"
                   target="_blank">
                  e3nn: Euclidean Neural Networks
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                16th of August 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Mario Geiger, Tess Smidt
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLe3nn' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author Dr. <a
                class="item-subtitle link-style-black"
                href="https://mariogeiger.ch/"
                target="_blank">Mario Geiger</a> who is a PostDoc at MIT in the group of Professor Tess Smidt.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                We present e3nn, a generalized framework for creating E(3) equivariant trainable functions, also known
                as
                Euclidean neural networks. e3nn naturally operates on geometry and geometric tensors that describe
                systems
                in 3D and transform predictably under a change of coordinate system. The core of e3nn are equivariant
                operations such as the TensorProduct class or the spherical harmonics functions that can be composed to
                create more complex modules such as convolutions and attention mechanisms. These core operations of e3nn
                can be used to efficiently articulate Tensor Field Networks, 3D Steerable CNNs, Clebsch-Gordan Networks,
                SE(3) Transformers and other E(3) equivariant networks.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2207.02505"
                   target="_blank">
                  Pure Transformers are Powerful Graph Learners
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                9th of August 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Jinwoo Kim, Tien Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, Seunghoon Hong
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLTokenGT' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper <a
                class="item-subtitle link-style-black"
                href="https://jw9730.notion.site/jw9730/Jinwoo-Kim-0560795427964cafa7481dc448baa4aa"
                target="_blank">Jinwoo Kim</a> who is a Ph.D. student at Vision and Learning Laboratory at KAIST,
                advised
                by Prof. Seunghoon Hong.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                We show that standard Transformers without graph-specific modifications can lead to promising results in
                graph learning both in theory and practice. Given a graph, we simply treat all nodes and edges as
                independent tokens, augment them with token embeddings, and feed them to a Transformer. With an
                appropriate choice of token embeddings, we prove that this approach is theoretically at least as
                expressive as an invariant graph network (2-IGN) composed of equivariant linear layers, which is already
                more expressive than all message-passing Graph Neural Networks (GNN). When trained on a large-scale
                graph
                dataset (PCQM4Mv2), our method coined Tokenized Graph Transformer (TokenGT) achieves significantly
                better
                results compared to GNN baselines and competitive results compared to Transformer variants with
                sophisticated graph-specific inductive bias. Our implementation is available at this https URL.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.05064"
                   target="_blank">
                  Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                2nd of August 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Nicholas Gao, Stephan G√ºnnemann
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLAbInitioPotential' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a
                class="item-subtitle link-style-black"
                href="https://scholar.google.com/citations?user=3GIKgWoAAAAJ&hl=en"
                target="_blank">Nicholas Gao</a> who is a PhD student in the group of Stephan G√ºnnemann at TU Munich.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Solving the Schr√∂dinger equation is key to many quantum mechanical properties. However, an analytical
                solution is only tractable for single-electron systems. Recently, neural networks succeeded at modeling
                wave functions of many-electron systems. Together with the variational Monte-Carlo (VMC) framework, this
                led to solutions on par with the best known classical methods. Still, these neural methods require
                tremendous amounts of computational resources as one has to train a separate model for each molecular
                geometry. In this work, we combine a Graph Neural Network (GNN) with a neural wave function to
                simultaneously solve the Schr√∂dinger equation for multiple geometries via VMC. This enables us to model
                continuous subsets of the potential energy surface with a single training pass. Compared to existing
                state-of-the-art networks, our Potential Energy Surface Network PESNet speeds up training for multiple
                geometries by up to 40 times while matching or surpassing their accuracy. This may open the path to
                accurate and orders of magnitude cheaper quantum mechanical calculations.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2205.12454"
                   target="_blank">
                  Recipe for a General, Powerful, Scalable Graph Transformer
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                26th of July 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Ladislav Ramp√°≈°ek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, Dominique Beaini
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGPS' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a
                class="item-subtitle link-style-black"
                href="https://rampasek.github.io/"
                target="_blank">Ladislav Ramp√°≈°ek</a> who is a postdoctoral fellow at Mila and Universit√© de Montr√©al
                working on
                GRL. He obtained his PhD at the University of Toronto. <a
                class="item-subtitle link-style-black"
                href="https://mila.quebec/en/person/mikhail-galkin/"
                target="_blank">Mikhail Galkin</a> is a Postdoctoral Fellow at McGill
                University. He completed his PhD at the University of Bonn, specializing in knowledge graphs (KGs). <a
                class="item-subtitle link-style-black"
                href="https://vijaydwivedi.com.np/"
                target="_blank">Vijay
                Prakash Dwivedi</a>
                is an ML PhD student at Nanyang Technological University, Singapore working with Prof. Luu
                Anh Tuan (NTU) and Prof. Xavier Bresson (NUS).
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Solving the Schr√∂dinger equation is key to many quantum mechanical properties. However, an analytical
                solution is only tractable for single-electron systems. Recently, neural networks succeeded at modeling
                wave functions of many-electron systems. Together with the variational Monte-Carlo (VMC) framework, this
                led to solutions on par with the best known classical methods. Still, these neural methods require
                tremendous amounts of computational resources as one has to train a separate model for each molecular
                geometry. In this work, we combine a Graph Neural Network (GNN) with a neural wave function to
                simultaneously solve the Schr√∂dinger equation for multiple geometries via VMC. This enables us to model
                continuous subsets of the potential energy surface with a single training pass. Compared to existing
                state-of-the-art networks, our Potential Energy Surface Network PESNet speeds up training for multiple
                geometries by up to 40 times while matching or surpassing their accuracy. This may open the path to
                accurate and orders of magnitude cheaper quantum mechanical calculations.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2201.12843"
                   target="_blank">
                  On Recoverability of Graph Neural Network Representations (2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                12th of July 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Maxim Fishman, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Avi Mendelson
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLRecoverable' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                (45 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper Maxim Fishman who is an MSc student in the Electrical and Computer Engineering
                Department of Technion, advised by Dr. Chaim Baskin and Prof. Avi Mendelson. Previously, he did his BSc
                in
                Computer Science and BSc in Physics at Technion. As an applied mathematician and engineer he works as a
                researcher and developer at Intel.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Despite their growing popularity, graph neural networks (GNNs) still have multiple unsolved problems,
                including finding more expressive aggregation methods, propagation of information to distant nodes, and
                training on large-scale graphs. Understanding and solving such problems require developing analytic
                tools
                and techniques. In this work, we propose the notion of recoverability, which is tightly related to
                information aggregation in GNNs, and based on this concept, develop the method for GNN embedding
                analysis.
                We define recoverability theoretically and propose a method for its efficient empirical estimation. We
                demonstrate, through extensive experimental results on various datasets and different GNN architectures,
                that estimated recoverability correlates with aggregation method expressivity and graph sparsification
                quality. Therefore, we believe that the proposed method could provide an essential tool for
                understanding
                the roots of the aforementioned problems, and potentially lead to a GNN design that overcomes them. The
                code to reproduce our experiments is available at this https URL
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2204.10348"
                   target="_blank">
                  Simulate Time-integrated Coarse-grained Molecular Dynamics with Geometric Machine Learning
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                5th of July 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Xiang Fu, Tian Xie, Nathan J. Rebello, Bradley D. Olsen, Tommi Jaakkola
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLCoarseMD' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                (86 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper <a
                class="item-subtitle link-style-black"
                href="https://xiangfu.co/"
                target="_blank">Xiang Fu</a> who is a PhD student at MIT in the groups of Tommi Jaakkola and Pulkit
                Agrawal. Currently, he works on molecular dynamics and coarse-graining or other abstraction-related
                approaches.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Molecular dynamics (MD) simulation is the workhorse of various scientific domains but is limited by high
                computational cost. Learning-based force fields have made major progress in accelerating ab-initio MD
                simulation but are still not fast enough for many real-world applications that require long-time MD
                simulation. In this paper, we adopt a different machine learning approach where we coarse-grain a
                physical
                system using graph clustering, and model the system evolution with a very large time-integration step
                using graph neural networks. A novel score-based GNN refinement module resolves the long-standing
                challenge of long-time simulation instability. Despite only trained with short MD trajectory data, our
                learned simulator can generalize to unseen novel systems and simulate for much longer than the training
                trajectories. Properties requiring 10-100 ns level long-time dynamics can be accurately recovered at
                several-orders-of-magnitude higher speed than classical force fields. We demonstrate the effectiveness
                of
                our method on two realistic complex systems: (1) single-chain coarse-grained polymers in implicit
                solvent;
                (2) multi-component Li-ion polymer electrolyte systems.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2204.05351"
                   target="_blank">
                  Graph Ordering Attention Networks
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                28th of June 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Michail Chatzianastasis, Johannes F. Lutzeyer, George Dasoulas, Michalis Vazirgiannis
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGOAT' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                (~70 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a
                class="item-subtitle link-style-black"
                href="https://michailchatzianastasis.github.io/"
                target="_blank">Michail Chatzianastasis</a> is a first year PhD student at Ecole Polytechnique in the
                Data
                Science and Mining group lead by Prof. Michalis Vazirgiannis.
                He works on machine learning for graph-structured data; in particular, on Graph Neural Networks and
                AutoML
                as well as Neural Architecture Search methods. Also joining us is Dr. <a
                class="item-subtitle link-style-black"
                href="https://johanneslutzeyer.com/"
                target="_blank">Johannes Lutzeyer</a> is a postdoctoral researcher in the Data Science and Mining group
                at
                the Ecole Polytechnique. He works in the field of Graph Representation Learning with a focus on Graph
                Neural Networks and the spectral analysis of Graph Shift Operator matrices.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Graph Neural Networks (GNNs) have been successfully used in many problems involving graph-structured
                data,
                achieving state-of-the-art performance. GNNs typically employ a message-passing scheme, in which every
                node aggregates information from its neighbors using a permutation-invariant aggregation function.
                Standard well-examined choices such as the mean or sum aggregation functions have limited capabilities,
                as
                they are not able to capture interactions among neighbors. In this work, we formalize these interactions
                using an information-theoretic framework that notably includes synergistic information. Driven by this
                definition, we introduce the Graph Ordering Attention (GOAT) layer, a novel GNN component that captures
                interactions between nodes in a neighborhood. This is achieved by learning local node orderings via an
                attention mechanism and processing the ordered representations using a recurrent neural network
                aggregator. This design allows us to make use of a permutation-sensitive aggregator while maintaining
                the
                permutation-equivariance of the proposed GOAT layer. The GOAT model demonstrates its increased
                performance
                in modeling graph metrics that capture complex information, such as the betweenness centrality and the
                effective size of a node. In practical use-cases, its superior modeling capability is confirmed through
                its success in several real-world node classification benchmarks.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2202.02296"
                   target="_blank">
                  Graph-Coupled Oscillator Networks (ICML 2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                21st of June 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  T. Konstantin Rusch, Benjamin P. Chamberlain, James Rowbottom, Siddhartha Mishra, Michael M. Bronstein
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGraphCon' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                (~60 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper <a
                class="item-subtitle link-style-black"
                href="https://konstantinrusch.com/"
                target="_blank">Konstantin Rusch</a> who is an applied math PhD student at ETH Zurich supervised by
                Siddhartha Mishra. His research is focused on physics-based ML for developing new methods and tackling
                scientific problems.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                We propose Graph-Coupled Oscillator Networks (GraphCON), a novel framework for deep learning on graphs.
                It
                is based on discretizations of a second-order system of ordinary differential equations (ODEs), which
                model a network of nonlinear forced and damped oscillators, coupled via the adjacency structure of the
                underlying graph. The flexibility of our framework permits any basic GNN layer (e.g. convolutional or
                attentional) as the coupling function, from which a multi-layer deep neural network is built up via the
                dynamics of the proposed ODEs. We relate the oversmoothing problem, commonly encountered in GNNs, to the
                stability of steady states of the underlying ODE and show that zero-Dirichlet energy steady states are
                not
                stable for our proposed ODEs. This demonstrates that the proposed framework mitigates the oversmoothing
                problem. Finally, we show that our approach offers competitive performance with respect to the
                state-of-the-art on a variety of graph-based learning tasks.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2204.01613"
                   target="_blank">
                  SPECTRE : Spectral Conditioning Helps to Overcome the Expressivity Limits of One-shot Graph Generators
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                14th of June 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Karolis Martinkus, Andreas Loukas, Nathana√´l Perraudin, Roger Wattenhofer
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLSpectre' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                (64 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First authors <a
                class="item-subtitle link-style-black"
                href="https://disco.ethz.ch/members/mkarolis"
                target="_blank">Karolis Martinkus</a> and Dr <a
                class="item-subtitle link-style-black"
                href="https://andreasloukas.blog/"
                target="_blank">Andreas Loukas</a>. Karolis is a PhD student at ETH Zurich working with Prof. Roger
                Wattenhofer. He has a lot of work on graph generation and GNNs in general.
                Andreas received his PhD from TU Delft and did PostDocs at TU Berlin and EPFL before starting as
                Principal
                Scientist at Genentech.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                We approach the graph generation problem from a spectral perspective by first generating the dominant
                parts of the graph Laplacian spectrum and then building a graph matching these eigenvalues and
                eigenvectors. Spectral conditioning allows for direct modeling of the global and local graph structure
                and
                helps to overcome the expressivity and mode collapse issues of one-shot graph generators. Our novel GAN,
                called SPECTRE, enables the one-shot generation of much larger graphs than previously possible with
                one-shot models. SPECTRE outperforms state-of-the-art deep autoregressive generators in terms of
                modeling
                fidelity, while also avoiding expensive sequential generation and dependence on node ordering. A case in
                point, in sizable synthetic and real-world graphs SPECTRE achieves a 4-to-170 fold improvement over the
                best competitor that does not overfit and is 23-to-30 times faster than autoregressive generators.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2206.01729"
                   target="_blank">
                  Torsional Diffusion for Molecular Conformer Generation
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                7th of June 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, Tommi S. Jaakkola
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLtordiff' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                (86 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First authors <a
                class="item-subtitle link-style-black"
                href="https://gcorso.github.io/"
                target="_blank">Gabriele Corso</a> and <a
                class="item-subtitle link-style-black"
                href="https://people.csail.mit.edu/bjing/"
                target="_blank">Bowen Jing</a> who both are PhD students at MIT. Bowen is co-supervised by Prof. Tommi
                Jaakkola and Prof. Bonnie Berger. Gabriele by Prof Jaakkola and Prof. Regina Barzilay.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Diffusion-based generative models generate samples by mapping noise to data via the reversal of a
                diffusion process that typically consists of independent Gaussian noise in every data coordinate. This
                diffusion process is, however, not well suited to the fundamental task of molecular conformer generation
                where the degrees of freedom differentiating conformers lie mostly in torsion angles. We, therefore,
                propose Torsional Diffusion that generates conformers by leveraging the definition of a diffusion
                process
                over the space T^m, a high dimensional torus representing torsion angles, and a SE(3)-equivariant model
                capable of accurately predicting the score over this process. Empirically, we demonstrate that our model
                outperforms state-of-the-art methods in terms of both diversity and precision of generated conformers,
                reducing the mean minimum RMSD by respectively 31% and 17%. When compared to Gaussian diffusion models,
                torsional diffusion enables significantly more accurate generation while performing two orders of
                magnitude fewer inference time-steps.

              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2102.07835"
                   target="_blank">
                  Topological Graph Neural Networks (ICLR 2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                31st of May 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Max Horn, Edward De Brouwer, Michael Moor, Yves Moreau, Bastian Rieck, Karsten Borgwardt
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLTOGL' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                (67 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a
                class="item-subtitle link-style-black"
                href="https://scholar.google.com/citations?user=-Pm4XtAAAAAJ&hl"
                target="_blank">Edward De Brouwer</a> who is a last year PhD student at KU Leuven, Belgium on machine
                learning for
                healthcare. His research focuses on developing robust and trustworthy methods for the clinical domain,
                using neural differential equations, topological data analysis and causal inference, among others. In
                2020-2021, he was a visiting researcher at ETH Z√ºrich, where he worked on topological methods for graph
                neural networks.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have
                been
                shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that
                incorporates global topological information of a graph using persistent homology. TOGL can be easily
                integrated into any type of GNN and is strictly more expressive (in terms the Weisfeiler--Lehman graph
                isomorphism test) than message-passing GNNs. Augmenting GNNs with TOGL leads to improved predictive
                performance for graph and node classification tasks, both on synthetic data sets, which can be
                classified
                by humans using their topology but not by ordinary GNNs, and on real-world data.
              </p>
            </div>
          </div>
          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2201.12176"
                   target="_blank">
                  Generative Coarse-Graining of Molecular Conformations (2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                24th of May 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Wujie Wang, Minkai Xu, Chen Cai, Benjamin Kurt Miller, Tess Smidt, Yusu Wang, Jian Tang, Rafael
                  G√≥mez-Bombarelli
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLCoarseGrain' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                (61 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper <a
                class="item-subtitle link-style-black"
                href="https://wwj.mit.edu/wujie-wangs-website"
                target="_blank">Wujie Wang</a> who is a 4th year graduate student at MIT in
                Materials Sc. & Eng. He holds a B.A. in Physics from Wesleyan and B.S in Engineering and Applied
                Sciences
                from Caltech. Also joining us is <a
                class="item-subtitle link-style-black"
                href="https://minkaixu.com/"
                target="_blank">Minkai Xu</a> who was working at MILA with Jian Tang and now received a
                Standford Graduate Fellowship with which he will pursue his PhD at Stanford.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Coarse-graining (CG) of molecular simulations simplifies the particle representation by grouping
                selected
                atoms into pseudo-beads and therefore drastically accelerates simulation. However, such CG procedure
                induces information losses, which makes accurate backmapping, i.e., restoring fine-grained (FG)
                coordinates from CG coordinates, a long-standing challenge. Inspired by the recent progress in
                generative
                models and equivariant networks, we propose a novel model that rigorously embeds the vital probabilistic
                nature and geometric consistency requirements of the backmapping transformation. Our model encodes the
                FG
                uncertainties into an invariant latent space and decodes them back to FG geometries via equivariant
                convolutions. To standardize the evaluation of this domain, we further provide three comprehensive
                benchmarks based on molecular dynamics trajectories. Extensive experiments show that our approach always
                recovers more realistic structures and outperforms existing data-driven methods with a significant
                margin.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2204.07697"
                   target="_blank">
                  Theory of Graph Neural Networks: Representation and Learning (ICLR 2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                17th of May 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Stefanie Jegelka
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGNNTheory' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                (~70 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                <a
                  class="item-subtitle link-style-black"
                  href="https://cptq.github.io/"
                  target="_blank">Derek Lim</a>, who is a PhD students in the group of Prof. Stefanie Jegelka
                at MIT. Derek previously presented ESAN and Sign/Basis Nets in the reading group!
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Graph Neural Networks (GNNs), neural network architectures targeted to learning representations of
                graphs,
                have become a popular learning model for prediction tasks on nodes, graphs and configurations of points,
                with wide success in practice. This article summarizes a selection of the emerging theoretical results
                on
                approximation and learning properties of widely used message passing GNNs and higher-order GNNs,
                focusing
                on representation, generalization and extrapolation. Along the way, it summarizes mathematical
                connections.
              </p>
            </div>
          </div>
          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2202.03376"
                   target="_blank">
                  Message Passing Neural PDE Solvers (ICLR 2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                10th of May 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Johannes Brandstetter, Daniel Worrall, Max Welling
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLNeuralPDE' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Johannes's <a class="item-subtitle link-style-black"
                              href="https://hannes-stark.com/assets/.pdf"
                              target="_blank">slides</a> are coming soon! (87 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                The two first authors: <a
                class="item-subtitle link-style-black"
                href="https://www.jku.at/institut-fuer-machine-learning/ueber-uns/team/ass-prof-dr-johannes-brandstetter/"
                target="_blank">Johannes Brandstetter</a> who is working with Microsoft's molecular ML group. Also
                joining us is <a
                class="item-subtitle link-style-black"
                href="https://danielewworrall.github.io/"
                target="_blank">Daniel Worrall</a> who is working at DeepMind. Previously he was a PostDoc in Prof. Max
                Welling's group.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of
                research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which
                piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only
                generalize over a subset of properties to which a generic solver would be faced, including: resolution,
                topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this
                work, we build a solver, satisfying these properties, where all the components are based on neural
                message
                passing, replacing all heuristically designed components in the computation graph with
                backprop-optimized
                neural function approximators. We show that neural message passing solvers representationally contain
                some
                classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage
                stability in training autoregressive models, we put forward a method that is based on the principle of
                zero-stability, posing stability as a domain adaptation problem. We validate our method on various
                fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain
                topologies, discretization, etc. in 1D and 2D. Our model outperforms state-of-the-art numerical solvers
                in
                the low resolution regime in terms of speed and accuracy.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2204.05249"
                   target="_blank">
                  Learning Local Equivariant Representations for Large-Scale Atomistic Dynamics
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                3rd of May 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J. Owen, Mordechai Kornbluth,
                  Boris Kozinsky
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLAllegro' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their <a class="item-subtitle link-style-black"
                         href="https://hannes-stark.com/assets/2022_05_03_logag.pdf"
                         target="_blank">slides</a>. (69 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                The authors <a
                class="item-subtitle link-style-black"
                href="https://simonbatzner.github.io/"
                target="_blank">Simon Batzner</a> and <a
                class="item-subtitle link-style-black"
                href="https://bkoz.seas.harvard.edu/news/doe-csgf-albert-musaelian"
                target="_blank">Albert Musaelian</a>. Simon is 3rd year PhD student at Harvard interested in how
                to leverage Deep Learning to advance Molecular Simulation. Prior to joining Harvard, he obtained a
                Master‚Äôs Degree from MIT and spent a year conducting research at the NASA Armstrong Research Center.
                Albert Musaelian is a second-year PhD student in the MIR group at Harvard and a Department of Energy
                Computational Science Graduate Fellow. He works on the design of machine learning techniques for
                atomic-scale modeling.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                A simultaneously accurate and computationally efficient parametrization of the energy and atomic forces
                of
                molecules and materials is a long-standing goal in the natural sciences. In pursuit of this goal, neural
                message passing has lead to a paradigm shift by describing many-body correlations of atoms through
                iteratively passing messages along an atomistic graph. This propagation of information, however, makes
                parallel computation difficult and limits the length scales that can be studied. Strictly local
                descriptor-based methods, on the other hand, can scale to large systems but do not currently match the
                high accuracy observed with message passing approaches. This work introduces Allegro, a strictly local
                equivariant deep learning interatomic potential that simultaneously exhibits excellent accuracy and
                scalability of parallel computation. Allegro learns many-body functions of atomic coordinates using a
                series of tensor products of learned equivariant representations, but without relying on message
                passing.
                Allegro obtains improvements over state-of-the-art methods on the QM9 and revised MD-17 data sets. A
                single tensor product layer is shown to outperform existing deep message passing neural networks and
                transformers on the QM9 benchmark. Furthermore, Allegro displays remarkable generalization to
                out-of-distribution data. Molecular dynamics simulations based on Allegro recover structural and kinetic
                properties of an amorphous phosphate electrolyte in excellent agreement with first principles
                calculations. Finally, we demonstrate the parallel scaling of Allegro with a dynamics simulation of 100
                million atoms.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.02096"
                   target="_blank">
                  Top-N: Equivariant set and graph generation without exchangeability (ICLR 2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                26th of April 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Cl√©ment Vignac, Pascal Frossard
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLTopN' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                (63 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a
                class="item-subtitle link-style-black"
                href="https://cvignac.github.io/"
                target="_blank">Cl√©ment Vignac</a> who is a PhD student at EPFL in the lab of Prof. Pascal Frossard. He
                works on ML for sets and graphs, incorporating symmetry priors of these data modalities, and graph
                generation.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                This work addresses one-shot set and graph generation, and, more specifically, the parametrization of
                probabilistic decoders that map a vector-shaped prior to a distribution over sets or graphs. Sets and
                graphs are most commonly generated by first sampling points i.i.d. from a normal distribution, and then
                processing these points along with the prior vector using Transformer layers or Graph Neural Networks.
                This architecture is designed to generate exchangeable distributions, i.e., all permutations of the
                generated outputs are equally likely. We however show that it only optimizes a proxy to the evidence
                lower
                bound, which makes it hard to train. We then study equivariance in generative settings and show that
                non-exchangeable methods can still achieve permutation equivariance. Using this result, we introduce
                Top-n
                creation, a differentiable generation mechanism that uses the latent vector to select the most relevant
                points from a trainable reference set. Top-n can replace i.i.d. generation in any Variational
                Autoencoder
                or Generative Adversarial Network. Experimentally, our method outperforms i.i.d. generation by 15% at
                SetMNIST reconstruction, by 33% at object detection on CLEVR, generates sets that are 74% closer to the
                true distribution on a synthetic molecule-like dataset, and generates more valid molecules on QM9.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2202.13060"
                   target="_blank">
                  Graph Attention Retrospective (2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                19th of April 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Kimon Fountoulakis, Amit Levi, Shenghao Yang, Aseem Baranwal, Aukosh Jagannath
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGATRetro' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Kimon's <a class="item-subtitle link-style-black"
                           href="https://hannes-stark.com/assets/kimon.pdf"
                           target="_blank">slides</a>. (86 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author Professor <a
                class="item-subtitle link-style-black"
                href="https://opallab.ca/team/"
                target="_blank">Kimon Fountoulakis</a> who works at the University of Waterloo and previously did a
                PostDoc at Berkeley after finishing his PhD at University of Edinburgh with Prof. Gondzio.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Graph-based learning is a rapidly growing sub-field of machine learning with applications in social
                networks, citation networks, and bioinformatics. One of the most popular type of models is graph
                attention
                networks. These models were introduced to allow a node to aggregate information from the features of
                neighbor nodes in a non-uniform way in contrast to simple graph convolution which does not distinguish
                the
                neighbors of a node. In this paper, we study theoretically this expected behaviour of graph attention
                networks. We prove multiple results on the performance of the graph attention mechanism for the problem
                of
                node classification for a contextual stochastic block model. Here the features of the nodes are obtained
                from a mixture of Gaussians and the edges from a stochastic block model where the features and the edges
                are coupled in a natural way. First, we show that in an "easy" regime, where the distance between the
                means of the Gaussians is large enough, graph attention maintains the weights of intra-class edges and
                significantly reduces the weights of the inter-class edges. As a corollary, we show that this implies
                perfect node classification independent of the weights of inter-class edges. However, a classical
                argument
                shows that in the "easy" regime, the graph is not needed at all to classify the data with high
                probability. In the "hard" regime, we show that every attention mechanism fails to distinguish
                intra-class
                from inter-class edges. We evaluate our theoretical results on synthetic and real-world data.
              </p>
            </div>

            <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

            <div class="item" fxLayout="column">
              <div fxLayout="row">
                <h3>
                  <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2202.13013"
                     target="_blank">
                    Sign and Basis Invariant Networks for Spectral Graph Representation Learning (2022)
                  </a></h3>
              </div>
              <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
                <div>
                  <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                  12th of April 2022
                </div>
                <div>
                  <a>
                    <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                    Derek Lim, Joshua Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, Stefanie Jegelka
                  </a>
                </div>
              </div>
              <div>
                <iframe ngClass.gt-sm="float-right" [src]='safeURLSignNet' allowfullscreen></iframe>
                <p class="paragraph">
                  <a class="slight-bold">Resources:</a>
                  Their <a class="item-subtitle link-style-black"
                           href="https://hannes-stark.com/assets/final_reading_group_slides.pdf"
                           target="_blank">slides</a>. (96 participants)
                </p>
                <p class="paragraph">
                  <a class="slight-bold">Speaker:</a>
                  First authors <a
                  class="item-subtitle link-style-black"
                  href="https://cptq.github.io/"
                  target="_blank">Derek Lim</a> and <a
                  class="item-subtitle link-style-black"
                  href="https://joshrobinson.mit.edu/"
                  target="_blank">Joshua Robinson</a>, who both are PhD students in the group of Prof. Stefanie Jegelka
                  at
                  MIT. Josh is also co-advised by Prof. Suvrit Sra and Derek previously presented ESAN in the reading
                  group!
                </p>
                <p class="no-margin">
                  <a class="slight-bold">Abstract:</a>
                  Many machine learning tasks involve processing eigenvectors derived from data. Especially valuable are
                  Laplacian eigenvectors, which capture useful structural information about graphs and other geometric
                  objects. However, ambiguities arise when computing eigenvectors: for each eigenvector v, the sign
                  flipped
                  ‚àív is also an eigenvector. More generally, higher dimensional eigenspaces contain infinitely many
                  choices
                  of basis eigenvectors. These ambiguities make it a challenge to process eigenvectors and eigenspaces
                  in
                  a
                  consistent way. In this work we introduce SignNet and BasisNet -- new neural architectures that are
                  invariant to all requisite symmetries and hence process collections of eigenspaces in a principled
                  manner.
                  Our networks are universal, i.e., they can approximate any continuous function of eigenvectors with
                  the
                  proper invariances. They are also theoretically strong for graph representation learning -- they can
                  approximate any spectral graph convolution, can compute spectral invariants that go beyond message
                  passing
                  neural networks, and can provably simulate previously proposed graph positional encodings. Experiments
                  show the strength of our networks for learning spectral graph filters and learning graph positional
                  encodings.
                </p>
              </div>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2109.07103"
                   target="_blank">
                  Automatic Symmetry Discovery with Lie Algebra Convolutional Network (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                5th of April 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Nima Dehmamy, Robin Walters, Yanchen Liu, Dashun Wang, Rose Yu
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLLConv' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Prof. Dehmamy's <a class="item-subtitle link-style-black"
                                   href="https://hannes-stark.com/assets/L-conv-Long2-Automatic Symmetry Discovery with Lie Algebra Convolutional Network.pdf"
                                   target="_blank">slides</a> are coming soon. (81 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author Professor <a
                class="item-subtitle link-style-black"
                href="http://nimadehmamy.com/"
                target="_blank">Nima Dehmamy</a> who works at Northwestern University and earned his PhD in Physics at
                Boston University. His research involves AI in graph learning, using physics to understand optimization
                landscapes, and neuroscience.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Existing equivariant neural networks require prior knowledge of the symmetry group and discretization
                for
                continuous groups. We propose to work with Lie algebras (infinitesimal generators) instead of Lie
                groups.
                Our model, the Lie algebra convolutional network (L-conv) can automatically discover symmetries and does
                not require discretization of the group. We show that L-conv can serve as a building block to construct
                any group equivariant feedforward architecture. Both CNNs and Graph Convolutional Networks can be
                expressed as L-conv with appropriate groups. We discover direct connections between L-conv and physics:
                (1) group invariant loss generalizes field theory (2) Euler-Lagrange equation measures the robustness,
                and
                (3) equivariance leads to conservation laws and Noether current.These connections open up new avenues
                for
                designing more general equivariant networks and applying them to important problems in physical
                sciences.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2010.16103"
                   target="_blank">
                  Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                29th of March 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, Long Jin
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLlabelTrick' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Prof. Li's <a class="item-subtitle link-style-black"
                              href="https://hannes-stark.com/assets/DF-LT.pdf"
                              target="_blank">slides</a>. (45 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Professor <a
                class="item-subtitle link-style-black"
                href="https://sites.google.com/view/panli-purdue/home?authuser=0"
                target="_blank">Pan Li</a> from Purdue University. He obtained his Ph.D. in Electrical and
                Computer
                Engineering at the University of Illinois Urbana - Champaign before completing a one-year postdoc in the
                SNAP group at Stanford with Prof. Jure Leskovec. He works on principled GraphML and its mathematical
                foundations.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                In this paper, we provide a theory of using graph neural networks (GNNs) for multi-node representation
                learning (where we are interested in learning a representation for a set of more than one node). We know
                that GNN is designed to learn single-node representations. When we want to learn a node set
                representation
                involving multiple nodes, a common practice in previous works is to directly aggregate the multiple node
                representations learned by a GNN into a joint representation of the node set. In this paper, we show a
                fundamental constraint of such an approach, namely the inability to capture the dependence between nodes
                in the node set, and argue that directly aggregating individual node representations does not lead to an
                effective joint representation for multiple nodes. Then, we notice that a few previous successful works
                for multi-node representation learning, including SEAL, Distance Encoding, and ID-GNN, all used node
                labeling. These methods first label nodes in the graph according to their relationships with the target
                node set before applying a GNN. Then, the node representations obtained in the labeled graph are
                aggregated into a node set representation. By investigating their inner mechanisms, we unify these node
                labeling techniques into a single and most basic form, namely labeling trick. We prove that with
                labeling
                trick a sufficiently expressive GNN learns the most expressive node set representations, thus in
                principle
                can solve any joint learning tasks over node sets. Experiments on one important two-node representation
                learning task, link prediction, verified our theory. Our work establishes a theoretical foundation of
                using GNNs for joint prediction tasks over node sets.
              </p>
            </div>
          </div>


          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="http://vladlen.info/papers/neural-fields.pdf"
                   target="_blank">
                  Geometry Processing with Neural Fields (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                22nd of March 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Guandao Yang, Serge Belongie, Bharath Hariharan, Vladlen Koltun
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGeomProc' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Guandao's <a class="item-subtitle link-style-black"
                             href="https://hannes-stark.com/assets/NeurIPS2021-GPwithNIF-Mar22.pdf"
                             target="_blank">slides</a>. (67 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a
                class="item-subtitle link-style-black"
                href="https://www.guandaoyang.com/"
                target="_blank">Guandao Yang</a>, who is a a Computer Science PhD student at Cornell University, advised
                by Serge Belongie and Bharath Hariharan. His research interests include computer vision for augmented
                reality and 3D generation.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Most existing geometry processing algorithms use meshes as the default shape representation.
                Manipulating
                meshes, however, requires one to maintain high quality in the surface discretization. For example,
                changing the topology of a mesh usually requires additional procedures such as remeshing. This paper
                instead proposes the use of neural fields for geometry processing. Neural fields can compactly store
                complicated shapes without spatial discretization. Moreover, neural fields are infinitely
                differentiable,
                which allows them to be optimized for objectives that involve higher-order derivatives. This raises the
                question: can geometry processing be done entirely using neural fields? We introduce loss functions and
                architectures to show that some of the most challenging geometry processing tasks, such as deformation
                and
                filtering, can be done with neural fields. Experimental results show that our methods are on par with
                the
                well-established mesh-based methods without committing to a particular surface discretization. Code is
                available at <a
                class="item-subtitle link-style-black"
                href="https://github.com/stevenygd/NFGP"
                target="_blank">https://github.com/stevenygd/NFGP</a>.
              </p>
            </div>
          </div>


          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.02905"
                   target="_blank">
                  Geometric and Physical Quantities Improve E(3) Equivariant Message Passing (2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                15th of March 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, Max Welling
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLSEGNN' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their <a class="item-subtitle link-style-black"
                         href="https://hannes-stark.com/assets/SEGNN.pdf"
                         target="_blank">slides</a>. (104 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper professor <a
                class="item-subtitle link-style-black"
                href="https://www.jku.at/institut-fuer-machine-learning/ueber-uns/team/ass-prof-dr-johannes-brandstetter/"
                target="_blank">Johannes Brandstetter</a> who holds his
                professorship at JKU Linz, where he previously worked with Prof. Sepp Hochreiter. He is a guest
                researcher
                at the University of Amsterdam in Prof. Max Wellings's group and started working with Microsoft's
                molecular ML group. He has a physics and data science background and was one of the main researchers at
                CERN who made the first direct observation of the Higgs boson decayinginto pairs of fermions! Next to
                his
                impressive work on physics-informed ML he also works on RL, NLP, Few-Shot Learning, and Generative
                Modeling. Also joining us was Professor <a
                class="item-subtitle link-style-black"
                href="https://erikbekkers.bitbucket.io/"
                target="_blank">Erik Bekkers</a>!
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Including covariant information, such as position, force, velocity or spin is important in many tasks in
                computational physics and chemistry. We introduce Steerable E(3) Equivariant Graph Neural Networks
                (SEGNNs) that generalise equivariant graph networks, such that node and edge attributes are not
                restricted
                to invariant scalars, but can contain covariant information, such as vectors or tensors. This model,
                composed of steerable MLPs, is able to incorporate geometric and physical information in both the
                message
                and update functions. Through the definition of steerable node attributes, the MLPs provide a new class
                of
                activation functions for general use with steerable feature fields. We discuss ours and related work
                through the lens of equivariant non-linear convolutions, which further allows us to pin-point the
                successful components of SEGNNs: non-linear message aggregation improves upon classic linear (steerable)
                point convolutions; steerable messages improve upon recent equivariant graph networks that send
                invariant
                messages. We demonstrate the effectiveness of our method on several tasks in computational physics and
                chemistry and provide extensive ablation studies.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black"
                   href="https://www.chaitjo.com/post/deep-learning-for-routing-problems"
                   target="_blank">
                  Recent Advances in Deep Learning for Routing Problems (2022)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                8th of March 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Chaitanya K. Joshi, Rishabh Anand
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLRouting' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Chaitanya's <a class="item-subtitle link-style-black"
                               href="https://www.chaitjo.com/post/deep-learning-for-routing-problems/recent-advances-in-deep-learning-for-routing-problems.pdf"
                               target="_blank">slides</a>. (70 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Author of the blog post and associated paper <a
                class="item-subtitle link-style-black"
                href="https://www.chaitjo.com/"
                target="_blank">Chaitanya K. Joshi</a>, who is an incoming PhD student at the University
                of Cambridge, supervised by Prof. Pietro Li√≤. His research explores the intersection of Graph and
                Geometric Deep Learning with applications in biomedicine and drug discovery. He previously worked on
                Graph
                Neural Network architectures and applications in Combinatorial Optimization at the NTU Graph Deep
                Learning
                Lab and A*STAR, Singapore.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Developing Graph Neural Network-driven solvers for combinatorial optimization problems such as the
                Travelling Salesperson Problem have seen a surge of academic interest recently. This talk aims to serve
                as
                a whirlwind tour of this research area. We first presents a Neural Combinatorial Optimization pipeline
                that unifies several recently proposed models into one single framework. Through the lens of the
                pipeline,
                we analyze recent advances in deep learning for routing problems and provide new directions to stimulate
                future research.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2111.12128"
                   target="_blank">
                  On the Unreasonable Effectiveness of Feature propagation in Learning on Graphs with Missing Node
                  Features (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                1st of March 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Emanuele Rossi, Henry Kenlay, Maria I. Gorinova, Benjamin Paul Chamberlain, Xiaowen Dong, Michael
                  Bronstein
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLUnreasonableEffect' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Ema's <a class="item-subtitle link-style-black"
                         href="https://docs.google.com/presentation/d/11dAeJRalTI7K1YAxMNz_yElZ0lVO5Bw7n0LBqSd-OUY/edit?usp=sharing"
                         target="_blank">slides</a>. (69 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a
                class="item-subtitle link-style-black"
                href="https://www.emanuelerossi.co.uk/"
                target="_blank">Emanuele Rossi</a> who is a Machine Learning Researcher at Twitter and a Ph.D.
                student at Imperial College London, working on Graph Neural Networks and supervised by Prof. Michael
                Bronstein. His research interests span a wide array of topics around graph neural networks, including
                scalability, dynamic graphs, and learning with missing node features. Before his current position,
                Emanuele was working at Fabula AI, which was then acquired by Twitter in June 2019. Previously, he
                completed an MPhil at the University of Cambridge and a BEng at Imperial College London, both in
                Computer
                Science.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                While Graph Neural Networks (GNNs) have recently become the de facto standard for modeling relational
                data, they impose a strong assumption on the availability of the node or edge features of the graph. In
                many real-world applications, however, features are only partially available; for example, in social
                networks, age and gender are available only for a small subset of users. We present a general approach
                for
                handling missing features in graph machine learning applications that is based on minimization of the
                Dirichlet energy and leads to a diffusion-type differential equation on the graph. The discretization of
                this equation produces a simple, fast and scalable algorithm which we call Feature Propagation. We
                experimentally show that the proposed approach outperforms previous methods on seven common
                node-classification benchmarks and can withstand surprisingly high rates of missing features: on average
                we observe only around 4% relative accuracy drop when 99% of the features are missing. Moreover, it
                takes
                only 10 seconds to run on a graph with ‚àº2.5M nodes and ‚àº123M edges on a single GPU.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.04941"
                   target="_blank">
                  Symmetric Spaces for Graph Embeddings: A Finsler-Riemannian Approach (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                22nd of February 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Federico L√≥pez, Beatrice Pozzetti, Steve Trettel, Michael Strube, Anna Wienhard
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLSymSpaces' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper <a
                class="item-subtitle link-style-black"
                href="https://fedelopez77.github.io/"
                target="_blank">Federico L√≥pez</a>, who is a Ph.D candidate in HITS at Heidelberg University. Prior to
                this, he studied Software Engineering at the University of Buenos Aires.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Learning faithful graph representations as sets of vertex embeddings has become a fundamental
                intermediary
                step in a wide range of machine learning applications. We propose the systematic use of symmetric spaces
                in representation learning, a class encompassing many of the previously used embedding targets. This
                enables us to introduce a new method, the use of Finsler metrics integrated in a Riemannian optimization
                scheme, that better adapts to dissimilar structures in the graph. We develop a tool to analyze the
                embeddings and infer structural properties of the data sets. For implementation, we choose Siegel
                spaces,
                a versatile family of symmetric spaces. Our approach outperforms competitive baselines for graph
                reconstruction tasks on various synthetic and real-world datasets. We further demonstrate its
                applicability on two downstream tasks, recommender systems and node classification.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.14012"
                   target="_blank">
                  Graph Posterior Network: Bayesian Predictive Uncertainty for Node Classification (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                15th of February 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Maximilian Stadler, Bertrand Charpentier, Simon Geisler, Daniel Z√ºgner, Stephan G√ºnnemann
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGraphPostNet' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their <a class="item-subtitle link-style-black"
                         href="https://hannes-stark.com/assets/.pdf"
                         target="_blank">slides</a> are coming soon. (83 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First authors <a
                class="item-subtitle link-style-black"
                href="https://maximilian-stadler.de/"
                target="_blank">Maximilian Stadler</a> and <a
                class="item-subtitle link-style-black"
                href="https://www.in.tum.de/en/daml/team/bertrand-charpentier/"
                target="_blank">Bertrand Charpentier</a>. Maximilian received his M.Sc.
                with high distinction from the Technical University of Munich. He wrote his thesis in the group of Prof.
                G√ºnnemann. In this group, Bertrand is a PhD candidate working on uncertainty estimation and ML for
                graphs
                in this group.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                The interdependence between nodes in graphs is key to improve class predictions on nodes and utilized in
                approaches like Label Propagation (LP) or in Graph Neural Networks (GNN). Nonetheless, uncertainty
                estimation for non-independent node-level predictions is under-explored. In this work, we explore
                uncertainty quantification for node classification in three ways: (1) We derive three axioms explicitly
                characterizing the expected predictive uncertainty behavior in homophilic attributed graphs. (2) We
                propose a new model Graph Posterior Network (GPN) which explicitly performs Bayesian posterior updates
                for
                predictions on interdependent nodes. GPN provably obeys the proposed axioms. (3) We extensively evaluate
                GPN and a strong set of baselines on semi-supervised node classification including detection of
                anomalous
                features, and detection of left-out classes. GPN outperforms existing approaches for uncertainty
                estimation in the experiments.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2111.14522"
                   target="_blank">
                  Understanding over-squashing and bottlenecks on graphs via curvature (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                8th of February 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, Michael M. Bronstein
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLUnderstandingOversquashing' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their <a class="item-subtitle link-style-black"
                         href="https://hannes-stark.com/assets/understanding_oversquashing_slides_feb22.pdf"
                         target="_blank">slides</a>. (95 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Jake Topping and Dr. <a class="item-subtitle link-style-black"
                                        href="https://scholar.google.com/citations?user=yzjjeqsAAAAJ&hl=en"
                                        target="_blank">Francesco Di Giovanni</a>. Jake is a PhD student at Oxford
                working with Prof. Xiaowen Dong and Prof. Michael
                Bronstein. Francesco obtained his PhD from University College London and is working together with Ben at
                Twitter Research.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Most graph neural networks (GNNs) use the message passing paradigm, in which node features are
                propagated
                on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as
                a
                factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This
                phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks
                where
                the number of k-hop neighbors grows rapidly with k. We provide a precise description of the
                over-squashing
                phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we
                introduce
                a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the
                over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to
                alleviate the over-squashing.
              </p>
            </div>
          </div>
          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.14961"
                   target="_blank">
                  Roto-translated Local Coordinate Frames For Interacting Dynamical Systems (NeurIPS 2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                1st of February 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Miltiadis Kofinas, Naveen Shankar Nagaraja, Efstratios Gavves
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLLoCS' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Miltiadis's <a class="item-subtitle link-style-black"
                               href="https://hannes-stark.com/assets/Kofinas_LoCS_LoGaG_presentation_1_February_2022.pdf"
                               target="_blank">slides</a>. (50 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper <a class="item-subtitle link-style-black"
                                             href="https://www.linkedin.com/in/miltiadiskofinas/"
                                             target="_blank">Miltiadis Kofinas</a> who is a PhD student at the
                University of Amsterdam.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Modelling interactions is critical in learning complex dynamical systems, namely systems of interacting
                objects with highly non-linear and time-dependent behaviour. A large class of such systems can be
                formalized as geometric graphs, i.e., graphs with nodes positioned in the Euclidean space given an
                arbitrarily chosen global coordinate system, for instance vehicles in a traffic scene. Notwithstanding
                the
                arbitrary global coordinate system, the governing dynamics of the respective dynamical systems are
                invariant to rotations and translations, also known as Galilean invariance. As ignoring these
                invariances
                leads to worse generalization, in this work we propose local coordinate frames per node-object to induce
                roto-translation invariance to the geometric graph of the interacting dynamical system. Further, the
                local
                coordinate frames allow for a natural definition of anisotropic filtering in graph neural networks.
                Experiments in traffic scenes, 3D motion capture, and colliding particles demonstrate that the proposed
                approach comfortably outperforms the recent state-of-the-art.
              </p>
            </div>
          </div>
          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2101.10050"
                   target="_blank">
                  Learning Parametrised Graph Shift Operators (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                25t of January 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  George Dasoulas, Johannes Lutzeyer, Michalis Vazirgiannis
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLPGSO' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their <a class="item-subtitle link-style-black"
                         href="https://hannes-stark.com/assets/LoGaG_PGSO_slides.pdf"
                         target="_blank">slides</a>. (51 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Paper author <a class="item-subtitle link-style-black"
                                href="https://gdasoulas.github.io/"
                                target="_blank">George Dasoulas</a>, who is a final year PhD
                student in both the Data Science and Mining group at the Ecole Polytechnique and Noah‚Äôs Ark Lab of
                Huawei
                Technologies France. Under the supervision of Prof. Michalis Vazirgiannis, he works in the field of
                Graph
                Representation Learning, studying the expressive power of Graph Neural Networks (GNNs). Using the gained
                theoretical understanding of GNN architectures, his goal is to make impactful contributions to
                real-world
                problems that are characterized by an inherent graph structure ranging from communication graphs to
                molecular networks. Dr. <a class="item-subtitle link-style-black"
                                           href="https://scholar.google.com/citations?user=OfT4ns8AAAAJ&hl=en"
                                           target="_blank">Johannes Lutzeyer</a> is currently a postdoctoral researcher
                in
                the Data Science and
                Mining group at the Ecole Polytechnique. He works in the field of Graph Representation Learning with a
                focus on Graph Neural Networks and their theoretical analysis. He completed his PhD thesis at Imperial
                College London, where he studied the spectral properties of different graph representation matrices such
                as the adjacency and Laplacian matrices.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                In many domains data is currently represented as graphs and therefore, the graph representation of this
                data becomes increasingly important in machine learning. Network data is, implicitly or explicitly,
                always
                represented using a graph shift operator (GSO) with the most common choices being the adjacency,
                Laplacian
                matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where
                specific parameter values result in the most commonly used GSOs and message-passing operators in graph
                neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are
                used
                in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included
                in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors
                independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are
                shown
                to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they
                are found to automatically replicate the GSO regularisation found in the literature. On several
                real-world
                datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in
                both node- and graph-classification tasks.
              </p>
            </div>
          </div>


          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.14056" target="_blank">
                  How to transfer algorithmic reasoning knowledge to learn new algorithms?</a> + <a
                class="item-subtitle link-style-black"
                href="https://arxiv.org/abs/2110.05442" target="_blank">
                Neural Algorithmic Reasoners are Implicit Planners (2021)</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                18th of January 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  First paper: Louis-Pascal A. C. Xhonneux, Andreea Deac, Petar Velickovic, Jian Tang
                </a>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Second paper: Andreea Deac, Petar Veliƒçkoviƒá, Ognjen Milinkoviƒá, Pierre-Luc Bacon, Jian Tang, Mladen
                  Nikoliƒá
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLNAR' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their <a class="item-subtitle link-style-black"
                         href="https://hannes-stark.com/assets/2021-NeurIPS-graph-neural-cellular-automata-long.pdf"
                         target="_blank">slides </a>. (85 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speakers:</a>

                First authors of the papers <a href="https://www.linkedin.com/in/louis-pascal-xhonneux-3a85a2141/"
                                               target="_blank"
                                               class="link-style-green">Luis-Pascal Xhonneux</a> and <a
                href="https://andreeadeac22.github.io/" target="_blank"
                class="link-style-green">Andreea-Ioana Deac</a>.
                Luis-Pascal did his master's at Cambridge and is now a PhD student at Mila. Andreea is a PhD student
                in Machine Learning at Mila, with Prof Jian Tang. She is broadly interested in how learning can be
                improved through the use of graph representations, having previously worked on algorithmic alignment
                for implicit planning and applications to biotechnology (drug discovery and drug combinations).
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract Paper 1:</a>
                Learning to execute algorithms is a fundamental problem that has been widely studied. Prior work has
                shown that to enable systematic generalisation on graph algorithms it is critical to have access to the
                intermediate steps of the program/algorithm. In many reasoning tasks, where algorithmic-style reasoning
                is important, we only have access to the input and output examples. Thus, inspired by the success of
                pre-training on similar tasks or data in Natural Language Processing (NLP) and Computer Vision, we set
                out to study how we can transfer algorithmic reasoning knowledge. Specifically, we investigate how we
                can use algorithms for which we have access to the execution trace to learn to solve similar tasks for
                which we do not. We investigate two major classes of graph algorithms, parallel algorithms such as
                breadth-first search and Bellman-Ford and sequential greedy algorithms such as Prim and Dijkstra. Due to
                the fundamental differences between algorithmic reasoning knowledge and feature extractors such as used
                in Computer Vision or NLP, we hypothesise that standard transfer techniques will not be sufficient to
                achieve systematic generalisation. To investigate this empirically we create a dataset including 9
                algorithms and 3 different graph types. We validate this empirically and show how instead multi-task
                learning can be used to achieve the transfer of algorithmic reasoning knowledge.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract Paper 2:</a>
                Implicit planning has emerged as an elegant technique for combining learned models of the world with
                end-to-end model-free reinforcement learning. We study the class of implicit planners inspired by value
                iteration, an algorithm that is guaranteed to yield perfect policies in fully-specified tabular
                environments. We find that prior approaches either assume that the environment is provided in such a
                tabular form -- which is highly restrictive -- or infer "local neighbourhoods" of states to run value
                iteration over -- for which we discover an algorithmic bottleneck effect. This effect is caused by
                explicitly running the planning algorithm based on scalar predictions in every state, which can be
                harmful to data efficiency if such scalars are improperly predicted. We propose eXecuted Latent Value
                Iteration Networks (XLVINs), which alleviate the above limitations. Our method performs all planning
                computations in a high-dimensional latent space, breaking the algorithmic bottleneck. It maintains
                alignment with value iteration by carefully leveraging neural graph-algorithmic reasoning and
                contrastive self-supervised learning. Across eight low-data settings -- including classical control,
                navigation and Atari -- XLVINs provide significant improvements to data efficiency against value
                iteration-based implicit planners, as well as relevant model-free baselines. Lastly, we empirically
                verify that XLVINs can closely align with value iteration.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.14237"
                   target="_blank">
                  Learning Graph Cellular Automata (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                11th of January 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Daniele Grattarola, Lorenzo Livi, Cesare Alippi
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGCA' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Daniele's <a class="item-subtitle link-style-black"
                             href="https://hannes-stark.com/assets/2021-NeurIPS-graph-neural-cellular-automata-long.pdf"
                             target="_blank">slides </a>. (90 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper Dr. <a class="item-subtitle link-style-black"
                                                 href="https://danielegrattarola.github.io/"
                                                 target="_blank">Daniele Grattarola</a> who is a post-doc researcher at
                EPFL, working in collaboration with A. Loukas, B. Correia, P. Vandergheynst and M. Bronstein. He
                obtained
                his PhD working in
                Graph Machine Learning Group at the Universit√† della Svizzera Italiana in 2021. His research is on graph
                neural networks and their applications to dynamical systems and computational biology, specifically for
                protein design.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Cellular automata (CA) are a class of computational models that exhibit rich dynamics emerging from the
                local interaction of cells arranged in a regular lattice. In this work we focus on a generalised version
                of typical CA, called graph cellular automata (GCA), in which the lattice structure is replaced by an
                arbitrary graph. In particular, we extend previous work that used convolutional neural networks to learn
                the transition rule of conventional CA and we use graph neural networks to learn a variety of transition
                rules for GCA. First, we present a general-purpose architecture for learning GCA, and we show that it
                can
                represent any arbitrary GCA with finite and discrete state space. Then, we test our approach on three
                different tasks: 1) learning the transition rule of a GCA on a Voronoi tessellation; 2) imitating the
                behaviour of a group of flocking agents; 3) learning a rule that converges to a desired target state.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.06935"
                   target="_blank">
                  Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                4th of January 2022
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, Jian Tang
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLNBFNets' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Zhaocheng's <a class="item-subtitle link-style-black"
                               href="https://hannes-stark.com/assets/NBFNetLoGaG.pdf"
                               target="_blank">slides </a>. (77 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a class="item-subtitle link-style-black"
                                href="https://kiddozhu.github.io/"
                                target="_blank">Zhaocheng Zhu</a>, who is a third-year Ph.D.
                candidate at Mila - Quebec AI Institute, University of Montreal, advised by Prof. Jian Tang. He works on
                graph representation learning, machine learning systems, and drug discovery.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Link prediction is a very fundamental task on graphs. Inspired by traditional path-based methods, in
                this
                paper we propose a general and flexible representation learning framework based on paths for link
                prediction. Specifically, we define the representation of a pair of nodes as the generalized sum of all
                path representations, with each path representation as the generalized product of the edge
                representations
                in the path. Motivated by the Bellman-Ford algorithm for solving the shortest path problem, we show that
                the proposed path formulation can be efficiently solved by the generalized Bellman-Ford algorithm. To
                further improve the capacity of the path formulation, we propose the Neural Bellman-Ford Network
                (NBFNet),
                a general graph neural network framework that solves the path formulation with learned operators in the
                generalized Bellman-Ford algorithm. The NBFNet parameterizes the generalized Bellman-Ford algorithm with
                3
                neural components, namely INDICATOR, MESSAGE and AGGREGATE functions, which corresponds to the boundary
                condition, multiplication operator, and summation operator respectively. The NBFNet is very general,
                covers many traditional path-based methods, and can be applied to both homogeneous graphs and
                multi-relational graphs (e.g., knowledge graphs) in both transductive and inductive settings.
                Experiments
                on both homogeneous graphs and knowledge graphs show that the proposed NBFNet outperforms existing
                methods
                by a large margin in both transductive and inductive settings, achieving new state-of-the-art results.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black"
                   href="https://physical-reasoning.github.io/assets/pdf/papers/06.pdf"
                   target="_blank">
                  Learning Graph Search Heuristics (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                28th of December 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Michal P√°ndy, Rex Ying, Gabriele Corso, Petar Veliƒçkoviƒá, Jure Leskovec, Pietro Li√≤
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGraphSearch' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Michal's <a class="item-subtitle link-style-black" href="https://youtu.be/FpXIGTFD8Qs"
                            target="_blank">slides </a> should be coming soon! (46 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper <a class="item-subtitle link-style-black"
                                             href="https://mpmisko.github.io/"
                                             target="_blank">Michal P√°ndy</a> who is an incoming ML Engineer at Waymo.
                He
                will join Prof. Shimon Whiteson 's team to work on self-driving cars. He did his master's at the
                University of Cambridge and is currently a research intern at Google Research.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Searching for a path between two nodes in a graph is one of the most well-studied and fundamental
                problems
                in computer science. In numerous domains such as robotics, AI, or biology, practitioners develop search
                heuristics to accelerate their pathfinding algorithms. However, it is a laborious and complex process to
                hand-design heuristics based on the problem and the structure of a given use case. Here we present PHIL
                (Path Heuristic with Imitation Learning), a novel neural architecture and a training algorithm for
                discovering graph search and navigation heuristics from data by leveraging recent advances in imitation
                learning and graph representation learning. At training time, we aggregate datasets of search
                trajectories
                and ground-truth shortest path distances, which we use to train a specialized graph neural network-based
                heuristic function using backpropagation through steps of the pathfinding process. Our heuristic
                function
                learns graph embeddings useful for inferring node distances, runs in constant time independent of graph
                sizes, and can be easily incorporated in an algorithm such as A* at test time. Experiments show that
                PHIL
                reduces the number of explored nodes compared to state-of-the-art methods on benchmark datasets by 40.8%
                on average and allows for fast planning in time-critical robotics domains.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.04624"
                   target="_blank">
                  Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                21st of December 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Wengong Jin, Jeremy Wohlwend, Regina Barzilay, Tommi Jaakkola
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLIterativeGNN' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                I recommend <a class="item-subtitle link-style-black" href="https://youtu.be/FpXIGTFD8Qs"
                               target="_blank">this lecture</a> on immunology to learn a bit about antibodies. (61
                participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author Dr. <a class="item-subtitle link-style-black" href="http://people.csail.mit.edu/wengong/"
                                    target="_blank">Wengong Jin</a> who is a Postdoctoral Associate at Eric and Wendy
                Schmidt Center of the Broad
                Institute. He finished his Ph.D. in MIT CSAIL, advised by <a class="item-subtitle link-style-black"
                                                                             href="https://www.regina.csail.mit.edu/"
                                                                             target="_blank">Regina Barzilay</a> and <a
                class="item-subtitle link-style-black" href="https://people.csail.mit.edu/tommi/"
                target="_blank">Tommi Jaakkola</a>. He develops
                novel machine learning algorithms for biology, including drug discovery, immunology, genetic
                engineering,
                and synthetic biology. He is particularly interested in deep generative models and graph neural
                networks.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Antibodies are versatile proteins that bind to pathogens like viruses and stimulate the adaptive immune
                system. The specificity of antibody binding is determined by complementarity-determining regions (CDRs)
                at
                the tips of these Y-shaped proteins. In this paper, we propose a generative model to automatically
                design
                the CDRs of antibodies with enhanced binding specificity or neutralization capabilities. Previous
                generative approaches formulate protein design as a structure-conditioned sequence generation task,
                assuming the desired 3D structure is given a priori. In contrast, we propose to co-design the sequence
                and
                3D structure of CDRs as graphs. Our model unravels a sequence autoregressively while iteratively
                refining
                its predicted global structure. The inferred structure in turn guides subsequent residue choices. For
                efficiency, we model the conditional dependence between residues inside and outside of a CDR in a
                coarse-grained manner. Our method achieves superior log-likelihood on the test set and outperforms
                previous baselines in designing antibodies capable of neutralizing the SARS-CoV-2 virus.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.02910"
                   target="_blank">
                  Equivariant Subgraph Aggregation Networks (2021)
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                14th of December 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath
                  Balamurugan, Michael M. Bronstein, Haggai Maron
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLESAN' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Their
                <a href="https://hannes-stark.com/assets/ESAN_Reading_Group_Slides.pdf"
                   target="_blank"
                   class="link-style-green">slides</a>.
                (35 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speakers:</a>
                Joint first authors <a class="item-subtitle link-style-black"
                                       href="https://www.linkedin.com/in/beabevi/"
                                       target="_blank">Beatrice Bevilacqua</a>, <a
                class="item-subtitle link-style-black"
                href="https://scholar.google.com/citations?user=PT2CDA4AAAAJ&hl=en"
                target="_blank">Fabrizio Frasca</a>,
                and <a class="item-subtitle link-style-black" href="https://cptq.github.io/"
                       target="_blank">Derek Lim</a>. Beatrice is a PhD Student at
                Purdue University with Prof. Bruno Ribeiro. Fabrizio is a PhD candidate at Imperial College London
                supervised by Prof. Michael Bronstein and he works as an ML researcher at Twitter. Derek is a PhD
                student
                at MIT under the supervision of Prof. Stefanie Jegelka.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Message-passing neural networks (MPNNs) are the leading architecture for deep learning on
                graph-structured
                data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these
                architectures are limited in their expressive power. This paper proposes a novel framework called
                Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that
                while
                two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we
                propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process
                it
                using a suitable equivariant architecture. We develop novel variants of the 1-dimensional
                Weisfeiler-Leman
                (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of
                these
                new WL variants. We further prove that our approach increases the expressive power of both MPNNs and
                more
                expressive architectures. Moreover, we provide theoretical results that describe how design choices such
                as the subgraph selection policy and equivariant neural architecture affect our architecture's
                expressive
                power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can
                be
                viewed as a stochastic version of our framework. A comprehensive set of experiments on real and
                synthetic
                datasets demonstrates that our framework improves the expressive power and overall performance of
                popular
                GNN architectures.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2110.07875"
                   target="_blank">
                  Graph Neural Networks with Learnable Structural and Positional Representations</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                30th of November 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, Xavier Bresson
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLLSPE' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Vijay's
                <a href="https://hannes-stark.com/assets/VPDwivedi_GNN_LSPE_LoGaG_2021.pdf"
                   target="_blank"
                   class="link-style-green">slides</a>.
                (71 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a class="item-subtitle link-style-black" href="https://vijaydwivedi.com.np/"
                                target="_blank">Vijay Prakash Dwivedi</a> who is a PhD student in Machine Learning at
                Nanyang Technological
                University, Singapore working with Prof. <a class="item-subtitle link-style-black"
                                                            href="https://tuanluu.github.io/"
                                                            target="_blank">Luu Anh Tuan</a> (NTU) and Prof. <a
                class="item-subtitle link-style-black"
                href="https://scholar.google.com/citations?user=9pSK04MAAAAJ&hl=en"
                target="_blank">Xavier Bresson</a> (NUS). His primary
                interest is ML for graph-structured data and he has contributed in "<a
                class="item-subtitle link-style-black" href="https://arxiv.org/abs/2003.00982"
                target="_blank">Benchmarking Graph Neural Networks</a>"
                and ‚Äú<a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2012.09699"
                        target="_blank">A Generalization of Transformer Networks to Graphs</a>‚Äù as part of his research.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been
                applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and
                natural language processing. A major issue with arbitrary graphs is the absence of canonical positional
                information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic
                nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding
                (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian
                eigenvectors. In this work, we propose to decouple structural and positional representations to make
                easy
                for the network to learn these two essential properties. We introduce a novel generic architecture which
                we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and
                fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from
                2.87% up to 64.14% when considering learnable PE for both GNN classes.
              </p>
            </div>
          </div>
          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://openreview.net/forum?id=Wi5KUNlqWty"
                   target="_blank">
                  How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                23rd of November 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Dongkwan Kim, Alice Oh
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Dongkwan's
              <a href="https://hannes-stark.com/assets/211124_SuperGAT@LoGaG.pdf"
                 target="_blank"
                 class="link-style-green">slides</a>.
              (48 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author <a class="item-subtitle link-style-black" href="https://dongkwan-kim.github.io/"
                              target="_blank">Dongkwan
              Kim</a> who is a Ph.D. student at KAIST School of Computing, advised by <a
              class="item-subtitle link-style-black" href="https://aliceoh9.github.io/" target="_blank">Alice Oh</a>.
              His
              research interest is graph representation learning models and applications to
              social networks, code, or molecules.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor
              nodes for better representation. However, what graph attention learns is not understood well,
              particularly
              when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT),
              an
              improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible
              with a self-supervised task to predict edges, whose presence and absence contain the inherent
              information
              about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more
              expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence
              the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our
              recipe
              provides guidance on which attention design to use when those two graph characteristics are known. Our
              experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of
              them,
              and our models designed by recipe show improved performance over baselines.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/1905.12560"
                   target="_blank">
                  On the equivalence between graph isomorphism testing and function approximation with GNNs
                </a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                16th of November 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Zhengdao Chen, Soledad Villar, Lei Chen, Joan Bruna
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Zhengdao's
              <a href="https://hannes-stark.com/assets/Slides_Zhengdao_Chen_LoGaG_11162021_2.pdf"
                 target="_blank"
                 class="link-style-green">slides</a>.
              (58 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>

              Joint authors of the paper <a class="item-subtitle link-style-black"
                                            href="https://cims.nyu.edu/~chenzh/">Zhengdao Chen</a> and Prof. <a
              class="item-subtitle link-style-black" href="https://www.ams.jhu.edu/villar/">Soledad Villar</a>. Zhengdao
              is a PhD
              student in Mathematics at New York University. Prof. Villar holds her position at the
              <a
                class="item-subtitle link-style-black" href="https://engineering.jhu.edu/ams/">Department of Applied
                Mathematics & Statistics</a>, and <a
              class="item-subtitle link-style-black" href="https://www.minds.jhu.edu/">Mathematical Institute for Data
              Science</a> at Johns Hopkins University. She is interested in ML, optimization, graph
              representation learning, and GNNs.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Graph neural networks (GNNs) have achieved lots of success on graph-structured data. In the light of this,
              there has been increasing interest in studying their representation power. One line of work focuses on the
              universal approximation of permutation-invariant functions by certain classes of GNNs, and another
              demonstrates the limitation of GNNs via graph isomorphism tests.
              Our work connects these two perspectives and proves their equivalence. We further develop a framework of
              the representation power of GNNs with the language of sigma-algebra, which incorporates both viewpoints.
              Using this framework, we compare the expressive power of different classes of GNNs as well as other
              methods on graphs. In particular, we prove that order-2 Graph G-invariant networks fail to distinguish
              non-isomorphic regular graphs with the same degree. We then extend them to a new architecture, Ring-GNNs,
              which succeeds on distinguishing these graphs and provides improvements on real-world social network
              datasets.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2109.04173" target="_blank">
                  Relating Graph Neural Networks to Structural Causal Model</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                9th of November 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Matej Zeƒçeviƒá, Devendra Singh Dhami, Petar Veliƒçkoviƒá, Kristian Kersting
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGNNSCM' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Matej's
                <a href="https://hannes-stark.com/assets/MZ-NeuroCausality-Talk-LoGaG.pdf"
                   target="_blank"
                   class="link-style-green">slides</a>.
                (100 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a class="item-subtitle link-style-black" href="https://matej-zecevic.de/" target="_blank">
                Matej Zeƒçeviƒá</a> who is a PhD candidate at TU Darmstadt working with <a
                class="item-subtitle link-style-black" href="https://ml-research.github.io/people/kkersting/index.html"
                target="_blank">
                Kristian Kersting</a> on
                Causality for ML. Before that, he completed his M.Sc. in Computer Science under <a
                class="item-subtitle link-style-black"
                href="https://www.ias.informatik.tu-darmstadt.de/Member/JanPeters"
                target="_blank">
                Jan Peters</a>, <a
                class="item-subtitle link-style-black" href="https://cifar.ca/bios/stefan-bauer/" target="_blank">
                Stefan Bauer</a>,
                and <a
                class="item-subtitle link-style-black" href="https://www.is.mpg.de/~bs" target="_blank">
                Bernhard Sch√∂lkopf</a> at MPI for Intelligent Systems (T√ºbingen).
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Causality can be described in terms of a structural causal model (SCM) that carries information on the
                variables of interest and their mechanistic relations. For most processes of interest the underlying SCM
                will only be partially observable, thus causal inference tries to leverage any exposed information.
                Graph
                neural networks (GNN) as universal approximators on structured input pose a viable candidate for causal
                learning, suggesting a tighter integration with SCM. To this effect we present a theoretical analysis
                from
                first principles that establishes a novel connection between GNN and SCM while providing an extended
                view
                on general neural-causal models. We then establish a new model class for GNN-based causal inference that
                is necessary and sufficient for causal effect identification. Our empirical illustration on simulations
                and standard benchmarks validate our theoretical proofs.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2102.06790" target="_blank">
                  A Unified Lottery Ticket Hypothesis for Graph Neural Networks</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                2nd of November 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, Zhangyang Wang
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLLTH' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Tianlong's
                <a href="https://hannes-stark.com/assets/Paper Reading Presentation in LoGaG.pdf"
                   target="_blank"
                   class="link-style-green">slides</a>.
                (54 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author <a class="item-subtitle link-style-black" href="https://tianlong-chen.github.io/about/"
                                target="_blank">Tianlong Chen (ÈôàÂ§©Èæô)</a> who is a third-year Ph.D. student of Electrical
                and Computer Engineering
                (DICE) at VITA, The University of Texas at Austin, advised by Dr. <a
                class="item-subtitle link-style-black" href="https://vita-group.github.io/" target="_blank">Zhangyang
                (Atlas) Wang</a>. His research
                interests include AutoML, Adversarial Robustness, Self-Supervision, and Graph Neural Networks.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                With graphs rapidly growing in size and deeper graph neural networks (GNNs) emerging, the training and
                inference of GNNs become increasingly expensive. Existing network weight pruning algorithms cannot
                address
                the main space and computational bottleneck in GNNs, caused by the size and connectivity of the graph.
                To
                this end, this paper first presents a unified GNN sparsification (UGS) framework that simultaneously
                prunes the graph adjacency matrix and the model weights, for effectively accelerating GNN inference on
                large-scale graphs. Leveraging this new tool, we further generalize the recently popular lottery ticket
                hypothesis to GNNs for the first time, by defining a graph lottery ticket (GLT) as a pair of core
                sub-dataset and sparse sub-network, which can be jointly identified from the original GNN and the full
                dense graph by iteratively applying UGS. Like its counterpart in convolutional neural networks, GLT can
                be
                trained in isolation to match the performance of training with the full model and graph, and can be
                drawn
                from both randomly initialized and self-supervised pre-trained GNNs. Our proposal has been
                experimentally
                verified across various GNN architectures and diverse tasks, on both small-scale graph datasets (Cora,
                Citeseer and PubMed), and large-scale datasets from the challenging Open Graph Benchmark (OGB).
                Specifically, for node classification, our found GLTs achieve the same accuracies with 20%~98% MACs
                saving
                on small graphs and 25%~85% MACs saving on large ones. For link prediction, GLTs lead to 48%~97% and 70%
                MACs saving on small and large graph datasets, respectively, without compromising predictive
                performance.
                Codes available at <a class="item-subtitle link-style-black"
                                      href="https://github.com/VITA-Group/Unified-LTH-GNN" target="_blank">this https
                URL</a>.
              </p>
            </div>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.10934" target="_blank">
                  GRAND: Graph Neural Diffusion</a> + <a class="item-subtitle link-style-black"
                                                         href="https://arxiv.org/abs/2110.09443" target="_blank">
                Beltrami Flow and Neural Diffusion on Graphs</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                26th of October 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  First paper: Benjamin Paul Chamberlain, James Rowbottom, Maria Gorinova, Stefan Webb, Emanuele Rossi,
                  Michael M.
                  Bronstein
                </a>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Second paper: Benjamin Paul Chamberlain, James Rowbottom, Davide Eynard, Francesco Di Giovanni,
                  Xiaowen Dong, Michael M Bronstein
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLGRAND' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Ben's and James's
                <a href="https://hannes-stark.com/assets/Graph_Neural_Networks_and_Diffusion_PDEs.pdf"
                   target="_blank"
                   class="link-style-green">slides</a>.
                (75 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Joint first authors of the paper
                <a href="https://www.linkedin.com/in/jamesrowbottom/" target="_blank"
                   class="link-style-green">James Rowbottom</a> and Dr. <a
                href="https://www.linkedin.com/in/benjaminchamberlain/" target="_blank"
                class="link-style-green">Benjamin Paul Chamberlain</a>.
                James received his master in AI with distinction from Imperial College London and worked on GRAND as an
                ML
                Reserch intern at Prof.
                <a href="https://www.imperial.ac.uk/people/m.bronstein" target="_blank"
                   class="link-style-green">Michael Bronstein's</a>
                graph ML research group at Twitter. Ben is a Machine Learning Researcher at Twitter who received his PhD
                from Imperial College London.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract GRAND:</a>
                We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous
                diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In
                our
                model, the layer structure and topology correspond to the discretisation choices of temporal and spatial
                operators. Our approach
                allows a principled development of a broad new class of GNNs that are able to address the common plights
                of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models
                are stability with respect to perturbations in the data and this is addressed for both implicit and
                explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve
                competitive results on many standard graph benchmarks.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract BLEND:</a>
                We propose a novel class of graph neural networks based on the discretised Beltrami flow, a
                non-Euclidean
                diffusion PDE. In our model, node features are supplemented with positional encodings derived from the
                graph topology and jointly evolved by the Beltrami flow, producing simultaneously continuous feature
                learning and topology evolution. The resulting model generalises many popular graph neural networks and
                achieves state-of-the-art results on several benchmarks.
              </p>
            </div>
          </div>
          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2006.05205" target="_blank">
                  On the Bottleneck of Graph Neural Networks and its Practical Implications</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                19th of October 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Uri Alon, Eran Yahav
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLoversquashing' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Uri's
                <a href="https://hannes-stark.com/assets/bottleneck-tum.pdf"
                   target="_blank"
                   class="link-style-green">slides</a>.
                (50 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                First author of the paper Dr. <a href="https://urialon.ml/" target="_blank"
                                                 class="link-style-green">Uri Alon</a> who obtained my PhD at Technion
                (Israel), advised by Prof. <a href="https://www.cs.technion.ac.il/~yahave/" target="_blank"
                                              class="link-style-green">Eran Yahav</a>. He is now a Postdoctoral
                Researcher
                at the Language Technologies Institute of Carnegie Mellon
                University, working with Prof. <a href="http://www.phontron.com/" target="_blank"
                                                  class="link-style-green">Graham Neubig</a> on NLP and learning from
                source code, where GNNs find many
                applications. My most recent GNN work includes "<a href="https://openreview.net/pdf?id=i80OPhOCVH2"
                                                                   target="_blank"
                                                                   class="link-style-green">On the Bottleneck of Graph
                Neural Networks and its
                Practical Implications</a>" and "<a href="https://arxiv.org/pdf/2105.14491.pdf" target="_blank"
                                                    class="link-style-green">How Attentive are Graph Attention
                Networks?</a>".
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008),
                one of the major problems in training GNNs was their struggle to propagate information between distant
                nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck
                when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially
                growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating
                from
                distant nodes and perform poorly when the prediction task depends on long-range interaction. In this
                paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck
                hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs
                that
                absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and
                GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems,
                suffers
                from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without
                any
                tuning or additional weights. Our code is available at this <a
                href="https://github.com/tech-srl/bottleneck/" target="_blank"
                class="link-style-green">https URL</a>.
              </p>
            </div>
          </div>


          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2102.05152" target="_blank">
                  On Explainability of Graph Neural Networks via Subgraph Explorations</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                12th of October 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, Shuiwang Ji
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Haiyang's
              <a href="https://hannes-stark.com/assets/On Explainability of Graph Neural Networks via Subgraph.pdf"
                 target="_blank"
                 class="link-style-green">slides</a>.
              (59 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              Author of the paper
              <a href="https://scholar.google.com/citations?user=LZKU1hUAAAAJ&hl=en" target="_blank"
                 class="link-style-green">Haiyang Yu</a> who is a PhD student
              at Texas A&M University supervised by Prof. <a href="http://people.tamu.edu/~sji/" target="_blank"
                                                             class="link-style-green">Shuiwang Ji</a>.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are
              considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes
              or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this
              work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs.
              Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently
              exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we
              propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions
              among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute
              Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying
              subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly
              improved explanations, while keeping computations at a reasonable level.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2010.09891" target="_blank">
                  FLAG: Adversarial Data Augmentation for Graph Neural Networks</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                28th of September 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, Tom Goldstein
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Kezhi's
              <a href="https://hannes-stark.com/assets/FLAG_slides.pdf" target="_blank"
                 class="link-style-green">slides</a>.
              (46 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author <a
              href="https://scholar.google.com/citations?user=MG46jrMAAAAJ&hl=en" target="_blank"
              class="link-style-green">Kezhi Kong</a> who is a PhD student at the University of Maryland. Advised by
              Prof. Tom Goldstein, he does research in Machine Learning, with a focus on Graph Learning and
              Adversarial
              Attacks/Defenses.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Data augmentation helps neural networks generalize better, but it remains an open question how to
              effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most
              existing graph regularizers focus on augmenting graph topological structures by adding/removing edges,
              we
              offer a novel direction to augment in the input node feature space for better performance. We propose a
              simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which
              iteratively augments node features with gradient-based adversarial perturbations during training, and
              boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code
              and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and
              in both transductive and inductive settings. Without modifying a model's architecture or training setup,
              FLAG yields a consistent and salient performance boost across both node and graph classification tasks.
              Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and
              ogbg-code
              datasets.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.03893" target="_blank">
                  Rethinking Graph Transformers with Spectral Attention</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                21st of September 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Devin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent L√©tourneau, Prudencio Tossou
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLSAN' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Devin's
                <a href="https://hannes-stark.com/assets/spectral_attention_networks.pdf" target="_blank"
                   class="link-style-green">slides</a>.
                (69 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Joint first authors of the paper: Dr.
                <a href="https://www.linkedin.com/in/dbeaini/" target="_blank"
                   class="link-style-green">Dominique Beaini</a> who obtained his PhD at √âcole Polytechnique de
                Montr√©al
                and is now working as ML Researcher at Valence Discovery on using GNNs for molecules.
                And <a href="https://www.linkedin.com/in/devin-kreuzer-847b52b5/" target="_blank"
                       class="link-style-green">Devin Kreuzer</a> who is doing his masters at McGill University
                supervised
                by Prof. <a href="https://williamleif.github.io/" target="_blank"
                            class="link-style-green">William L. Hamilton</a> while working at MILA on GNNs and
                AI-enabled
                drug discovery.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                In recent years, the Transformer architecture has proven to be very successful in sequence processing,
                but
                its application to other data structures, such as graphs, has remained limited due to the difficulty
                of
                properly defining positions. Here, we present the Spectral Attention Network (SAN), which uses a
                learned
                positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position
                of
                each node in a given graph. This LPE is then added to the node features of the graph and passed to a
                fully-connected Transformer. By leveraging the full spectrum of the Laplacian, our model is
                theoretically
                powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance.
                Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an
                information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat
                transfer and electric interaction. When tested empirically on a set of 4 standard datasets, our model
                performs on par or better than state-of-the-art GNNs, and outperforms any attention-based model by a
                wide
                margin, becoming the first fully-connected architecture to perform well on graph benchmarks.
              </p>
            </div>
          </div>


          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2107.01952" target="_blank">
                  Partition and Code: learning how to compress graphs</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                14th of September 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Giorgos Bouritsas, Andreas Loukas, Nikolaos Karalias, Michael M. Bronstein
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Giorgos's slides as
              <a href="https://hannes-stark.com/assets/PnC_slides_logag.pdf" target="_blank"
                 class="link-style-green">pdf</a> or <a href="https://hannes-stark.com/assets/PnC_slides_logag.key"
                                                        target="_blank"
                                                        class="link-style-green">keynote</a>.
              (67 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              The main presenter will be first author <a
              href="https://www.linkedin.com/in/giorgos-bouritsas/?originalSubdomain=uk" target="_blank"
              class="link-style-green">Giorgos Bouritsas</a> who is a PhD student at Imperial College London
              under the supervision of Prof. <a
              href="https://www.imperial.ac.uk/people/m.bronstein" target="_blank"
              class="link-style-green">Michael Bronstein</a> and he is currently a visiting PhD at EPFL, Switzerland.
              Also joining us will be paper author <a
              href="https://andreasloukas.blog/" target="_blank"
              class="link-style-green">Dr. Andreas Loukas</a> who is a research scientist (Ambizione fellow) at
              the LTS2 lab in EPFL.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Can we use machine learning to compress graph data? The absence of ordering in graphs poses a
              significant
              challenge to conventional compression algorithms, limiting their attainable gains as well as their
              ability
              to discover relevant patterns. On the other hand, most graph compression approaches rely on
              domain-dependent handcrafted representations and cannot adapt to different underlying graph
              distributions.
              This work aims to establish the necessary principles a lossless graph compression method should follow
              to
              approach the entropy storage lower bound. Instead of making rigid assumptions about the graph
              distribution, we formulate the compressor as a probabilistic model that can be learned from data and
              generalise to unseen instances. Our "Partition and Code" framework entails three steps: first, a
              partitioning algorithm decomposes the graph into elementary structures, then these are mapped to the
              elements of a small dictionary on which we learn a probability distribution, and finally, an entropy
              encoder translates the representation into bits. All three steps are parametric and can be trained with
              gradient descent. We theoretically compare the compression quality of several graph encodings and prove,
              under mild conditions, a total ordering of their expected description lengths. Moreover, we show that,
              under the same conditions, PnC achieves compression gains w.r.t. the baselines that grow either linearly
              or quadratically with the number of vertices. Our algorithms are quantitatively evaluated on diverse
              real-world networks obtaining significant performance improvements with respect to different families of
              non-parametric and parametric graph compressors.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.12575" target="_blank">
                  Weisfeiler and Lehman Go Cellular: CW Networks</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                7th of September 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yu Guang Wang, Pietro Li√≤, Guido Mont√∫far, Michael
                  Bronstein
                </a>
              </div>
            </div>


            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLCWNetworks' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                The <a href="https://youtu.be/MTQGNVTn9lQ" target="_blank"
                       class="link-style-green">Recording</a> of the whole presentation.
                Cristian's and Fabrizio's slides
                <a href="https://crisbodnar.github.io/files/cwn_logag_talk.pdf" target="_blank"
                   class="link-style-green">Slides</a>. My
                <a
                  href="https://hannes-stark.com/assets/Weisfeiler and Lehman Go Cellular CW Networks.pdf"
                  target="_blank"
                  class="link-style-green">Paper Annotations</a>.
                (49 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Joint first authors of the paper
                <a href="https://crisbodnar.github.io/" target="_blank"
                   class="link-style-green">Cristian Bodnar</a> and <a
                href="https://scholar.google.com/citations?user=PT2CDA4AAAAJ&hl=en" target="_blank"
                class="link-style-green">Fabrizio Frasca</a>. Cristian is a second-year PhD student
                at Cambridge supervised by Prof. <a href="https://www.cl.cam.ac.uk/~pl219/" target="_blank"
                                                    class="link-style-green">Pietro Li√≤</a>. He works on topological
                and
                geometric deep learning. Fabrizio
                is a PhD candidate at Imperial College London supervised by Prof. <a
                href="https://www.imperial.ac.uk/people/m.bronstein" target="_blank"
                class="link-style-green">Michael Bronstein</a> and he works as an ML
                researcher at Twitter.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Graph Neural Networks (GNNs) are limited in their expressive power, struggle with long-range
                interactions
                and lack a principled way to model higher-order structures. These problems can be attributed to the
                strong
                coupling between the computational graph and the input graph structure. The recently proposed Message
                Passing Simplicial Networks naturally decouple these elements by performing message passing on the
                clique
                complex of the graph. Nevertheless, these models are severely constrained by the rigid combinatorial
                structure of Simplicial Complexes (SCs). In this work, we extend recent theoretical results on SCs to
                regular Cell Complexes, topological objects that flexibly subsume SCs and graphs. We show that this
                generalisation provides a powerful set of graph ``lifting'' transformations, each leading to a unique
                hierarchical message passing procedure. The resulting methods, which we collectively call CW Networks
                (CWNs), are strictly more powerful than the WL test and, in certain cases, not less powerful than the
                3-WL
                test. In particular, we demonstrate the effectiveness of one such scheme, based on rings, when applied
                to
                molecular graph problems. The proposed architecture benefits from provably larger expressivity than
                commonly used GNNs, principled modelling of higher-order signals and from compressing the distances
                between nodes. We demonstrate that our model achieves state-of-the-art results on a variety of
                molecular
                datasets.
              </p>

            </div>

          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://openreview.net/forum?id=-qh0M9XWxnv"
                   target="_blank">
                  Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                31st of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Muhammet Balcilar, Guillaume Renton, Pierre H√©roux, Benoit Ga√ºz√®re, S√©bastien Adam, Paul Honeine
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Muhammet's
              <a href="https://hannes-stark.com/assets/structural_spectral_awareness_GNN.pdf" target="_blank"
                 class="link-style-green">Slides</a>.
              (54 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper Dr.
              <a href="https://scholar.google.com.tr/citations?user=LRyde44AAAAJ&hl=en" target="_blank"
                 class="link-style-green">Muhammet Balcilar</a>
              who works as R&I Researcher at Interdigital, Rennes. He obtained his Ph.D. at Yildiz Technical
              University
              and has held several PostDoc positions since then.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              In the recent literature of Graph Neural Networks (GNN), the expressive power of models has been studied
              through their capability to distinguish if two given graphs are isomorphic or not. Since the graph
              isomorphism problem is NP-intermediate, and Weisfeiler-Lehman (WL) test can give sufficient but not
              enough
              evidence in polynomial time, the theoretical power of GNNs is usually evaluated by the equivalence of
              WL-test order, followed by an empirical analysis of the models on some reference inductive and
              transductive datasets. However, such analysis does not account the signal processing pipeline, whose
              capability is generally evaluated in the spectral domain. In this paper, we argue that a spectral
              analysis
              of GNNs behavior can provide a complementary point of view to go one step further in the understanding
              of
              GNNs. By bridging the gap between the spectral and spatial design of graph convolutions, we
              theoretically
              demonstrate some equivalence of the graph convolution process regardless it is designed in the spatial
              or
              the spectral domain. Using this connection, we managed to re-formulate most of the state-of-the-art
              graph
              neural networks into one common framework. This general framework allows to lead a spectral analysis of
              the most popular GNNs, explaining their performance and showing their limits according to spectral point
              of view. Our theoretical spectral analysis is confirmed by experiments on various graph databases.
              Furthermore, we demonstrate the necessity of high and/or band-pass filters on a graph dataset, while the
              majority of GNN is limited to only low-pass and inevitably it fails.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2103.06857" target="_blank">
                  Should Graph Neural Networks Use Features, Edges, Or Both?</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                24th of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Lukas Faber, Yifan Lu, Roger Wattenhofer
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a> My
              <a
                href="https://hannes-stark.com/assets/Should Graph Neural Networks Use Edges Features of Both.pdf"
                target="_blank"
                class="link-style-green">Paper Annotations</a>.
              (48 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper
              <a href="https://disco.ethz.ch/members/lfaber" target="_blank"
                 class="link-style-green">Lukas Faber</a>
              who is a PhD student in the Distributed Computing Group at ETH Z√ºrich supervised by Prof. Roger
              Wattenhofer. He is also working at Google Z√ºrich.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Graph Neural Networks (GNNs) are the first choice for learning algorithms on graph data. GNNs promise to
              integrate (i) node features as well as (ii) edge information in an end-to-end learning algorithm. How
              does
              this promise work out practically? In this paper, we study to what extend GNNs are necessary to solve
              prominent graph classification problems. We find that for graph classification, a GNN is not more than
              the
              sum of its parts. We also find that, unlike features, predictions with an edge-only model do not always
              transfer to GNNs.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.07594" target="_blank">
                  Graph Contrastive Learning Automated</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                17th of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Yuning You, Tianlong Chen, Yang Shen, Zhangyang Wang
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a> My
              <a
                href="https://hannes-stark.com/assets/Graph Contrastive Learning Automated.pdf"
                target="_blank"
                class="link-style-green">Paper Annotations</a>.
              Yuning's
              <a href="https://hannes-stark.com/assets/yuning_LoGaG_talk.pdf" target="_blank"
                 class="link-style-green">Slides</a>.
              (32 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper
              <a href="https://yyou1996.github.io/" target="_blank" class="link-style-green">Yuning You</a>
              who is a third-year Ph.D. student in ECE at Texas A&M University supervised by Prof.
              <a href="https://shen-lab.github.io/" target="_blank" class="link-style-green">Yang Shen</a>,
              and unofficially co-supervised by Prof. Zhangyang Wang. He has done a lot of popular work on
              self-supervised learning on graphs.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Self-supervised learning on graph-structured data has drawn recent interest for learning generalizable,
              transferable and robust representations from unlabeled graphs. Among many, graph contrastive learning
              (GraphCL) has emerged with promising representation learning performance. Unfortunately, unlike its
              counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data augmentations, which have
              to be manually picked per dataset, by either rules of thumb or trial-and-errors, owing to the diverse
              nature of graph data. That significantly limits the more general applicability of GraphCL. Aiming to
              fill in this crucial gap, this paper proposes a unified bi-level optimization framework to
              automatically, adaptively and dynamically select data augmentations when performing GraphCL on specific
              graph data. The general framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as
              min-max optimization. The selections of augmentations made by JOAO are shown to be in general aligned
              with previous "best practices" observed from handcrafted tuning: yet now being automated, more flexible
              and versatile. Moreover, we propose a new augmentation-aware projection head mechanism, which will route
              output features through different projection heads corresponding to different augmentations chosen at
              each training step. Extensive experiments demonstrate that JOAO performs on par with or sometimes better
              than the state-of-the-art competitors including GraphCL, on multiple graph datasets of various scales
              and types, yet without resorting to any laborious dataset-specific tuning on augmentation selection. We
              release the code at this https URL.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2105.04550" target="_blank">
                  Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth</a>
              </h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                10th of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, Kenji Kawaguchi
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a> My
              <a
                href="https://hannes-stark.com/assets/Optimization%20of%20Graph%20Neural%20Networks%20Implicit%20Acceleration%20by%20Skip%20Connections%20and%20More%20Depth.pdf"
                target="_blank"
                class="link-style-green">Paper Annotations</a>.
              Keyulu's
              <a href="https://people.csail.mit.edu/keyulux/pdf/optimization.pdf" target="_blank"
                 class="link-style-green">Slides</a>.
              (50 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper Dr. <a href="https://people.csail.mit.edu/keyulux/" target="_blank"
                                               class="link-style-green">Keyulu Xu</a>: He received his Ph.D. in EECS
              from MIT, where he was affiliated
              with CSAIL and advised by <a href="https://people.csail.mit.edu/stefje/" target="_blank"
                                           class="link-style-green">Stefanie Jegelka</a>. His papers got multiple
              spotlights + orals and one was, for
              instance, the highest reviewed paper at ICLR 2021. Also joining us is paper author <a
              href="http://www.mozhi.umiacs.io/" target="_blank" class="link-style-green">Mozhi Zhang</a> who is a
              last year PhD student at the University of Maryland working with Jordan Boyd-Graber as advisor on
              generalization properties of neural networks among other topics.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Graph Neural Networks (GNNs) have been studied through the lens of expressive power and generalization.
              However, their optimization properties are less well understood. We take the first step towards
              analyzing
              GNN training by studying the gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that
              despite the non-convexity of training, convergence to a global minimum at a linear rate is guaranteed
              under mild assumptions that we validate on real world graphs. Second, we study what may affect the GNNs‚Äô
              training speed. Our results show that the training of GNNs is implicitly accelerated by skip
              connections,
              more depth, and/or a good label distribution. Empirical results confirm that our theoretical results for
              linearized GNNs align with the training behavior of nonlinear GNNs. Our results provide the first
              theoretical support for the success of GNNs with skip connections in terms of optimization, and suggest
              that deep GNNs with skip connections would be promising in practice.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.05234" target="_blank">
                  Do Transformers Really Perform Bad for Graph Representation?</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                3rd of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a> My
              <a
                href="https://hannes-stark.com/assets/Do%20Transformers%20Really%20Perform%20Bad%20for%20Graph%20Representation.pdf"
                target="_blank"
                class="link-style-green">Paper Annotations</a>.
              (35 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              Hey thats me!
              <a href="https://hannes-stark.com/" target="_blank"
                 class="link-style-green">Hannes St√§rk</a>.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              The Transformer architecture has become a dominant choice in many domains, such as natural language
              processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards
              of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how
              Transformers could perform well for graph representation learning. In this paper, we solve this mystery
              by
              presenting Graphormer, which is built upon the standard Transformer architecture, and could attain
              excellent results on a broad range of graph representation learning tasks, especially on the recent OGB
              Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of
              effectively encoding the structural information of a graph into the model. To this end, we propose
              several
              simple yet effective structural encoding methods to help Graphormer better model graph-structured data.
              Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our
              ways
              of encoding the structural information of graphs, many popular GNN variants could be covered as the
              special cases of Graphormer.
            </p>
          </div>
        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>
    </div>
  </div>
</div>

