<div fxLayout="column" fxLayoutAlign="center center">

  <div class="width100" fxLayout="row" fxLayout.lt-md="column" fxLayoutAlign="center center">
    <div fxFlex="35" fxLayoutAlign="end center" fxLayoutAlign.lt-md="center center">
      <img class="profile-image" src="assets/randomEuclidean.png" alt="Euclid Elements">
    </div>
    <div fxFlex="55" fxFlex.xs="65" ngClass.sm="small-margin" ngClass.xs="small-margin--xs">
      <h1 class="Title" ngClass.lt-md="text-align-center">The Learning on Graphs and Geometry Reading Group!</h1>
      <!--
      <div class="underline-title--top" fxLayoutAlign.lt-md="center center">
        <h1 class="left-titles">M. Sc. Computer Science Student</h1>
      </div>
      -->
      <div class="item" fxLayout="column" fxLayoutAlign.lt-md="center center">
        <p class="text-align-left">
          Welcome to the LoGaG reading group that I am organizing with the incredible support of Prof.
          <a
            href="https://www.cl.cam.ac.uk/~pl219/" target="_blank" class="link-style-green">Pietro Liò</a>!
          Below the schedule you can find previously covered papers with my Notes and other resources that came out of
          the meeting.
        </p>
        <p>
          <a
            href="https://us02web.zoom.us/j/4556095405?pwd=UmZRbC95d050TkRsZ2QvWER3SDdqUT09" target="_blank"
            class="link-style-green">Zoom link</a>: We meet every Tuesday at 5pm CEST / 3pm UTC / 11am EST / 8am PST
        </p>
        <p>
          <a
            href="https://join.slack.com/t/logag/shared_invite/zt-u0mbo1ec-zElmvd1oSCXGjXvxLSokvg"
            target="_blank"
            class="link-style-green">Slack Workspace</a>: For discussions outside of our meetings and to vote for
          papers.
        </p>
        <p>
          Watch our
          <a
            href="https://www.youtube.com/channel/UC4uWMmEGc5EZVn5pAox-iww/videos"
            target="_blank"
            class="link-style-green">recorded meetings</a>!
        </p>
        <p>
          You can also subscribe to the meetings via <a
          href="https://calendar.google.com/calendar/u/0?cid=dmR1am4ycGJwa2hncjVmNTVjbTM5cWJtdThAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ"
          target="_blank"
          class="link-style-green">Google Calendar</a>, or <a
          href="https://calendar.google.com/calendar/ical/vdujn2pbpkhgr5f55cm39qbmu8%40group.calendar.google.com/public/basic.ics"
          target="_blank"
          class="link-style-green">via iCal</a>. Alternatively, <a
          href="https://calendar.google.com/event?action=TEMPLATE&tmeid=MWhmZDJzMnI0aG10YTlvZzZpZGVsYnJmaWNfMjAyMTA4MDNUMTUwMDAwWiB2ZHVqbjJwYnBraGdyNWY1NWNtMzlxYm11OEBn&tmsrc=vdujn2pbpkhgr5f55cm39qbmu8%40group.calendar.google.com&scp=ALL"
          target="_blank"
          class="link-style-green">add the events</a>.
        </p>
        <div class="item" fxLayout="column" fxLayoutAlign.lt-md="center center">
          <div fxLayout="row">
            <h3>
              <a class="item-title">Get weekly updates with the next paper and all other information via email!</a>
            </h3>
          </div>
          <div class="item-content">
            <div>
              <form class="contact" [formGroup]="form" method="post" (ngSubmit)="onSubmit()">
                <div fxLayout="row" fxLayout.lt-md="column">
                  <mat-form-field>
                    <mat-label>Name (optional)</mat-label>
                    <input matInput [formControl]="name">
                    <mat-error *ngIf="name.invalid">Please enter your name</mat-error>
                  </mat-form-field>
                  <mat-form-field [ngClass.gt-sm]="'margin-left'">
                    <mat-label>Email</mat-label>
                    <input matInput [formControl]="email" placeholder="email@example.com" required type="email">
                    <mat-error *ngIf="email.invalid">Please enter a valid email</mat-error>
                  </mat-form-field>
                  <!--
                  <mat-form-field>
                    <mat-label>Message</mat-label>
                    <textarea matInput [formControl]="message" placeholder="I'm interested in..."
                              ></textarea>
                    <mat-error *ngIf="message.invalid">Please enter a message</mat-error>
                  </mat-form-field>-->
                </div>
                <div style="text-align:center">
                  <button mat-button [class.spinner]="isLoading" [disabled]="isLoading" class="submit submit-button"
                          type="submit">
                    Submit
                  </button>
                </div>
                <input [formControl]="honeypot" class="hidden" type="text"/>
                <div [ngClass]="!(success && submit)? 'hidden' : 'visible'" class="success-message">
                  <span>{{responseMessage}}</span>
                </div>
                <div [ngClass]="!(!success && submit)? 'hidden' : 'visible'" class="failed-message">
                  <span>{{responseMessage}}</span>
                </div>
              </form>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div fxFlex="10" fxFlex.xs=""></div>
  </div>

  <div fxLayout="column" class="width100">
    <!-- section 0 -->
    <div>
      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" class="underline-title">
          <h1 class="left-titles">LATEST VIDEO</h1>
        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>


      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" fxLayout="column">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.03893" target="_blank">
                  Rethinking Graph Transformers with Spectral Attention</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                21st of September 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Devin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent Létourneau, Prudencio Tossou
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLSAN' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Devin's
                <a href="https://hannes-stark.com/assets/spectral_attention_networks.pdf" target="_blank"
                   class="link-style-green">slides</a>. Recording is coming soon!
                (69 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Joint first authors of the paper: Dr.
                <a href="https://www.linkedin.com/in/dbeaini/" target="_blank"
                   class="link-style-green">Dominique Beaini</a> who obtained his PhD at École Polytechnique de Montréal
                and is now working as ML Researcher at Valence Discovery on using GNNs for molecules.
                And <a href="https://www.linkedin.com/in/devin-kreuzer-847b52b5/" target="_blank"
                       class="link-style-green">Devin Kreuzer</a> who is doing his masters at McGill University
                supervised
                by Prof. <a href="https://williamleif.github.io/" target="_blank"
                            class="link-style-green">William L. Hamilton</a> while working at MILA on GNNs and
                AI-enabled
                drug discovery.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                In recent years, the Transformer architecture has proven to be very successful in sequence processing,
                but
                its application to other data structures, such as graphs, has remained limited due to the difficulty of
                properly defining positions. Here, we present the Spectral Attention Network (SAN), which uses a learned
                positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position
                of
                each node in a given graph. This LPE is then added to the node features of the graph and passed to a
                fully-connected Transformer. By leveraging the full spectrum of the Laplacian, our model is
                theoretically
                powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance.
                Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an
                information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat
                transfer and electric interaction. When tested empirically on a set of 4 standard datasets, our model
                performs on par or better than state-of-the-art GNNs, and outperforms any attention-based model by a
                wide
                margin, becoming the first fully-connected architecture to perform well on graph benchmarks.
              </p>
            </div>
          </div>

        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>
    </div>

    <div class="top-margin">
      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" class="underline-title">
          <h1 class="left-titles">UPCOMING</h1>
        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>


      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" fxLayout="column">



          <div class="item" fxLayout="column">
            <div fxLayout="row">

            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                5th of October 2021
              </div>

            </div>
            <p class="paragraph">
              No meeting since the ICLR deadline is 9h after our session would start. Good luck to everyone who is
              submitting a paper!
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2102.05152" target="_blank">
                  On Explainability of Graph Neural Networks via Subgraph Explorations</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                12th of October 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, Shuiwang Ji
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              Author of the paper
              <a href="https://scholar.google.com/citations?user=LZKU1hUAAAAJ&hl=en" target="_blank"
                 class="link-style-green">Haiyang Yu</a> who is a PhD student
              at Texas A&M University supervised by Prof. <a href="http://people.tamu.edu/~sji/" target="_blank"
                                                             class="link-style-green">Shuiwang Ji</a>.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are
              considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes
              or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this
              work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs.
              Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently
              exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we
              propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions
              among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute
              Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying
              subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly
              improved explanations, while keeping computations at a reasonable level.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2006.05205" target="_blank">
                  On the Bottleneck of Graph Neural Networks and its Practical Implications</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                19th of October 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Uri Alon, Eran Yahav
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper Dr.
              <a href="https://urialon.ml/" target="_blank"
                 class="link-style-green">Uri Alon</a> who obtained his PhD at Technion advised by Prof.
              <a href="https://www.cs.technion.ac.il/~yahave/" target="_blank"
                 class="link-style-green">Eran Yahav</a> and is now a PostDoc at Carnegie Mellon University working on
              generating source code where GNNs find many applications.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008),
              one of the major problems in training GNNs was their struggle to propagate information between distant
              nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck
              when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially
              growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from
              distant nodes and perform poorly when the prediction task depends on long-range interaction. In this
              paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck
              hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that
              absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and
              GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffers
              from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any
              tuning or additional weights. Our code is available at this <a
              href="https://github.com/tech-srl/bottleneck/" target="_blank"
              class="link-style-green">https URL</a>.
            </p>
          </div>


          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.10934" target="_blank">
                  GRAND: Graph Neural Diffusion</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                26th of October 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Benjamin Paul Chamberlain, James Rowbottom, Maria Gorinova, Stefan Webb, Emanuele Rossi, Michael M.
                  Bronstein
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              Joint first author of the paper
              <a href="https://www.linkedin.com/in/jamesrowbottom/?originalSubdomain=uk" target="_blank"
                 class="link-style-green">James Rowbottom</a>
              who received his master in AI with distinction from Imperial College London and worked on GRAND as an ML
              Reserch intern at Prof.
              <a href="https://www.imperial.ac.uk/people/m.bronstein" target="_blank"
                 class="link-style-green">Michael Bronstein's</a>
              graph ML research group at Twitter.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous
              diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our
              model, the layer structure and topology correspond to the discretisation choices of temporal and spatial
              operators. Our approach
              allows a principled development of a broad new class of GNNs that are able to address the common plights
              of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models
              are stability with respect to perturbations in the data and this is addressed for both implicit and
              explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve
              competitive results on many standard graph benchmarks.
            </p>
          </div>


          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">

            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                2nd of November 2021
              </div>

            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              Maybe you? If you want to present a paper (doesn't have to be yours) or you have someone who might be
              interested in presenting his work then just write me a message on
              <a href="https://join.slack.com/t/logag/shared_invite/zt-u0mbo1ec-zElmvd1oSCXGjXvxLSokvg" target="_blank"
                 class="link-style-green">Slack</a>
              or somewhere else!
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2109.04173" target="_blank">
                  Relating Graph Neural Networks to Structural Causal Model</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                9th of November 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Matej Zečević, Devendra Singh Dhami, Petar Veličković, Kristian Kersting
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author <a class="item-subtitle link-style-black" href="https://matej-zecevic.de/" target="_blank">
              Matej Zečević</a> who is a PhD candidate at TU Darmstadt working with <a
              class="item-subtitle link-style-black" href="https://ml-research.github.io/people/kkersting/index.html"
              target="_blank">
              Kristian Kersting</a> on
              Causality for ML. Before that, he completed his M.Sc. in Computer Science under <a
              class="item-subtitle link-style-black" href="https://www.ias.informatik.tu-darmstadt.de/Member/JanPeters"
              target="_blank">
              Jan Peters</a>, <a
              class="item-subtitle link-style-black" href="https://cifar.ca/bios/stefan-bauer/" target="_blank">
              Stefan Bauer</a>,
              and <a
              class="item-subtitle link-style-black" href="https://www.is.mpg.de/~bs" target="_blank">
              Bernhard Schölkopf</a> at MPI for Intelligent Systems (Tübingen).
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Causality can be described in terms of a structural causal model (SCM) that carries information on the
              variables of interest and their mechanistic relations. For most processes of interest the underlying SCM
              will only be partially observable, thus causal inference tries to leverage any exposed information. Graph
              neural networks (GNN) as universal approximators on structured input pose a viable candidate for causal
              learning, suggesting a tighter integration with SCM. To this effect we present a theoretical analysis from
              first principles that establishes a novel connection between GNN and SCM while providing an extended view
              on general neural-causal models. We then establish a new model class for GNN-based causal inference that
              is necessary and sufficient for causal effect identification. Our empirical illustration on simulations
              and standard benchmarks validate our theoretical proofs.
            </p>
          </div>

        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>
    </div>


    <div class="top-margin">
      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" class="underline-title">
          <h1 class="left-titles">PREVIOUS PAPERS WITH NOTES</h1>
        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>


      <div>
        <div fxFlex="10" fxFlex.xs="2"></div>
        <div fxFlex="80" fxFlex.xs="96" fxLayout="column">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2010.09891" target="_blank">
                  FLAG: Adversarial Data Augmentation for Graph Neural Networks</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                28th of September 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, Tom Goldstein
                </a>
              </div>
            </div>
            <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Kezhi's
                <a href="https://hannes-stark.com/assets/FLAG_slides.pdf" target="_blank"
                   class="link-style-green">slides</a>.
                (46 participants)
              </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author <a
              href="https://scholar.google.com/citations?user=MG46jrMAAAAJ&hl=en" target="_blank"
              class="link-style-green">Kezhi Kong</a> who is a PhD student at the University of Maryland. Advised by
              Prof. Tom Goldstein, he does research in Machine Learning, with a focus on Graph Learning and Adversarial
              Attacks/Defenses.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Data augmentation helps neural networks generalize better, but it remains an open question how to
              effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most
              existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we
              offer a novel direction to augment in the input node feature space for better performance. We propose a
              simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which
              iteratively augments node features with gradient-based adversarial perturbations during training, and
              boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code
              and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and
              in both transductive and inductive settings. Without modifying a model's architecture or training setup,
              FLAG yields a consistent and salient performance boost across both node and graph classification tasks.
              Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code
              datasets.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.03893" target="_blank">
                  Rethinking Graph Transformers with Spectral Attention</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                21st of September 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Devin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent Létourneau, Prudencio Tossou
                </a>
              </div>
            </div>
            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLSAN' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                Devin's
                <a href="https://hannes-stark.com/assets/spectral_attention_networks.pdf" target="_blank"
                   class="link-style-green">slides</a>. Recording is coming soon!
                (69 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Joint first authors of the paper: Dr.
                <a href="https://www.linkedin.com/in/dbeaini/" target="_blank"
                   class="link-style-green">Dominique Beaini</a> who obtained his PhD at École Polytechnique de Montréal
                and is now working as ML Researcher at Valence Discovery on using GNNs for molecules.
                And <a href="https://www.linkedin.com/in/devin-kreuzer-847b52b5/" target="_blank"
                       class="link-style-green">Devin Kreuzer</a> who is doing his masters at McGill University
                supervised
                by Prof. <a href="https://williamleif.github.io/" target="_blank"
                            class="link-style-green">William L. Hamilton</a> while working at MILA on GNNs and
                AI-enabled
                drug discovery.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                In recent years, the Transformer architecture has proven to be very successful in sequence processing,
                but
                its application to other data structures, such as graphs, has remained limited due to the difficulty of
                properly defining positions. Here, we present the Spectral Attention Network (SAN), which uses a learned
                positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position
                of
                each node in a given graph. This LPE is then added to the node features of the graph and passed to a
                fully-connected Transformer. By leveraging the full spectrum of the Laplacian, our model is
                theoretically
                powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance.
                Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an
                information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat
                transfer and electric interaction. When tested empirically on a set of 4 standard datasets, our model
                performs on par or better than state-of-the-art GNNs, and outperforms any attention-based model by a
                wide
                margin, becoming the first fully-connected architecture to perform well on graph benchmarks.
              </p>
            </div>
          </div>


          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2107.01952" target="_blank">
                  Partition and Code: learning how to compress graphs</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                14th of September 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Giorgos Bouritsas, Andreas Loukas, Nikolaos Karalias, Michael M. Bronstein
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Giorgos's slides as
              <a href="https://hannes-stark.com/assets/PnC_slides_logag.pdf" target="_blank"
                 class="link-style-green">pdf</a> or <a href="https://hannes-stark.com/assets/PnC_slides_logag.key"
                                                        target="_blank"
                                                        class="link-style-green">keynote</a>.
              (67 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              The main presenter will be first author <a
              href="https://www.linkedin.com/in/giorgos-bouritsas/?originalSubdomain=uk" target="_blank"
              class="link-style-green">Giorgos Bouritsas</a> who is a PhD student at Imperial College London
              under the supervision of Prof. <a
              href="https://www.imperial.ac.uk/people/m.bronstein" target="_blank"
              class="link-style-green">Michael Bronstein</a> and he is currently a visiting PhD at EPFL, Switzerland.
              Also joining us will be paper author <a
              href="https://andreasloukas.blog/" target="_blank"
              class="link-style-green">Dr. Andreas Loukas</a> who is a research scientist (Ambizione fellow) at
              the LTS2 lab in EPFL.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Can we use machine learning to compress graph data? The absence of ordering in graphs poses a significant
              challenge to conventional compression algorithms, limiting their attainable gains as well as their ability
              to discover relevant patterns. On the other hand, most graph compression approaches rely on
              domain-dependent handcrafted representations and cannot adapt to different underlying graph distributions.
              This work aims to establish the necessary principles a lossless graph compression method should follow to
              approach the entropy storage lower bound. Instead of making rigid assumptions about the graph
              distribution, we formulate the compressor as a probabilistic model that can be learned from data and
              generalise to unseen instances. Our "Partition and Code" framework entails three steps: first, a
              partitioning algorithm decomposes the graph into elementary structures, then these are mapped to the
              elements of a small dictionary on which we learn a probability distribution, and finally, an entropy
              encoder translates the representation into bits. All three steps are parametric and can be trained with
              gradient descent. We theoretically compare the compression quality of several graph encodings and prove,
              under mild conditions, a total ordering of their expected description lengths. Moreover, we show that,
              under the same conditions, PnC achieves compression gains w.r.t. the baselines that grow either linearly
              or quadratically with the number of vertices. Our algorithms are quantitatively evaluated on diverse
              real-world networks obtaining significant performance improvements with respect to different families of
              non-parametric and parametric graph compressors.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.12575" target="_blank">
                  Weisfeiler and Lehman Go Cellular: CW Networks</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                7th of September 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yu Guang Wang, Pietro Liò, Guido Montúfar, Michael
                  Bronstein
                </a>
              </div>
            </div>


            <div>
              <iframe ngClass.gt-sm="float-right" [src]='safeURLCWNetworks' allowfullscreen></iframe>
              <p class="paragraph">
                <a class="slight-bold">Resources:</a>
                The <a href="https://youtu.be/MTQGNVTn9lQ" target="_blank"
                       class="link-style-green">Recording</a> of the whole presentation.
                Cristian's and Fabrizio's slides
                <a href="https://crisbodnar.github.io/files/cwn_logag_talk.pdf" target="_blank"
                   class="link-style-green">Slides</a>. My
                <a
                  href="https://hannes-stark.com/assets/Weisfeiler and Lehman Go Cellular CW Networks.pdf"
                  target="_blank"
                  class="link-style-green">Paper Annotations</a>.
                (49 participants)
              </p>
              <p class="paragraph">
                <a class="slight-bold">Speaker:</a>
                Joint first authors of the paper
                <a href="https://crisbodnar.github.io/" target="_blank"
                   class="link-style-green">Cristian Bodnar</a> and <a
                href="https://scholar.google.com/citations?user=PT2CDA4AAAAJ&hl=en" target="_blank"
                class="link-style-green">Fabrizio Frasca</a>. Cristian is a second-year PhD student
                at Cambridge supervised by Prof. <a href="https://www.cl.cam.ac.uk/~pl219/" target="_blank"
                                                    class="link-style-green">Pietro Liò</a>. He works on topological and
                geometric deep learning. Fabrizio
                is a PhD candidate at Imperial College London supervised by Prof. <a
                href="https://www.imperial.ac.uk/people/m.bronstein" target="_blank"
                class="link-style-green">Michael Bronstein</a> and he works as an ML
                researcher at Twitter.
              </p>
              <p class="no-margin">
                <a class="slight-bold">Abstract:</a>
                Graph Neural Networks (GNNs) are limited in their expressive power, struggle with long-range
                interactions
                and lack a principled way to model higher-order structures. These problems can be attributed to the
                strong
                coupling between the computational graph and the input graph structure. The recently proposed Message
                Passing Simplicial Networks naturally decouple these elements by performing message passing on the
                clique
                complex of the graph. Nevertheless, these models are severely constrained by the rigid combinatorial
                structure of Simplicial Complexes (SCs). In this work, we extend recent theoretical results on SCs to
                regular Cell Complexes, topological objects that flexibly subsume SCs and graphs. We show that this
                generalisation provides a powerful set of graph ``lifting'' transformations, each leading to a unique
                hierarchical message passing procedure. The resulting methods, which we collectively call CW Networks
                (CWNs), are strictly more powerful than the WL test and, in certain cases, not less powerful than the
                3-WL
                test. In particular, we demonstrate the effectiveness of one such scheme, based on rings, when applied
                to
                molecular graph problems. The proposed architecture benefits from provably larger expressivity than
                commonly used GNNs, principled modelling of higher-order signals and from compressing the distances
                between nodes. We demonstrate that our model achieves state-of-the-art results on a variety of
                molecular
                datasets.
              </p>

            </div>

          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://openreview.net/forum?id=-qh0M9XWxnv"
                   target="_blank">
                  Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                31st of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Muhammet Balcilar, Guillaume Renton, Pierre Héroux, Benoit Gaüzère, Sébastien Adam, Paul Honeine
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a>
              Muhammet's
              <a href="https://hannes-stark.com/assets/structural_spectral_awareness_GNN.pdf" target="_blank"
                 class="link-style-green">Slides</a>.
              (54 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper Dr.
              <a href="https://scholar.google.com.tr/citations?user=LRyde44AAAAJ&hl=en" target="_blank"
                 class="link-style-green">Muhammet Balcilar</a>
              who works as R&I Researcher at Interdigital, Rennes. He obtained his Ph.D. at Yildiz Technical University
              and has held several PostDoc positions since then.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              In the recent literature of Graph Neural Networks (GNN), the expressive power of models has been studied
              through their capability to distinguish if two given graphs are isomorphic or not. Since the graph
              isomorphism problem is NP-intermediate, and Weisfeiler-Lehman (WL) test can give sufficient but not enough
              evidence in polynomial time, the theoretical power of GNNs is usually evaluated by the equivalence of
              WL-test order, followed by an empirical analysis of the models on some reference inductive and
              transductive datasets. However, such analysis does not account the signal processing pipeline, whose
              capability is generally evaluated in the spectral domain. In this paper, we argue that a spectral analysis
              of GNNs behavior can provide a complementary point of view to go one step further in the understanding of
              GNNs. By bridging the gap between the spectral and spatial design of graph convolutions, we theoretically
              demonstrate some equivalence of the graph convolution process regardless it is designed in the spatial or
              the spectral domain. Using this connection, we managed to re-formulate most of the state-of-the-art graph
              neural networks into one common framework. This general framework allows to lead a spectral analysis of
              the most popular GNNs, explaining their performance and showing their limits according to spectral point
              of view. Our theoretical spectral analysis is confirmed by experiments on various graph databases.
              Furthermore, we demonstrate the necessity of high and/or band-pass filters on a graph dataset, while the
              majority of GNN is limited to only low-pass and inevitably it fails.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">

          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2103.06857" target="_blank">
                  Should Graph Neural Networks Use Features, Edges, Or Both?</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                24th of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Lukas Faber, Yifan Lu, Roger Wattenhofer
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a> My
              <a
                href="https://hannes-stark.com/assets/Should Graph Neural Networks Use Edges Features of Both.pdf"
                target="_blank"
                class="link-style-green">Paper Annotations</a>.
              (48 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper
              <a href="https://disco.ethz.ch/members/lfaber" target="_blank"
                 class="link-style-green">Lukas Faber</a>
              who is a PhD student in the Distributed Computing Group at ETH Zürich supervised by Prof. Roger
              Wattenhofer. He is also working at Google Zürich.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Graph Neural Networks (GNNs) are the first choice for learning algorithms on graph data. GNNs promise to
              integrate (i) node features as well as (ii) edge information in an end-to-end learning algorithm. How does
              this promise work out practically? In this paper, we study to what extend GNNs are necessary to solve
              prominent graph classification problems. We find that for graph classification, a GNN is not more than the
              sum of its parts. We also find that, unlike features, predictions with an edge-only model do not always
              transfer to GNNs.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">


          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.07594" target="_blank">
                  Graph Contrastive Learning Automated</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                17th of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Yuning You, Tianlong Chen, Yang Shen, Zhangyang Wang
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a> My
              <a
                href="https://hannes-stark.com/assets/Graph Contrastive Learning Automated.pdf"
                target="_blank"
                class="link-style-green">Paper Annotations</a>.
              Yuning's
              <a href="https://hannes-stark.com/assets/yuning_LoGaG_talk.pdf" target="_blank"
                 class="link-style-green">Slides</a>.
              (32 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper
              <a href="https://yyou1996.github.io/" target="_blank" class="link-style-green">Yuning You</a>
              who is a third-year Ph.D. student in ECE at Texas A&M University supervised by Prof.
              <a href="https://shen-lab.github.io/" target="_blank" class="link-style-green">Yang Shen</a>,
              and unofficially co-supervised by Prof. Zhangyang Wang. He has done a lot of popular work on
              self-supervised learning on graphs.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Self-supervised learning on graph-structured data has drawn recent interest for learning generalizable,
              transferable and robust representations from unlabeled graphs. Among many, graph contrastive learning
              (GraphCL) has emerged with promising representation learning performance. Unfortunately, unlike its
              counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data augmentations, which have
              to be manually picked per dataset, by either rules of thumb or trial-and-errors, owing to the diverse
              nature of graph data. That significantly limits the more general applicability of GraphCL. Aiming to
              fill in this crucial gap, this paper proposes a unified bi-level optimization framework to
              automatically, adaptively and dynamically select data augmentations when performing GraphCL on specific
              graph data. The general framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as
              min-max optimization. The selections of augmentations made by JOAO are shown to be in general aligned
              with previous "best practices" observed from handcrafted tuning: yet now being automated, more flexible
              and versatile. Moreover, we propose a new augmentation-aware projection head mechanism, which will route
              output features through different projection heads corresponding to different augmentations chosen at
              each training step. Extensive experiments demonstrate that JOAO performs on par with or sometimes better
              than the state-of-the-art competitors including GraphCL, on multiple graph datasets of various scales
              and types, yet without resorting to any laborious dataset-specific tuning on augmentation selection. We
              release the code at this https URL.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2105.04550" target="_blank">
                  Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth</a>
              </h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                10th of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, Kenji Kawaguchi
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a> My
              <a
                href="https://hannes-stark.com/assets/Optimization%20of%20Graph%20Neural%20Networks%20Implicit%20Acceleration%20by%20Skip%20Connections%20and%20More%20Depth.pdf"
                target="_blank"
                class="link-style-green">Paper Annotations</a>.
              Keyulu's
              <a href="https://people.csail.mit.edu/keyulux/pdf/optimization.pdf" target="_blank"
                 class="link-style-green">Slides</a>.
              (50 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              First author of the paper Dr. <a href="https://people.csail.mit.edu/keyulux/" target="_blank"
                                               class="link-style-green">Keyulu Xu</a>: He received his Ph.D. in EECS
              from MIT, where he was affiliated
              with CSAIL and advised by <a href="https://people.csail.mit.edu/stefje/" target="_blank"
                                           class="link-style-green">Stefanie Jegelka</a>. His papers got multiple
              spotlights + orals and one was, for
              instance, the highest reviewed paper at ICLR 2021. Also joining us is paper author <a
              href="http://www.mozhi.umiacs.io/" target="_blank" class="link-style-green">Mozhi Zhang</a> who is a
              last year PhD student at the University of Maryland working with Jordan Boyd-Graber as advisor on
              generalization properties of neural networks among other topics.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              Graph Neural Networks (GNNs) have been studied through the lens of expressive power and generalization.
              However, their optimization properties are less well understood. We take the first step towards analyzing
              GNN training by studying the gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that
              despite the non-convexity of training, convergence to a global minimum at a linear rate is guaranteed
              under mild assumptions that we validate on real world graphs. Second, we study what may affect the GNNs’
              training speed. Our results show that the training of GNNs is implicitly accelerated by skip connections,
              more depth, and/or a good label distribution. Empirical results confirm that our theoretical results for
              linearized GNNs align with the training behavior of nonlinear GNNs. Our results provide the first
              theoretical support for the success of GNNs with skip connections in terms of optimization, and suggest
              that deep GNNs with skip connections would be promising in practice.
            </p>
          </div>

          <hr ngClass.gt-sm="separator" ngClass.lt-md="separator--small">
          <div class="item" fxLayout="column">
            <div fxLayout="row">
              <h3>
                <a class="item-subtitle link-style-black" href="https://arxiv.org/abs/2106.05234" target="_blank">
                  Do Transformers Really Perform Bad for Graph Representation?</a></h3>
            </div>
            <div fxLayout.gt-md="row" fxLayout="column" class="paragraph">
              <div>
                <fa-icon class="icon-right-margin" [icon]="calendar"></fa-icon>
                3rd of August 2021
              </div>
              <div>
                <a>
                  <fa-icon ngClass.gt-md="icon-right-margin--left" [icon]="authors"></fa-icon>
                  Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu
                </a>
              </div>
            </div>
            <p class="paragraph">
              <a class="slight-bold">Resources:</a> My
              <a
                href="https://hannes-stark.com/assets/Do%20Transformers%20Really%20Perform%20Bad%20for%20Graph%20Representation.pdf"
                target="_blank"
                class="link-style-green">Paper Annotations</a>.
              (35 participants)
            </p>
            <p class="paragraph">
              <a class="slight-bold">Speaker:</a>
              Hey thats me!
              <a href="https://hannes-stark.com/" target="_blank"
                 class="link-style-green">Hannes Stärk</a>.
            </p>
            <p class="no-margin">
              <a class="slight-bold">Abstract:</a>
              The Transformer architecture has become a dominant choice in many domains, such as natural language
              processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards
              of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how
              Transformers could perform well for graph representation learning. In this paper, we solve this mystery by
              presenting Graphormer, which is built upon the standard Transformer architecture, and could attain
              excellent results on a broad range of graph representation learning tasks, especially on the recent OGB
              Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of
              effectively encoding the structural information of a graph into the model. To this end, we propose several
              simple yet effective structural encoding methods to help Graphormer better model graph-structured data.
              Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways
              of encoding the structural information of graphs, many popular GNN variants could be covered as the
              special cases of Graphormer.
            </p>
          </div>
        </div>
        <div fxFlex="10" fxFlex.xs="2"></div>
      </div>
    </div>
  </div>
</div>

